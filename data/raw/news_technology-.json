{
  "industry": "technology",
  "collected_at": "2026-02-26T10:23:50.391095+00:00",
  "hours": 24,
  "limit": 25,
  "count": 25,
  "items": [
    {
      "industry": "technology",
      "source": "VentureBeat",
      "title": "8 billion tokens a day forced AT&T to rethink AI orchestration — and cut costs by 90%",
      "url": "https://venturebeat.com/orchestration/8-billion-tokens-a-day-forced-at-and-t-to-rethink-ai-orchestration-and-cut",
      "published": "2026-02-26T21:30:00+00:00",
      "summary": "<p>When your average daily token usage is 8 billion a day, you have a massive scale problem. This was the case at AT&amp;T, and chief data officer Andy Markus and his team recognized that it simply wasn’t feasible (or economical) to push everything through large reasoning models. So, when building out an internal Ask AT&amp;T personal assistant, they reconstructed the orchestration layer. The result: A multi-agent stack built on LangChain where large language model “super agents” direct smaller, underlying “worker” agents performing more concise, purpose-driven work. This flexible orchestration layer has dramatically improved latency, speed and response times, Markus told VentureBeat. Most notably, his team has seen up to 90% cost savings. “I believe the future of agentic AI is many, many, many small language models (SLMs),” he said. “We find small language models to be just about as accurate, if not as accurate, as a large language model on a given domain area.”</p><p>Most recently, Markus and his team used this re-architected stack along with Microsoft Azure to build and deploy Ask AT&amp;T Workflows, a graphical drag-and-drop agent builder for employees to automate tasks. </p><p>The agents pull from a suite of proprietary AT&amp;T tools that handle document processing, natural language-to-SQL conversion, and image analysis. “As the workflow is executed, it&#x27;s AT&amp;T’s data that&#x27;s really driving the decisions,” Markus said. Rather than asking general questions, “we&#x27;re asking questions of our data, and we bring our data to bear to make sure it focuses on our information as it makes decisions.” Still, a human always oversees the “chain reaction” of agents. All agent actions are logged, data is isolated throughout the process, and role-based access is enforced when agents pass workloads off to one another. “Things do happen autonomously, but the human on the loop still provides a check and balance of the entire process,” Markus said.</p><h2>Not overbuilding, using ‘interchangeable and selectable’ models</h2><p>AT&amp;T doesn’t take a &quot;build everything from scratch&quot; mindset, Markus noted; it’s more relying on models that are “interchangeable and selectable” and “never rebuilding a commodity.” As functionality matures across the industry, they’ll deprecate homegrown tools in lieu of off the shelf options, he explained. “Because in this space, things change every week, if we&#x27;re lucky, sometimes multiple times a week,” he said. “We need to be able to pilot, plug in and plug out different components.” They do “really rigorous” evaluations of available options as well as their own; for instance, their Ask Data with Relational Knowledge Graph has topped the Spider 2.0 text to SQL accuracy leaderboard, and other tools have scored highly on the BERT SQL benchmark. In the case of homegrown agentic tools, his team uses LangChain as a core framework, fine-tunes models with standard retrieval-augmented generation (RAG) and other in-house algorithms, and partners closely with Microsoft, using the tech giant’s search functionality for their vector store. Ultimately, though, it’s important not to just fuse agentic AI or other advanced tools into everything for the sake of it, Markus advised. “Sometimes we over complicate things,” he said. “Sometimes I&#x27;ve seen a solution over engineered.” Instead, builders should ask themselves whether a given tool actually needs to be agentic. This could include questions like: What accuracy level could be achieved if it was a simpler, single-turn generative solution? How could they break it down into smaller pieces where each piece could be delivered “way more accurately”?, as Markus put it. Accuracy, cost and tool responsiveness should be core principles. “Even as the solutions have gotten more complicated, those three pretty basic principles still give us a lot of direction,” he said. </p><h2>How 100,000 employees are actually using it</h2><p>Ask AT&amp;T Workflows has be",
      "content_text": "<p>When your average daily token usage is 8 billion a day, you have a massive scale problem. This was the case at AT&amp;T, and chief data officer Andy Markus and his team recognized that it simply wasn’t feasible (or economical) to push everything through large reasoning models. So, when building out an internal Ask AT&amp;T personal assistant, they reconstructed the orchestration layer. The result: A multi-agent stack built on LangChain where large language model “super agents” direct smaller, underlying “worker” agents performing more concise, purpose-driven work. This flexible orchestration layer has dramatically improved latency, speed and response times, Markus told VentureBeat. Most notably, his team has seen up to 90% cost savings. “I believe the future of agentic AI is many, many, many small language models (SLMs),” he said. “We find small language models to be just about as accurate, if not as accurate, as a large language model on a given domain area.”</p><p>Most recently, Markus and his team used this re-architected stack along with Microsoft Azure to build and deploy Ask AT&amp;T Workflows, a graphical drag-and-drop agent builder for employees to automate tasks. </p><p>The agents pull from a suite of proprietary AT&amp;T tools that handle document processing, natural language-to-SQL conversion, and image analysis. “As the workflow is executed, it&#x27;s AT&amp;T’s data that&#x27;s really driving the decisions,” Markus said. Rather than asking general questions, “we&#x27;re asking questions of our data, and we bring our data to bear to make sure it focuses on our information as it makes decisions.” Still, a human always oversees the “chain reaction” of agents. All agent actions are logged, data is isolated throughout the process, and role-based access is enforced when agents pass workloads off to one another. “Things do happen autonomously, but the human on the loop still provides a check and balance of the entire process,” Markus said.</p><h2>Not overbuilding, using ‘interchangeable and selectable’ models</h2><p>AT&amp;T doesn’t take a &quot;build everything from scratch&quot; mindset, Markus noted; it’s more relying on models that are “interchangeable and selectable” and “never rebuilding a commodity.” As functionality matures across the industry, they’ll deprecate homegrown tools in lieu of off the shelf options, he explained. “Because in this space, things change every week, if we&#x27;re lucky, sometimes multiple times a week,” he said. “We need to be able to pilot, plug in and plug out different components.” They do “really rigorous” evaluations of available options as well as their own; for instance, their Ask Data with Relational Knowledge Graph has topped the Spider 2.0 text to SQL accuracy leaderboard, and other tools have scored highly on the BERT SQL benchmark. In the case of homegrown agentic tools, his team uses LangChain as a core framework, fine-tunes models with standard retrieval-augmented generation (RAG) and other in-house algorithms, and partners closely with Microsoft, using the tech giant’s search functionality for their vector store. Ultimately, though, it’s important not to just fuse agentic AI or other advanced tools into everything for the sake of it, Markus advised. “Sometimes we over complicate things,” he said. “Sometimes I&#x27;ve seen a solution over engineered.” Instead, builders should ask themselves whether a given tool actually needs to be agentic. This could include questions like: What accuracy level could be achieved if it was a simpler, single-turn generative solution? How could they break it down into smaller pieces where each piece could be delivered “way more accurately”?, as Markus put it. Accuracy, cost and tool responsiveness should be core principles. “Even as the solutions have gotten more complicated, those three pretty basic principles still give us a lot of direction,” he said. </p><h2>How 100,000 employees are actually using it</h2><p>Ask AT&amp;T Workflows has been rolled out to 100,000-plus employees. More than half say they use it every day, and active adopters report productivity gains as high as 90%, Markus said. “We&#x27;re looking at, are they using the system repeatedly? Because stickiness is a good indicator of success,” he said. The agent builder offers “two journeys” for employees. One is pro-code, where users can program Python behind the scenes, dictating rules for how agents should work. The other is no-code, featuring a drag-and-drop visual interface for a “pretty light user experience,” Markus said. Interestingly, even proficient users are gravitating toward the latter option. At a recent hackathon geared to a technical audience, participants were given a choice of both, and more than half chose low code. “This was a surprise to us, because these people were all very competent in the programming aspect,” Markus said. Employees are using agents across a variety of functions; for instance, a network engineer may build a series of them to address alerts and reconnect customers when they lose connectivity. In this scenario, one agent can correlate telemetry to identify the network issue and its location, pull change logs and check for known issues. Then, it can open a trouble ticket. Another agent could then come up with ways to solve the issue and even write new code to patch it. Once the problem is resolved, a third agent can then write up a summary with preventative measures for the future. “The [human] engineer would watch over all of it, making sure the agents are performing as expected and taking the right actions,” Markus said. </p><h2>AI-fueled coding is the future</h2><p>That same engineering discipline — breaking work into smaller, purpose-built pieces — is now reshaping how AT&amp;T writes code itself, through what Markus calls &quot;AI-fueled coding.&quot; He compared the process to RAG; devs use agile coding methods in an integrated development environment (IDE) along with “function-specific” build archetypes that dictates how code should interact. The output is not loose code; the code is “very close to production grade,” and could reach that quality in one turn. “We&#x27;ve all worked with vibe coding, where we have an agentic kind of code editor,” Markus noted. But AI-fueled coding “eliminates a lot of the back and forth iterations that you might see in vibe coding.” He sees this coding technique as “tangibly redefining” the software development cycle, ultimately shortening development timelines and increasing output of production-grade code. Non-technical teams can also get in on the action, using plain language prompts to build software prototypes. His team, for instance, has used the technique to build an internal curated data product in 20 minutes; without AI, building it would have taken six weeks. “We develop software with it, modify software with it, do data science with it, do data analytics with it, do data engineering with it,” Markus said. “So it&#x27;s a game changer.”</p>",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "MIT Technology Review",
      "title": "America was winning the race to find Martian life. Then China jumped in.",
      "url": "https://www.technologyreview.com/2026/02/26/1133584/america-china-mars-sample-return-space-race-nasa/",
      "published": "2026-02-26T10:00:00+00:00",
      "summary": "To most people, rocks are just rocks. To geologists, they are much, much more: crystal-filled time capsules with the power to reveal the state of the planet at the very moment they were forged.&#160; For decades, NASA had been on a time capsule hunt like none other—one across Mars. Its rovers have journeyed around a&#8230;",
      "content_text": "Two messages were encoded on the 70-foot parachute used by the Perseverance rover as it descended toward Mars. This annotated image shows how NASA systems engineer Ian Clark used a binary code to spell out “Dare Mighty Things” in the orange and white strips; he also included the GPS coordinates for the mission’s headquarters at the Jet Propulsion Laboratory. NASA/JPL-CALTECH VIA AP IMAGES “Put simply, this is the most scientifically careful sample collection mission possible, conducted in one of the most promising places on Mars to look for signs of past life,” says Jonathan Lunine , the chief scientist at NASA’s Jet Propulsion Laboratory in California. “And, of course, should evidence of life be found in the sediments, that would be an historic discovery.” It got off to an auspicious start. On July 30, 2020, in the throes of the covid-19 pandemic, NASA’s Perseverance rover launched atop a rocket from Florida’s Cape Canaveral. The NASA administrator at the time, Jim Bridenstine, didn’t mince words: “We are in extraordinary times right now,” he told reporters, “yet we have in fact persevered, and we have protected this mission because it is so important.” But just earlier that same month, the mission to Mars had turned into a race . China was now prepping its own sample return spacecraft. And that’s when things for MSR started to unravel. XINMEI LIU China was comparatively late to develop a competitive space program, but once it began doing so, it wasted no time. In 2003, it first sent one of its astronauts into space, via its own bespoke rocket; in the two decades since, it has launched its own space station and sent multiple uncrewed spacecraft to the moon—first orbiters, then landers—as part of its Chang’e Project , named after a lunar goddess. But a real turning point for China’s interplanetary ambitions came in 2020, the same year as Perseverance’s launch to Mars. That December, Chang’e-5 touched down in the moon’s Ocean of Storms, a realm of frozen lava 1,600 miles long. It grabbed some 2-billion-year-old rocks, put them in a rocket, and blasted them into the firmament. The samples were captured by a small orbiting spacecraft; crucially, the idea was not all that dissimilar from how MSR imagined catching its own samples, baseball-glove style. China’s lunar haul was then dropped off back on Earth just before Christmas. It marked the first time since 1976 that samples had been returned from the moon, and the mission was seamless. China brought back soil samples from the moon’s Ocean of Storms during its Chang’e-5 mission, marking the first time since 1976 that samples had been returned from the moon. WIKIMEDIA COMMONS That same year, China made its first foray toward Mars. The project was called Tianwen-1 , meaning “Questions to Heaven”—the first in a new series of audacious space missions to the Red Planet and orbiting asteroids. While its success was far from guaranteed, China was willing to kick into high gear immediately, sending both an orbiting spacecraft and a rover to Mars at the same time. No other country had ever managed to perform this act of spaceflight acrobatics on its first try.",
      "cover_image_url": "https://wp.technologyreview.com/wp-content/uploads/2026/02/260225_xinmei_TR_RacetoMars_image1intro_final.jpg?resize=1200,600"
    },
    {
      "industry": "technology",
      "source": "Wired",
      "title": "TurboTax Service Codes: Up to 20% Off",
      "url": "https://www.wired.com/story/turbotax-coupon/",
      "published": "2026-02-26T06:05:00+00:00",
      "summary": "Tax season doesn’t have to be stressful. Save up to 20% on federal tax filings, $40 off Expert Assist, and more exclusive TurboTax discount codes on WIRED.",
      "content_text": "TurboTax has been my go-to for self-filing taxes for years. It’s good enough for me—and literally millions of other users. TurboTax has quickly become a favorite among users for its easy-to-use self-filing services, along with easy access to assistance from tax professionals. TurboTax has free self-service for qualifying people, and TurboTax Live Assisted, where you file online with tax expert help if needed, and TurboTax Full Service, where a tax expert will do your taxes and review them with you before filing. To help you figure out the often confusing process of filing taxes, I’ve written a guide on How to Pay Your Taxes Online , and The Best Tax Services . In both, I included TurboTax and similar services to help navigate between the options available for your specific tax needs. WIRED also has TurboTax coupons to save money while you begin the always-annoying task of filing your taxes this year. Canadian customers click here , these coupons are not for you. Save 10% With TurboTax Service Codes and 2026 Early Filing Offers Right now, you can get an extra 10% off Federal online tax filings with today’s TurboTax coupons. And best of all, these savings are in addition to the already-applied early-season discount: up to $40 off if you file by March 30. You can use this TurboTax coupon for 10% off their most popular online tax products, like TurboTax Full Service. This price includes W-2 and 1040, with state returns for an additional fee. With ExpertFull Service, a tax professional will do your taxes for you, which costs a bit more than DIY-ing it, but can save you headaches and messes when it comes to what you owe. Things only get more expensive as tax season approaches, so file now to get the best price of the season. If a TurboTax expert didn’t file for you last year, you’ll get even more in savings if you choose Expert Full Service. This service is now a simple flat fee of $150 for new customers, with Federal and State services included. Act soon, because those switching to TurboTax need to file by March 18 for this special discount. If your taxes are a bit more straightforward, but you still have some questions, TurboTax Expert Assist (Live) offers the best of both worlds, where you file online yourself with tax expert help nearby if you end up needing it. With these TurboTax deals, you’ll get a 10% off TurboTax Live Assisted, with options starting at only $36 and (temporarily) capping out at $150 if you want to fully hand off your taxes. (Like the above service, this discount is for Federal tax products and filing early is the best way to ensure you’re getting the lowest price of the season). *Price estimates are provided prior to a tax expert starting work on your taxes. Estimates are based on initial information you provide about your tax situation, including forms you upload to assist your expert in preparing your tax return and forms or schedules we think you’ll need to file based on what you tell us about your tax situation. Final price is determined at the time of print or electronic filing and may vary based on your actual tax situation, forms used to prepare your return, and forms or schedules included in your individual return. Prices are subject to change without notice and may impact your final price. Give a 20% Off TurboTax Discount Code, Get up to $500 To save even more, get a TurboTax referral link , and when your friends use your link, they can get 20% off TurboTax federal products if they’re new customers of the service. As an added bonus, for each friend who files using your link, you’ll get a $25 gift card—you can earn up to a whopping $500 in gift cards (at this point, you’re basically getting another refund in gift cards). Here’s the nitty gritty: friends can receive 20% off the preparation and filing of federal tax return or business federal tax return through a TurboTax Online or TurboTax Live product, through October 31. You’ll get an email with the gift card or charitable donation within 45 days of your friend’s purchase. (Restrictions apply on TurboTax Free Edition, Intuit TurboTax Verified Pro, TurboTax State returns, TurboTax CD/Download products, and other add-on offerings.) Save up to 20% on TurboTax Business Services If you’re a small business owner , Expert Assist is probably the best service for you, with qualified tax experts at the ready to help you with any of those tricky filing questions. Right now, if you file your business tax returns with TurboTax, you’ll get 20% off one 2025 personal federal tax return. This type of service is best for those with S-corps and partnerships, and TurboTax will match you with a small business tax expert who knows your industry to maximize your deductions. In addition to 20% off one personal tax filing offer for business customers , you can also get 10% off Business Expert Full Service and Expert Assist for small business owners. This discount applies to TurboTax federal products only (meaning no state products are included). These massive savings are in addition to early-season discounts. New Customers Can File For Free in the App If you’re unhappy with your old service, or just want to switch to TurboTax, you’ll be able to file your own taxes for free if you do it in the TurboTax app by February 28. If you have simple taxes, filing through your phone app is a great way to save big on usual filing fees. And best of all, this applies to Federal and State taxes, including all tax situations. You’ll just need to make sure you didn’t file with TurboTax last year, and this offer only applies to Do It Yourself, not TurboTax Experts, products. To save big and file for free as a new TurboTax customer, you’ll need to make sure you redeem this offer before midnight on February 28. File for $0 With TurboTax Free Edition As mentioned above, users can file for free with TurboTax free edition . Around 37% of taxpayers qualify. Simple Form 1040 returns only (no schedules except for Earned Income Tax Credit, Child Tax Credit, Student Loan Interest, and Schedule 1-A). While this may not be the blanket service for every type of tax filer, this works for those with simple finances, so do a little research and see if TurboTax free edition may be right for you to save big instead of spending big. TurboTax free edition includes $0 to file for Federal taxes and $0 for State taxes. This service is ideal for simple uploads, like W-2’s, where all you’ll need to do is import, upload, or snap a photo of your W–2 and TurboTax will autofill your info for you, saving you time and potential headaches. There are other ways to save too, with TurboTax discount codes for military members . As a thank you, military members can file their own taxes and get both their federal and state taxes done for free returns only (no schedules, except for EITC, CTC, student loan interest, and Schedule 1-A). Other Free TurboTax Tools for Tax Season In addition to the differing plans for individuals’ needs, TurboTax also has other helpful resources for this tax season, including a Tax Refund Calculator, where you can estimate your 2026 return by answering questions about your life and income. Similarly, TurboTax also offers a Tax Bracket Calculator to help you estimate your 2025 taxable income. All you need to do is enter your tax year filing status and taxable income to calculate your estimated tax rate. With TurboTax, you start for free and pay only when you file—plus you get 100% accurate calculations, guaranteed. That’s not all with the guarantees either. TurboTax ensures a “Maximum Refund Guarantee,” meaning if you get a larger refund or less taxes due from another tax preparation method, they’ll refund the applicable TurboTax federal and/or state purchase price paid. You can also get quick answers for any tax questions from AI-Powered Intuit Assist, and for the lazy filers among us (myself included), you can upload or snap a photo of your W-2 and the info will be autofilled for you. Is There a Service Code for TurboTax? TurboTax service codes are unique codes, usually issued by customer support for specific situations; these will require you to input the code into the \"I have a service code\" field before payment. These single-use codes are for any technical or billing issues you may have during the tax filing process. Even if you don’t have a service code, don’t fret—there are still tons of ways to get TurboTax coupons, including a discount of 10% when you file before Tax Day (April 15). Plus, you can get 20% off TurboTax referral codes if you use a friend’s invite link, and if you send your link to a friend and they purchase an online TurboTax product, you’ll receive e-gift cards of up to $500 as a reward from TurboTax. Save on federal and business tax filings by following here for limited-time offers throughout the 2026 tax season.",
      "cover_image_url": "https://media.wired.com/photos/66ea076fca863bb4c1028b64/191:100/w_1280,c_limit/WIRED-Coupons-11.jpg"
    },
    {
      "industry": "technology",
      "source": "Wired",
      "title": "Paramount Plus Coupon Codes and Deals: 50% Off",
      "url": "https://www.wired.com/story/paramount-plus-coupon-code/",
      "published": "2026-02-26T06:00:00+00:00",
      "summary": "Save on streaming with the latest Paramount+ promo codes and deals, including 50% off subscriptions, free trials, and more.",
      "content_text": "Once the most talked-about TV show in the country, South Park, is on Paramount+. Donâ€™t you want to know what got Trump in such a tizzy? Stream the much buzzed-about South Park, fan-favorite Yellowstone, original series MobLand, and rebooted crime drama Dexter & Dexter on Paramount+. The streaming network has a bingeable TV series for almost everyone. And whether you want to remember Lindsay Lohanâ€™s old face in the classic Mean Girls flick, or wonder just how many more sequels Tom Cruise has left in him with Top Gun: Maverick , thereâ€™s a bevy of films to stream, too. If youâ€™re like me and have at least half a dozen streaming services, our Paramount+ coupon codes can help you save so you can watch the content you want without having to get rid of one of your other beloved content platforms. (I love pretending the world isnâ€™t full of suffering around me and instead focus on Sylvester Stalloneâ€™s ever-changing Play-Doh face in Tulsa King .) Try Paramount+ Free With a One-Week Trial If youâ€™re unsure if youâ€™ll actually want to commit to Paramount+, or if thereâ€™s a sports event like March Madness games and you only need to access the content for a little while, Paramount+â€™s free trial is a great option. The trial lasts one week, is for new subscribers only, and canâ€™t be paired with other offers. There are tiered plans , including Essential, which allows for 3 devices, select Showtime series, NFL games, and can be streamed on up to 3 devices at once, but has ads; and Premium, which includes all that except there are no ads, downloadable content, CBS live, and all of Showtime content. Find the Right Paramount+ Plan Pricing and Get the Latest Deals Itâ€™s important that you choose the right Paramount+ streaming plan for you so that you can get the best bang for your buck. Lucky for you, all plans come with a 7-day free trial so you can make sure youâ€™re choosing the right plan for you. The first is Paramount+ Essential, which is $8 per month. It has ads included, but youâ€™ll have access to over 40,000 episodes and movies. And youâ€™ll be able to stream on 3 devices at once, be able to watch NFL games on CBS and UEFA Champions League, and select Showtime series are also available. Paramount+ Premium is the next tier (and the most popular choice), which starts at $13 per month (after the free trial ends), and youâ€™ll get everything mentioned in the previous tier, without ads. Youâ€™ll have all that as well as the ability to watch in 4K UHD, Dolby Vision or HDR10, downloadable movies and shows, streaming CBS live and all of Showtimeâ€™s content library. Can You Cancel Paramount Plus at any Time? If you find the service isnâ€™t right for you, or just need to cut down on subscriptions, you can cancel Paramount+ any time. However, the cancellation process depends on where you signed up. If you signed up directly on the website, youâ€™ll need to go to your account page . Save on a Paramount+ Subscription With Student and Military Discounts If youâ€™re a student now (or have your student ID lying around somewhere), you can get a Paramount+ plan at only $4 a month. All you have to do is verify your student status and youâ€™ll get 50% off any plan of your choosing for the first year. Or if youâ€™re a military member, Paramount+ gives 50% off any subscription for life. Watch Paramount+ Originals and Fan Favorites Thereâ€™s truly something for everyone in the family, with movies, kidsâ€™ shows, and Paramount+ originals included in every plan. If youâ€™re feeling spooky , Iâ€™d recommend Dexter: Resurrection , or Yellowjackets , but if youâ€™re looking for something more family-friendly , thereâ€™s super popular cartoons like Rango or Sonic the Hedgehog to choose from. Looking for specific recommendations? Iâ€™ve got you. There are tons of great new releases coming to Paramount+ this month, including Landman season 2, new Paramount+ original comedy series Crutch starring Tracy Morgan, and new episodes of (my favorite) newly premiered Ink Master Season 17. There are also tons of new movies, including The Cut , a boxing drama starring Orlando Bloom, dark comedy Shell , and true-crime tale M y Nightmare Stalker: The Eva LaRue Story . Plus, Paramount+ will be playing the important NFL holiday games. Check out the wide breadth of TV and movie content to choose from on Paramount+ (and use the Paramount+ promo codes above to save on whatever plan you decide). Stream Live Sports and Events on Paramount+ For better or worse, Iâ€™m a Chiefs fan (cue the booing). I usually get a Paramount+ plan during the football season to keep up with my favorite beefy, TBI-ridden men. You can stream all of the NFL coverage you want all season long, plus, 24/7 live channels are now streaming on Paramount+, so youâ€™ll never need to give your brain the time to process the horrors . Stream UFC Fights Live on Paramount+ Paramount+ has all the man-on-man action you want, from bloody brawls to KOâ€™s. Paramount+ is your one-stop shop to stream UFC live so you can catch every fight. This includes UFC 326: Holloway vs. Oliveira 2, airing March 7 and UFC Fight Night: Emmett vs. Vallejos, airing March 14. Watch March Madness With Paramount+ The annual tournament that determines which menâ€™s and womenâ€™s Division I teams will win the NCAA Basketball championships, March Madness, will be streaming on Paramount+ this spring. You can watch any men's March Madness games that are being broadcast on CBS with Paramount+. Let the games begin! Donâ€™t Miss the Champions League Soccer on Paramount+ If football (or soccer) is more your jam, Paramount+ also has you covered. You can watch Champions League Soccer at Paramount+, including fan favorites and heated rivalries from Real Madrid, AC Milan, Bayern Munich, Liverpool, Barcelona, and more.",
      "cover_image_url": "https://media.wired.com/photos/67b63b985c0507c1bb18ab66/191:100/w_1280,c_limit/WIRED-Coupons-R2_14.png"
    },
    {
      "industry": "technology",
      "source": "VentureBeat",
      "title": "Alibaba's new open source Qwen3.5-Medium models offer Sonnet 4.5 performance on local computers",
      "url": "https://venturebeat.com/technology/alibabas-new-open-source-qwen3-5-medium-models-offer-sonnet-4-5-performance",
      "published": "2026-02-26T03:39:00+00:00",
      "summary": "<p>Alibaba&#x27;s now famed Qwen AI development team has done it again: a little more than a day ago, they released the <a href=\"https://x.com/Alibaba_Qwen/status/2026339351530188939\">Qwen3.5 Medium Model series</a> consisting of four new large language models (LLMs) with support for agentic tool calling, three of which are available for commercial usage by enterprises and indie developers under the standard open source Apache 2.0 license:</p><ul><li><p>Qwen3.5-35B-A3B </p></li><li><p>Qwen3.5-122B-A10B </p></li><li><p>Qwen3.5-27B</p></li></ul><p>Developers can download them now on <a href=\"https://huggingface.co/collections/Qwen/qwen35\">Hugging Face</a> and <a href=\"https://modelscope.cn/collections/Qwen/Qwen35\">ModelScope</a>. A fourth model, Qwen3.5-Flash, appears to be proprietary and only available through the <a href=\"https://modelstudio.console.alibabacloud.com/ap-southeast-1/?tab=doc#/doc/?type=model&amp;url=2840914_2&amp;modelId=group-qwen3.5-flash\">Alibaba Cloud Model Studio API</a>, but still offers a strong advantage in cost compared to other models in the West (see pricing comparison table below). </p><p>But the big twist with the open source models is that they offer comparably high performance on third-party benchmark tests to similarly-sized proprietary models from major U.S. startups like OpenAI or Anthropic, actually beating OpenAI&#x27;s GPT-5-mini and Anthropic&#x27;s Claude Sonnet 4.5 — the latter model which was <a href=\"https://venturebeat.com/technology/anthropics-new-claude-can-code-for-30-hours-think-of-it-as-your-ai-coworker\">just released five months ago. </a></p><div></div><p>And, the Qwen team <a href=\"https://x.com/Alibaba_Qwen/status/2026502059479179602\">says</a> it has engineered these models to remain highly accurate even when &quot;quantized,&quot; a process that reduces their footprint further by reducing the numbers by which the model&#x27;s settings are stored from many values to far fewer. </p><p>Crucially, this release brings &quot;frontier-level&quot; context windows to the desktop PC. The flagship Qwen3.5-35B-A3B can now exceed a 1 million token context length on consumer-grade GPUs with 32GB of VRAM. While not something everyone has access to, this is far less compute than many other comparably-performant options. </p><p>This leap is made possible by near-lossless accuracy under 4-bit weight and KV cache quantization, allowing developers to process massive datasets without server-grade infrastructure.</p><h3><b>Technology: Delta force</b></h3><p>At the heart of Qwen 3.5&#x27;s performance is a sophisticated hybrid architecture. While many models rely solely on standard Transformer blocks, Qwen 3.5 integrates Gated Delta Networks combined with a sparse Mixture-of-Experts (MoE) system.The technical specifications for the Qwen3.5-35B-A3B reveal a highly efficient design:</p><ul><li><p><b>Parameter Efficiency</b>: While the model houses 35 billion parameters in total, it only activates <b>3 billion</b> for any given token.</p></li><li><p><b>Expert Diversity</b>: The MoE layer utilizes 256 experts, with 8 routed experts and 1 shared expert helping to maintain performance while slashing inference latency.</p></li><li><p><b>Near-Lossless Quantization</b>: The series maintains high accuracy even when compressed to 4-bit weights, significantly reducing the memory footprint for local deployment.</p></li><li><p><b>Base Model Release</b>: In a move to support the research community, Alibaba has open-sourced the <b>Qwen3.5-35B-A3B-Base</b> model alongside the instruct-tuned versions.</p></li></ul><h3><b>Product: Intelligence that &#x27;thinks&#x27; first</b></h3><p>Qwen 3.5 introduces a native &quot;Thinking Mode&quot; as its default state. Before providing a final answer, the model generates an internal reasoning chain—delimited by <code>&lt;think&gt;</code> tags—to work through complex logic.The product lineup is tailored for varying hardware environments:</p><ul><li><p><b>Qwen3.5-27B:</b> Optimi",
      "content_text": "<p>Alibaba&#x27;s now famed Qwen AI development team has done it again: a little more than a day ago, they released the <a href=\"https://x.com/Alibaba_Qwen/status/2026339351530188939\">Qwen3.5 Medium Model series</a> consisting of four new large language models (LLMs) with support for agentic tool calling, three of which are available for commercial usage by enterprises and indie developers under the standard open source Apache 2.0 license:</p><ul><li><p>Qwen3.5-35B-A3B </p></li><li><p>Qwen3.5-122B-A10B </p></li><li><p>Qwen3.5-27B</p></li></ul><p>Developers can download them now on <a href=\"https://huggingface.co/collections/Qwen/qwen35\">Hugging Face</a> and <a href=\"https://modelscope.cn/collections/Qwen/Qwen35\">ModelScope</a>. A fourth model, Qwen3.5-Flash, appears to be proprietary and only available through the <a href=\"https://modelstudio.console.alibabacloud.com/ap-southeast-1/?tab=doc#/doc/?type=model&amp;url=2840914_2&amp;modelId=group-qwen3.5-flash\">Alibaba Cloud Model Studio API</a>, but still offers a strong advantage in cost compared to other models in the West (see pricing comparison table below). </p><p>But the big twist with the open source models is that they offer comparably high performance on third-party benchmark tests to similarly-sized proprietary models from major U.S. startups like OpenAI or Anthropic, actually beating OpenAI&#x27;s GPT-5-mini and Anthropic&#x27;s Claude Sonnet 4.5 — the latter model which was <a href=\"https://venturebeat.com/technology/anthropics-new-claude-can-code-for-30-hours-think-of-it-as-your-ai-coworker\">just released five months ago. </a></p><div></div><p>And, the Qwen team <a href=\"https://x.com/Alibaba_Qwen/status/2026502059479179602\">says</a> it has engineered these models to remain highly accurate even when &quot;quantized,&quot; a process that reduces their footprint further by reducing the numbers by which the model&#x27;s settings are stored from many values to far fewer. </p><p>Crucially, this release brings &quot;frontier-level&quot; context windows to the desktop PC. The flagship Qwen3.5-35B-A3B can now exceed a 1 million token context length on consumer-grade GPUs with 32GB of VRAM. While not something everyone has access to, this is far less compute than many other comparably-performant options. </p><p>This leap is made possible by near-lossless accuracy under 4-bit weight and KV cache quantization, allowing developers to process massive datasets without server-grade infrastructure.</p><h3><b>Technology: Delta force</b></h3><p>At the heart of Qwen 3.5&#x27;s performance is a sophisticated hybrid architecture. While many models rely solely on standard Transformer blocks, Qwen 3.5 integrates Gated Delta Networks combined with a sparse Mixture-of-Experts (MoE) system.The technical specifications for the Qwen3.5-35B-A3B reveal a highly efficient design:</p><ul><li><p><b>Parameter Efficiency</b>: While the model houses 35 billion parameters in total, it only activates <b>3 billion</b> for any given token.</p></li><li><p><b>Expert Diversity</b>: The MoE layer utilizes 256 experts, with 8 routed experts and 1 shared expert helping to maintain performance while slashing inference latency.</p></li><li><p><b>Near-Lossless Quantization</b>: The series maintains high accuracy even when compressed to 4-bit weights, significantly reducing the memory footprint for local deployment.</p></li><li><p><b>Base Model Release</b>: In a move to support the research community, Alibaba has open-sourced the <b>Qwen3.5-35B-A3B-Base</b> model alongside the instruct-tuned versions.</p></li></ul><h3><b>Product: Intelligence that &#x27;thinks&#x27; first</b></h3><p>Qwen 3.5 introduces a native &quot;Thinking Mode&quot; as its default state. Before providing a final answer, the model generates an internal reasoning chain—delimited by <code>&lt;think&gt;</code> tags—to work through complex logic.The product lineup is tailored for varying hardware environments:</p><ul><li><p><b>Qwen3.5-27B:</b> Optimized for high efficiency, supporting a context length of over 800K tokens.</p></li><li><p><b>Qwen3.5-Flash: </b>The production-grade hosted version, featuring a default 1 million token context length and built-in official tools.</p></li><li><p><b>Qwen3.5-122B-A10B:</b> Designed for server-grade GPUs (80GB VRAM), this model supports 1M+ context lengths while narrowing the gap with the world&#x27;s largest frontier models.</p></li></ul><p>Benchmark results validate this architectural shift. The 35B-A3B model notably surpasses much larger predecessors, such as Qwen3-235B, as well as the aforementioned proprietary GPT-5 mini and Sonnet 4.5 in categories including knowledge (MMMLU) and visual reasoning (MMMU-Pro).</p><h3><b>Pricing and API integration</b></h3><p>For those not hosting their own weights, Alibaba Cloud Model Studio provides a competitive API for Qwen3.5-Flash.</p><ul><li><p><b>Input</b>: $0.1 per 1M tokens</p></li><li><p><b>Output</b>: $0.4 per 1M tokens</p></li><li><p><b>Cache Creation</b>: $0.125 per 1M tokens</p></li><li><p><b>Cache Read</b>: $0.01 per 1M tokens</p></li></ul><p>The API also features a granular Tool Calling pricing model, with Web Search at $10 per 1,000 calls and Code Interpreter currently offered for a limited time at no cost.</p><p>This makes Qwen3.5-Flash among the most affordable to run over API among all the major LLMs in the world. See a table comparing them below:</p><table><tbody><tr><td><p><b>Model</b></p></td><td><p><b>Input</b></p></td><td><p><b>Output</b></p></td><td><p><b>Total Cost</b></p></td><td><p><b>Source</b></p></td></tr><tr><td><p>Qwen 3 Turbo</p></td><td><p>$0.05</p></td><td><p>$0.20</p></td><td><p>$0.25</p></td><td><p><a href=\"https://www.alibabacloud.com/help/en/model-studio/developer-reference/model-pricing\">Alibaba Cloud</a></p></td></tr><tr><td><p><b>Qwen3.5-Flash</b></p></td><td><p><b>$0.10</b></p></td><td><p><b>$0.40</b></p></td><td><p><b>$0.50</b></p></td><td><p><a href=\"https://modelstudio.console.alibabacloud.com/ap-southeast-1/?tab=doc#/doc/?type=model&amp;url=2840914_2&amp;modelId=group-qwen3.5-flash\">Alibaba Cloud</a></p></td></tr><tr><td><p>deepseek-chat (V3.2-Exp)</p></td><td><p>$0.28</p></td><td><p>$0.42</p></td><td><p>$0.70</p></td><td><p><a href=\"https://api-docs.deepseek.com/quick_start/pricing\">DeepSeek</a></p></td></tr><tr><td><p>deepseek-reasoner (V3.2-Exp)</p></td><td><p>$0.28</p></td><td><p>$0.42</p></td><td><p>$0.70</p></td><td><p><a href=\"https://api-docs.deepseek.com/quick_start/pricing\">DeepSeek</a></p></td></tr><tr><td><p>Grok 4.1 Fast (reasoning)</p></td><td><p>$0.20</p></td><td><p>$0.50</p></td><td><p>$0.70</p></td><td><p><a href=\"https://docs.x.ai/docs/pricing\">xAI</a></p></td></tr><tr><td><p>Grok 4.1 Fast (non-reasoning)</p></td><td><p>$0.20</p></td><td><p>$0.50</p></td><td><p>$0.70</p></td><td><p><a href=\"https://docs.x.ai/docs/pricing\">xAI</a></p></td></tr><tr><td><p>MiniMax M2.5</p></td><td><p>$0.15</p></td><td><p>$1.20</p></td><td><p>$1.35</p></td><td><p><a href=\"https://www.minimax.io/news/minimax-m25\">MiniMax</a></p></td></tr><tr><td><p>MiniMax M2.5-Lightning</p></td><td><p>$0.30</p></td><td><p>$2.40</p></td><td><p>$2.70</p></td><td><p><a href=\"https://www.minimax.io/news/minimax-m25\">MiniMax</a></p></td></tr><tr><td><p>Gemini 3 Flash Preview</p></td><td><p>$0.50</p></td><td><p>$3.00</p></td><td><p>$3.50</p></td><td><p><a href=\"https://ai.google.dev/pricing\">Google</a></p></td></tr><tr><td><p>Kimi-k2.5</p></td><td><p>$0.60</p></td><td><p>$3.00</p></td><td><p>$3.60</p></td><td><p><a href=\"https://platform.moonshot.cn/docs/pricing\">Moonshot</a></p></td></tr><tr><td><p>GLM-5</p></td><td><p>$1.00</p></td><td><p>$3.20</p></td><td><p>$4.20</p></td><td><p><a href=\"https://docs.z.ai/guides/overview/pricing\">Z.ai</a></p></td></tr><tr><td><p>ERNIE 5.0</p></td><td><p>$0.85</p></td><td><p>$3.40</p></td><td><p>$4.25</p></td><td><p><a href=\"https://cloud.baidu.com/doc/WENXINWORKSHOP/s/rlitqm7pi\">Baidu</a></p></td></tr><tr><td><p>Claude Haiku 4.5</p></td><td><p>$1.00</p></td><td><p>$5.00</p></td><td><p>$6.00</p></td><td><p><a href=\"https://www.anthropic.com/pricing\">Anthropic</a></p></td></tr><tr><td><p>Qwen3-Max (2026-01-23)</p></td><td><p>$1.20</p></td><td><p>$6.00</p></td><td><p>$7.20</p></td><td><p><a href=\"https://www.alibabacloud.com/help/en/model-studio/developer-reference/model-pricing\">Alibaba Cloud</a></p></td></tr><tr><td><p>Gemini 3 Pro (≤200K)</p></td><td><p>$2.00</p></td><td><p>$12.00</p></td><td><p>$14.00</p></td><td><p><a href=\"https://ai.google.dev/pricing\">Google</a></p></td></tr><tr><td><p>GPT-5.2</p></td><td><p>$1.75</p></td><td><p>$14.00</p></td><td><p>$15.75</p></td><td><p><a href=\"https://openai.com/pricing\">OpenAI</a></p></td></tr><tr><td><p>Claude Sonnet 4.5</p></td><td><p>$3.00</p></td><td><p>$15.00</p></td><td><p>$18.00</p></td><td><p><a href=\"https://www.anthropic.com/pricing\">Anthropic</a></p></td></tr><tr><td><p>Gemini 3 Pro (&gt;200K)</p></td><td><p>$4.00</p></td><td><p>$18.00</p></td><td><p>$22.00</p></td><td><p><a href=\"https://ai.google.dev/pricing\">Google</a></p></td></tr><tr><td><p>Claude Opus 4.6</p></td><td><p>$5.00</p></td><td><p>$25.00</p></td><td><p>$30.00</p></td><td><p><a href=\"https://www.anthropic.com/pricing\">Anthropic</a></p></td></tr><tr><td><p>GPT-5.2 Pro</p></td><td><p>$21.00</p></td><td><p>$168.00</p></td><td><p>$189.00</p></td><td><p><a href=\"https://openai.com/pricing\">OpenAI</a></p></td></tr></tbody></table><h3><b>What it means for enterprise technical leaders and decision-makers</b></h3><p>With the launch of the Qwen3.5 Medium Models, the rapid iteration and fine-tuning once reserved for well-funded labs is now accessible for on-premise development at many non-technical firms, effectively decoupling sophisticated AI from massive capital expenditure.</p><p>Across the organization, this architecture transforms how data is handled and secured. The ability to ingest massive document repositories or hour-scale videos locally allows for deep institutional analysis without the privacy risks of third-party APIs. </p><p>By running these specialized &quot;Mixture-of-Experts&quot; models within a private firewall, organizations can maintain sovereign control over their data while utilizing native &quot;thinking&quot; modes and official tool-calling capabilities to build more reliable, autonomous agents. </p><p>Early adopters on Hugging Face have specifically lauded the model’s ability to &quot;narrow the gap&quot; in agentic scenarios where previously only the largest closed models could compete.</p><p>This shift toward architectural efficiency over raw scale ensures that AI integration remains cost-conscious, secure, and agile enough to keep pace with evolving operational needs.</p>",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "RAM now represents 35 percent of bill of materials for HP PCs",
      "url": "https://arstechnica.com/gadgets/2026/02/ram-now-represents-35-percent-of-bill-of-materials-for-hp-pcs/",
      "published": "2026-02-26T02:43:26+00:00",
      "summary": "<p>Article URL: <a href=\"https://arstechnica.com/gadgets/2026/02/ram-now-represents-35-percent-of-bill-of-materials-for-hp-pcs/\">https://arstechnica.com/gadgets/2026/02/ram-now-represents-35-percent-of-bill-of-materials-for-hp-pcs/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47161160\">https://news.ycombinator.com/item?id=47161160</a></p> <p>Points: 263</p> <p># Comments: 178</p>",
      "content_text": "In an illustration of the severity of the current memory shortage , HP Inc. CFO Karen Parkhill said that RAM has gone from accounting for “roughly 15 percent to 18 percent” of HP PCs’ bill of materials in its fiscal Q4 2025 to “roughly 35 percent” for the rest of the year. Parkhill was speaking during HP’s Q1 2026 earnings call, where the company said it expects the total addressable market for its Personal Systems business to decline by double digits this calendar year, as higher prices hurt customer demand. “We have seen memory costs increase roughly 100 percent sequentially, and we do forecast that to further increase as we move into the fiscal year,” Parkhill said, per a transcript of the call by Seeking Alpha . HP expects its financials to be most severely impacted by the RAM shortage in the second half of its fiscal year. “We are seeing increased input costs driven primarily by the rising prices of DRAM and NAND,” Bruce Broussard, HP’s interim CEO and director, said. “We expect this volatility to remain throughout fiscal [year 2026] and likely into fiscal [year 2027].” RAM shortage drives higher prices, lower specs HP’s CFO noted that a third of the margin for HP’s Personal Systems business comes from non-RAM-related categories, including IT services and peripherals. However, HP has also raised PC prices to keep making money while paying significantly more for RAM.",
      "cover_image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-2252687789-1152x648.jpg"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "Salesforce CEO Marc Benioff: This isn't our first SaaSpocalypse",
      "url": "https://techcrunch.com/2026/02/25/salesforce-ceo-marc-benioff-this-isnt-our-first-saaspocalypse/",
      "published": "2026-02-26T01:59:12+00:00",
      "summary": "Salesforce reported a solid year-end earnings and then pulled out all the stops to ward off more talk of the death of its business to AI.",
      "content_text": "Salesforce pulled out all the stops to convince investors that the AI revolution won’t be its death when it announced fourth-quarter earnings on Wednesday. Salesforce reported a solid quarter of $10.7 billion in revenue, up 13% year-over-year. For the year, it reported $41.5 billion in revenue, up 10% over the previous year, with both results boosted by its $8 billion acquisition of data management company Informatica last May. Net income landed at $7.46 billion, and the company offered strong guidance for the year ahead, projecting revenue of $45.8 billion to $46.2 billion — a 10% to 11% increase. It also said its “remaining performance obligation,” or RPO, is over $72 billion. That’s a figure that shows revenue under contact that has not yet been delivered or recognized as earned revenue. The numbers, though, could only do so much. Software-as-a-service stocks, with Salesforce as their poster child , have been getting hammered lately. Investors fear the rise of AI agents will undermine these companies, making their per-employee-seat business models obsolete . The situation has been dubbed the “SaaSpocalypse.” The concept hung so heavily in the air during the earnings call that CEO Marc Benioff mentioned the term at least six times. “You’ve heard about the SaaSpocalypse? And it isn’t our first. We’ve had a few of them,” he said, later adding, “If there is a SaaSpocalypse, it may be eaten by the Sasquatch because there are a lot of companies using a lot of SaaS because it just got better with agents.” In an attempt to convince the world of its continued health, Salesforce threw everything and the kitchen sink into this earnings report. The company increased its dividend by nearly 6% to $0.44 per share. It launched a new $50 billion share buyback program. That’s always a favorite with shareholders because it both creates a sturdy buyer of shares and reduces the number of shares in circulation (which can boost the stock price). Techcrunch event Boston, MA | June 9, 2026 The company also revamped the earnings call itself. It was part podcast, part infomercial, and part normal Q&A with a few questions from Wall Street analysts. Instead of running through the numbers, Benioff interviewed three Salesforce customers on camera to testify to their love of its new agentic options: the CEO of home appliance company SharkNinja; the CEO of Wyndham Hotels and Resorts; and, just to hammer the point, the CEO of SaaStr, the software industry conference and media company. We’ll truncate the interviews to the shortest summary: They all love Salesforce’s AI agent products. Salesforce also introduced a new metric for its agentic products: agentic work units (“AWU”). The idea here is that instead of simply counting “tokens” — the standard unit of AI processing volume — AWU attempts to measure something more meaningful: whether an agent actually completed a task. (Salesforce logged 19 trillion tokens last quarter, which sounds like a lot but really is not in the AI world.) “You can ask it a question and it can write you a poem, but that’s not really all that valuable in the enterprise world,” Salesforce president and CMO Patrick Stokes said on the call. So AWU is intended to measure when the agent writes to a record or does some other verifiable piece of work. On top of that, Salesforce also presented its own architectural vision of the coming world of agents. It shows SaaS software like itself owning most of the tech stack , with the AI model makers on the bottom as unseen, interchangeable, and commoditized work engines. This was a direct counter to one of the causes of a SaaSpocalypse sell-off earlier this month, after OpenAI released its enterprise agent, Frontier . OpenAI’s architectural vision shows OpenAI owning most of the stack, with systems-of-record SaaS providers (the databases and business-software platforms where companies store their core data) on the bottom as the unseen engines . And if all that wasn’t enough to influence investors: Benioff was dressed in a black leather jacket, echoing the signature look of the CEO clearly crushing it in the AI world: Nvidia’s Jensen Huang.",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2026/02/GettyImages-2221463618.jpg?resize=1200,800"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Zyora-Dev/zse: Zyora Server Inference Engine for LLM .",
      "url": "https://github.com/Zyora-Dev/zse",
      "published": "2026-02-26T01:15:25+00:00",
      "summary": "<p>I've been building ZSE (Z Server Engine) for the past few weeks — an open-source LLM inference engine focused on two things nobody has fully solved together: memory efficiency and fast cold starts.<p>The problem I was trying to solve: Running a 32B model normally requires ~64 GB VRAM. Most developers don't have that. And even when quantization helps with memory, cold starts with bitsandbytes NF4 take 2+ minutes on first load and 45–120 seconds on warm restarts — which kills serverless and autoscaling use cases.<p>What ZSE does differently:<p>Fits 32B in 19.3 GB VRAM (70% reduction vs FP16) — runs on a single A100-40GB<p>Fits 7B in 5.2 GB VRAM (63% reduction) — runs on consumer GPUs<p>Native .zse pre-quantized format with memory-mapped weights: 3.9s cold start for 7B, 21.4s for 32B — vs 45s and 120s with bitsandbytes, ~30s for vLLM<p>All benchmarks verified on Modal A100-80GB (Feb 2026)<p>It ships with:<p>OpenAI-compatible API server (drop-in replacement)<p>Interactive CLI (zse serve, zse chat, zse convert, zse hardware)<p>Web dashboard with real-time GPU monitoring<p>Continuous batching (3.45× throughput)<p>GGUF support via llama.cpp<p>CPU fallback — works without a GPU<p>Rate limiting, audit logging, API key auth<p>Install:<p>----- pip install zllm-zse zse serve Qwen/Qwen2.5-7B-Instruct For fast cold starts (one-time conversion):<p>----- zse convert Qwen/Qwen2.5-Coder-7B-Instruct -o qwen-7b.zse zse serve qwen-7b.zse # 3.9s every time<p>The cold start improvement comes from the .zse format storing pre-quantized weights as memory-mapped safetensors — no quantization step at load time, no weight conversion, just mmap + GPU transfer. On NVMe SSDs this gets under 4 seconds for 7B. On spinning HDDs it'll be slower.<p>All code is real — no mock implementations. Built at Zyora Labs. Apache 2.0.<p>Happy to answer questions about the quantization approach, the .zse format design, or the memory efficiency techniques.</p> <hr /> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47160526\">https://news.ycombinator.com/item?id=47160526</a></p> <p>Points: 54</p> <p># Comments: 7</p>",
      "content_text": "You can’t perform that action at this time.",
      "cover_image_url": "https://opengraph.githubassets.com/954a24c275fe4b19b3fc859ebc48f6fa1b0a438de9dd0f63d63185ae456040ae/Zyora-Dev/zse"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Tech Companies Shouldn’t Be Bullied Into Doing Surveillance",
      "url": "https://www.eff.org/deeplinks/2026/02/tech-companies-shouldnt-be-bullied-doing-surveillance",
      "published": "2026-02-26T00:37:32+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.eff.org/deeplinks/2026/02/tech-companies-shouldnt-be-bullied-doing-surveillance\">https://www.eff.org/deeplinks/2026/02/tech-companies-shouldnt-be-bullied-doing-surveillance</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47160226\">https://news.ycombinator.com/item?id=47160226</a></p> <p>Points: 267</p> <p># Comments: 86</p>",
      "content_text": "The Secretary of Defense has given an ultimatum to the artificial intelligence company Anthropic in an attempt to bully them into making their technology available to the U.S. military without any restrictions for their use. Anthropic should stick by their principles and refuse to allow their technology to be used in the two ways they have publicly stated they would not support: autonomous weapons systems and surveillance . The Department of Defense has reportedly threatened to label Anthropic a “supply chain risk,” in retribution for not lifting restrictions on how their technology is used. According to WIRED , that label would be, “a scarlet letter usually reserved for companies that do business with countries scrutinized by federal agencies, like China, which means the Pentagon would not do business with firms using Anthropic’s AI in their defense work.” Anthropic should stick by their principles and refuse to allow their technology to be used in the two ways they have publicly stated they would not support: autonomous weapons systems and surveillance . In 2025, reportedly Anthropic became the first AI company cleared for use in relation to classified operations and to handle classified information. This current controversy, however, began in January 2026 when, through a partnership with defense contractor Palantir, Anthropic came to suspect their AI had been used during the January 3 attack on Venezuela . In January 2026, Anthropic CEO Dario Amodei wrote to reiterate that surveillance against US persons and autonomous weapons systems were two “bright red lines” not to be crossed, or at least topics that needed to be handled with “extreme care and scrutiny combined with guardrails to prevent abuses.” You can also read Anthropic’s self-proclaimed core views on AI safety here , as well as their LLM, Claude’s, constitution here . Now, the U.S. government is threatening to terminate the government’s contract with the company if it doesn’t switch gears and voluntarily jump right across those lines. Companies, especially technology companies, often fail to live up to their public statements and internal policies related to human rights and civil liberties for all sorts of reasons, including profit. Government pressure shouldn’t be one of those reasons. Whatever the U.S. government does to threaten Anthropic, the AI company should know that their corporate customers, the public, and the engineers who make their products are expecting them not to cave. They, and all other technology companies, would do best to refuse to become yet another tool of surveillance.",
      "cover_image_url": "https://www.eff.org/files/banner_library/ai-soldiers-3b.png"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "Gushwork bets on AI search for customer leads - and early results are emerging",
      "url": "https://techcrunch.com/2026/02/25/gushwork-bets-on-ai-search-for-customer-leads-and-early-results-are-emerging/",
      "published": "2026-02-26T00:00:00+00:00",
      "summary": "Gushwork has raised $9 million in a seed round led by SIG and Lightspeed. The startup has seen early customer traction from AI search tools like ChatGPT.",
      "content_text": "As AI-powered search tools reshape how businesses are discovered online, India-founded startup Gushwork is helping companies capture customers from platforms such as ChatGPT, Gemini, and Perplexity — with early traction that is beginning to draw investor support. The two-year-old startup said Thursday it had raised $9 million in a seed round led by Susquehanna International Group (SIG) and Lightspeed, with participation from B Capital, Seaborne Capital, Beenext, Sparrow Capital, and 2.2 Capital. The round values Gushwork at $33 million post-money, up from about $7.5 million following its Lightspeed-led $2.1 million pre-seed in July 2023, a person familiar with the matter told TechCrunch. The latest financing brings Gushwork’s total funding to $11 million, the startup said. The funding comes as AI companies, including OpenAI and Perplexity , begin to chip away at traditional web search, prompting incumbents like Google to roll out AI-generated overviews and other conversational features across their search products. Gushwork is betting this shift will create a new opportunity to help businesses surface in AI-driven discovery channels using its automated marketing agents. Founded in 2023 by Nayrhit Bhattacharya (pictured above, right) and Adithya Venkatesh (pictured above, left), Gushwork initially focused on helping small and medium businesses outsource workflows using a mix of AI and human expertise. The startup began narrowing its focus toward search-led marketing after seeing strong customer demand for help with improving online visibility. “When we started, we were focused on helping businesses outsource faster and outsource better,” Bhattacharya told TechCrunch in an interview, adding that the pull around search from customers became increasingly hard to ignore. Gushwork’s platform uses a network of AI agents to automatically generate and update search-optimized content; build backlinks — typically 10 to 20 per customer — through a network of roughly 200 to 300 partner websites; and track inbound leads through an integrated content management system. The goal, Bhattacharya said, is to help businesses surface in both traditional search results and AI-generated answers without relying on large in-house marketing teams. The startup says it has signed up more than 300 paying customers — roughly 95% of them in the U.S. — with subscriptions starting at $800 per month. Gushwork is currently running at about $1.5 million in annualized recurring revenue after rolling out its AI search-focused product around three months ago and is targeting $3 million to $3.5 million ARR in the next three months, Bhattacharya said, adding that the startup is growing about 50% to 80% month over month. Techcrunch event Boston, MA | June 9, 2026 Across Gushwork’s customer base, about 20% of website traffic now comes from AI-driven search and chat platforms, but those sources account for around 40% of inbound leads, Bhattacharya said, citing the startup’s internal data. The higher-intent leads, Bhattacharya said, are already translating into business outcomes for some customers. In one case, a professional services client has closed between $200,000 and $350,000 worth of contracts after adopting the platform, he said, declining to disclose the customer’s name. He added that many users are seeing meaningful pipeline growth as AI-driven discovery gains traction. Gushwork’s customer base today is concentrated among high-ticket B2B service providers, industrial distributors, and contract manufacturers, primarily in the U.S., Bhattacharya said. The startup’s average subscription runs about $800 to $900 per month, or roughly $9,000 to $10,000 in annual contract value, he added. The shift toward AI-driven discovery is still in its early stages but is gaining momentum. Tools such as generative AI chatbots and AI web browsers are increasingly being used by buyers to research vendors and products. OpenAI said in July 2025 that ChatGPT received about 2.5 billion prompts a day globally, including roughly 330 million from U.S. users. Bhattacharya said the trend is beginning to reshape how some businesses approach online visibility. Gushwork plans to use the new funding to expand its engineering team, improve model accuracy, and scale its go-to-market efforts, Bhattacharya said. He added that the startup has more than 800 businesses on its waitlist that it plans to begin onboarding. The startup, headquartered in Delaware with an office in Bengaluru, has about 70 employees in India, along with several contractors.",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2026/02/gushwork-co-founders.jpg?resize=1200,800"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "Anthropic acquires computer-use AI startup Vercept after Meta poached one of its founders",
      "url": "https://techcrunch.com/2026/02/25/anthropic-acquires-vercept-ai-startup-agents-computer-use-founders-investors/",
      "published": "2026-02-25T23:49:19+00:00",
      "summary": "Seattle-based Vercept developed complex agentic tools, including a computer-use agent that could complete tasks inside applications like a person with a laptop would.",
      "content_text": "Anthropic on Wednesday announced that it has acquired Vercept, an AI startup with deep roots to some of the biggest names in Seattle’s tech scene. The acquisition marks the latest after Anthropic acquired coding agent engine Bun in December to help scale Claude Code. Vercept had created tools for more complex agentic tasks, including its product Vy, a computer-use agent in the cloud that could operate a remote Apple MacBook. Vercept is one of the many startups working on re-imagining the personal computer for the age of AI agents. As part of the deal, Anthropic is shuttering Vercept’s product on March 25. The startup was a grad of Seattle’s AI-focused incubato r A12, which spawned from the longstanding Allen Institute for AI. Vercept’s co-founders had roots with the Allen Institute, as well, and were previously researchers there. One co-founder, Matt Deitke, made news last year as one of the AI researchers who negotiated a monster $250 million salary from Meta to join its Superintelligence Lab. On Wednesday, Deitke congratulated his former colleagues in a post on X . Vercept was a relatively high-profile AI startup in the region. In a LinkedIn post announcing the acquisition by Anthropic, Vercept CEO Kiana Ehsani said the startup had raised a total of $50 million. She called out A12’s Seth Bannon, a board member, as the lead investor. Vercept previously announced it had raised a $16 million seed round last January. The list of angel investors was impressive, too, and included former Google CEO Eric Schmidt, Google DeepMind chief scientist Jeff Dean, Cruise founder Kyle Vogt, and Dropbox co-founder Arash Ferdowsi, GeekWire reported. In Anthropic’s announcement of the acquisition, the company named co-founders Ehsani, Luca Weihs, and Ross Girshick as some of the team brought on to join Anthropic in the acquisition. However, not all of Vercept’s co-founders are joining the Claude maker. Oren Etzioni, who has previously been named as a co-founder of Vercept and investor in the startup, is well known in Seattle as the founding leader of the Allen Institute for AI. Along with Deitke, he is also not joining Anthropic, and was vocally less pleased about the acqui-hire. He posted on LinkedIn : “After a little bit more than a year, Vercept is throwing in the towel and giving their customers 30 days to get off the platform. Sad. A fantastic team is joining Anthropic. I wish them the very best!” Techcrunch event Boston, MA | June 9, 2026 Etzioni is also a professor at the University of Washington and known for other startups he’s founded and backed as a VC. He did not respond to a request for comment. On Etzioni’s LinkedIn post, he accused Bannon, the Vercept lead investor, of being “partly responsible” for Vercept not hiring the correct business people. A back and forth ensued between the investors, with Bannon condemning Etzioni’s remarks: “… you disparaged the heroic work of the founders for achieving an outcome most could only dream of,” Bannon replied in the LinkedIn string. They also accused each other of other less savory things like lying and legal threats. While public spats between investors are entertaining, and essentially meaningless, the underlying motivation is notable. The stakes are high to build the next big AI winner, and now a promising startup that raised a decently sized war chest will be tucked into Anthropic. While the terms of the deal were not disclosed, Etzioni says he got a return on his money. Anthropic clearly wanted these researchers (perhaps — especially — with another of them at Meta). Still, Etzioni told GeekWire that he remains bummed. “I’m pleased to have gotten a positive return but obviously disappointed that after just a little over a year with so much traction, and such a fantastic team, we’re basically throwing in the towel,” he said. The founders joining Anthropic, however, appear happy, according to CEO’s Ehsani’s LinkedIn post. “The choices were clear: we could build independently and work toward the same vision as two separate versions of it, or join forces with an incredible team and accelerate that vision into reality. The decision became an easy choice,” she said of joining Anthropic.",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2026/01/anthropic-image-jagmeet-singh-techcrunch.jpg?resize=1200,800"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "New York sues Valve, alleging its loot boxes are ‘quintessential gambling’",
      "url": "https://www.theverge.com/games/884978/valve-lawsuit-loot-boxes-new-york-attorney-general-lawsuit",
      "published": "2026-02-25T23:22:18+00:00",
      "summary": "New York Attorney General Letitia James is suing Valve for \"illegally promoting gambling\" through the loot box systems it has built for video games like Counter-Strike 2, Team Fortress 2, and Dota 2, according to a press release. The attorney general seeks to \"permanently stop Valve from promoting gambling features in its games, disgorge all [&#8230;]",
      "content_text": "New York Attorney General Letitia James is suing Valve for “illegally promoting gambling” through the loot box systems it has built for video games like Counter-Strike 2 , Team Fortress 2 , and Dota 2 , according to a press release . The attorney general seeks to “permanently stop Valve from promoting gambling features in its games, disgorge all ill-gotten gains, and pay fines for violating New York’s laws.” “This loot box model that Valve has developed — charging an individual for a chance to win something of value based on luck alone — is quintessential gambling, prohibited under New York’s Constitution and Penal Law,” the lawsuit says . Valve has made “tens of millions of dollars” selling loot box keys to “thousands” of New York residents and has “made millions of dollars more in commissions from New Yorkers who sold virtual items obtained from loot boxes.” The company’s loot boxes are also “particularly pernicious” because they’re popular with children and adolescents, according to the complaint. Users can purchase keys to open loot boxes in some Valve games and receive randomly-selected virtual items as rewards. If they want, users can then sell those rewards on the Steam Community Market and on third-party marketplaces; the rarer items can be worth “thousands of dollars,” the lawsuit says. These systems, however, require that users pay Valve $2.49 plus tax to open the loot boxes, and users often get items that are “worth less than what the user spent on the key”. The lawsuit also notes that Valve’s experience for opening a loot box in Counter-Strike 2 resembles that of a slot machine. Valve didn’t immediately reply to a request for comment.",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/ss_d830cfd0550fbb64d80e803e93c929c3abb02056.1920x1080.jpg?quality=90&strip=all&crop=0%2C3.4613147178592%2C100%2C93.077370564282&w=1200"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "Nvidia has another record quarter amid record capex spends",
      "url": "https://techcrunch.com/2026/02/25/nvidia-earnings-record-capex-spend-ai/",
      "published": "2026-02-25T23:04:42+00:00",
      "summary": "\"The demand for tokens in the world has gone completely exponential,\" Nvidia CEO Jensen Huang said about the company's earnings.",
      "content_text": "Chip giant and world’s most valuable company Nvidia reported record profits in its most recent quarter on Wednesday, as demand for AI compute continues to skyrocket. “The demand for tokens in the world has gone completely exponential,” CEO Jensen Huang said on a call with analysts following the results. “I think we’re all seeing that, to the point where even our six-year-old GPUs in the cloud are completely consumed and the pricing is going up.” The company reported $68 billion in revenue in the most recent quarter, up 73% from the prior year, with $62 billion of that revenue coming from the company’s data center business. Notably, Nvidia divided the data center revenue into $51 billion in compute revenue (largely GPUs) and $11 billion in networking products like NVLink. The company reported $215 billion in revenue for the full year. As in previous quarters, the company did not report any revenue from chip exports to China, despite the recent lifting of export restrictions by the U.S. government. “While small amounts of H200 products for China-based customers were approved by the U.S. government, they have yet to generate any revenue, and we do not know whether any imports will be allowed into China,” Colette Kress, the company’s chief financial officer, said. “Our competitors in China, bolstered by recent IPOs, are making progress,” she continued, in an apparent reference to Moore Threads’ IPO in December , “and have the potential to disrupt the structure of the global AI industry over the long term.” During the investor call, Huang also addressed the company’s pending investment in OpenAI, which has been reported at $30 billion . Techcrunch event Boston, MA | June 9, 2026 “We continue to work with OpenAI toward a partnership agreement. We believe we are close,” Huang said. He also referenced partnerships with Anthropic, Meta, and Elon Musk’s xAI. However, statements Nvidia filed with the U.S. Securities and Exchange Commission on Wednesday emphasized that there was “no assurance” an investment would take place. Huang also addressed concerns about the sustainability of tech companies’ capex commitments , saying he believed the compute investments would soon bring revenue. “In this new world of AI, compute is revenue. Without compute, there’s no way to generate tokens. Without tokens, there’s no way to grow revenues,” Huang said. “We’ve reached the inflection point and we’re generating profitable tokens that are productive for customers and profitable for the cloud service providers”",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2219673294.jpg?resize=1200,750"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "http://info.cern.ch",
      "url": "https://info.cern.ch",
      "published": "2026-02-25T23:02:58+00:00",
      "summary": "<p>Article URL: <a href=\"https://info.cern.ch\">https://info.cern.ch</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47159302\">https://news.ycombinator.com/item?id=47159302</a></p> <p>Points: 221</p> <p># Comments: 58</p>",
      "content_text": "http://info.cern.ch - home of the first website From here you can:",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "How will OpenAI compete?",
      "url": "https://www.ben-evans.com/benedictevans/2026/2/19/how-will-openai-compete-nkg2x",
      "published": "2026-02-25T22:29:25+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.ben-evans.com/benedictevans/2026/2/19/how-will-openai-compete-nkg2x\">https://www.ben-evans.com/benedictevans/2026/2/19/how-will-openai-compete-nkg2x</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47158975\">https://news.ycombinator.com/item?id=47158975</a></p> <p>Points: 237</p> <p># Comments: 298</p>",
      "content_text": "As we all know, OpenAI has been running around trying to join the club, claiming a few months ago to have $1.4tr and 30 gigawatts of compute commitment for the future (with no timeline), while it reported 1.9 gigawatts in use at the end of 2025. Since it doesn’t have the scale of cashflows from existing businesses that the hyperscalers can use, it has so far managed to do this, or at least announce this, with a combination of capital-raising (not all of which has necessarily closed ) and other peoples balance sheets (some of which is also the famous ‘circular revenue’). You can watch plenty of three-hour podcasts discussing all of this, and plenty of people have opinions about TPUs, Nvidia’s product lead, and Oracle’s strategy of borrowing against a declining but cash-generative legacy business to burn its way into the new thing, but how much should the rest of us care? Is this a path to a competitive advantage, or just a seat at the table? We don’t really know what AI infrastructure costs will look like in the long term, but it’s quite possible that this turns out like the manufacture of airliners or semiconductors: there are no network effects, but with each generation the process gets more difficult and more expensive, and so those industries have gone from dozens of companies at the cutting edge to just Boeing and Airbus on one hand and TSMC on the other. Semiconductor manufacturing had both Moore’s Law, which everyone has heard of, and Rock’s Law, which most people haven’t: Moore’s Law said that the number of transistors on a chip was doubling every two years, but Rock’s Law said that the cost of a state-of-the-art semiconductor fab was doubling every four years. Maybe generative AI will work the same, with unit costs falling but fixed costs rising to the point that only a handful of companies are able to sustain the investment needed to build competitive models and everyone else is squeezed out.* This oligopoly would presumably have a price equilibrium, though it might be at high or low margins - this might all just be commodity infrastructure sold at marginal cost, especially given some of those at the table will be using their models to power other, much more differentiated businesses. Ask your favourite economist. ** So, when Sam Altman says he’s raised $100bn or $200bn, and when he says he’d like OpenAI to be building a gigawatt of compute every week (implying something in the order of a trillion dollars of annual capex), it would be easy to laugh at this as ‘braggawatts’, and apparently people at TSMC once dismissed him as ‘podcast bro’, but he’s trying to create a self-fulfilling prophecy. He’s trying to get OpenAI, a company with no revenue three years ago, a seat at a table where you’ll probably need to spend couple of hundred billion dollars a year on infrastructure, through force of will. His force of will has turned out to be pretty powerful so far. But, again, does that get you anything more than a seat at that table? TSMC isn’t just an oligopolist - it has a de facto monopoly on cutting edge chips - but that gives it little to no leverage or value-capture further up the stack. People built Windows apps, web services and iPhone apps - they don’t build TSMC apps or Intel apps. Developers had to build for Windows because it had almost all the users, and users had to buy Windows PCs because it had almost all the developers (a network effect!). But if you invent a brilliant new app or product or service using generative AI, or add it as a feature to an existing product, you use the APIs to call a foundation model running in the cloud and the users don’t know or care what model you used. No-one using Snap cares if it runs on AWS or GCP. When you buy an enterprise SaaS product you don’t care if it uses AWS or Azure. And if I do a Google Search and the first match is a product that’s running on Google Cloud, I would never know. That doesn’t mean these APIs are interchangeable - there are good reasons why AWS, GCP and Azure have very different market shares, and why developers choose each. But the customer doesn’t know or care. Running a cloud doesn’t give you leverage over third part products and services that are further up the stack. The difference now, perhaps, is that all of those services were separate silos: there was a common search and discovery layer at the top in Google and Facebook, and common infrastructures at the bottom in the cloud, but all those apps were never connected to each other. Now we have an emerging alphabet soup of standards and protocols for models and websites to talk to each other across ads, e-commerce and some kind of intent and automation (the brief enthusiasm around OpenClaw captured some of this). A website can surface its capabilities so that a subset can just show up in ChatGPT, be it a real estate search or a shopping cart. You’ll tell your agent to look at a recipe on Instagram and order the ingredients on Instacart. Everything can get piped to everything else, and everything can talk to each other! Meanwhile, (saying the quiet part out loud), if you could set and control those APIs and manage the flows, that gives you power. Standards have been a basic competitive weapon in every generation of technology - remember Microsoft’s slogan ‘embrace and extend’. In particular, OpenAI suggests now suggests you’ll use your ChatGPT account as the glue linking all of these together. That’s a network effect! I'm not sure about this: I’m not sure that this vision will really work, and if it does, I’m not sure it gives one company dominance. First, there’s a recurring fallacy in tech that you can abstract many different complex products into a simple standard interface - you could call this the ‘widget fallacy’. A decade ago people said ‘APIs are the new BD’, which was really the same concept, and it mostly failed. This is partly because there’s a huge gap between what looks cool in demos and all of the work and thought in the interaction models and the workflows in the actual product: very quickly you’ll run into an exception case and you’ll need the actual product UI and a human decision. It’s also because the incentives are misaligned: no-one wants to be someone else’s dumb API call, so there’s an inherent tension or trade-off between the distribution that an abstraction layer might give you (Google Shopping, Facebook shopping, and now ChatGPT shopping) and your desire to control the experience and the customer relationship. Remember, after all, that all of Instacart’s profits come from showing ads. Of course, this is just speculation - maybe it will all work this time! But the second problem is that if these are all separate systems plugged together by abstracted and automated APIs, is the user or developer locked into any one of them? If apps in the chatbot feed work, and OpenAI uses one standard and Gemini uses another, why stops a developer doing both? This is much less code than making both an iOS and Android app, and anyway, can’t you get the AI to write the code for you? What does that do to developer lock-ins? Meanwhile, yes, maybe I’ll log into all of these services with my OpenAI or Gemini account, but does it necessarily make sense for me to log into Tinder, Zillow and Workday with the same account? And, again, do they want that? Hmm. As I've written this essay, I’ve returned again and again to terms like platform, ecosystem, leverage and network effect. These terms get used a lot in tech, but they have pretty vague meanings. Google Cloud, Apple's App Store, Amazon Marketplace, and even TikTok are all ‘platforms’ but they're all very different. Maybe the word I'm really looking for is power. When I was at university, a long time ago now, my medieval history professor, Roger Lovatt, told me that power is the ability to make people do something that they don't want to do, and that's really the question here. Does OpenAI have the ability to get consumers, developers and enterprises to use its systems more than anybody else, regardless of what the system itself actually does? Microsoft, Apple and Facebook had that. So does Amazon - this is a real flywheel. ***",
      "cover_image_url": "http://static1.squarespace.com/static/50363cf324ac8e905e7df861/t/6993b59b8517240d2235240a/1771507646532/Frontier%2BLanguage%2BModel%2BIntelligence%2C%2BOver%2BTime%2B%2824%2BJan%2B%2726%29%2B-2.png.webp?format=1500w"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "How the new Galaxy S26 phones compare",
      "url": "https://www.theverge.com/gadgets/883733/samsung-galaxy-s26-vs-plus-ultra-specs-features-hardware-comparison",
      "published": "2026-02-25T22:26:31+00:00",
      "summary": "Samsung has just announced its new Galaxy S26 lineup, which includes the S26, S26 Plus, and S26 Ultra. While they aren't radical departures from last year's models, they bring a handful of notable upgrades. All three run on Qualcomm's Galaxy-centric Snapdragon 8 Elite Gen 5, which delivers improved performance and powers a slew of new [&#8230;]",
      "content_text": "Samsung has just announced its new Galaxy S26 lineup , which includes the S26, S26 Plus, and S26 Ultra. While they aren’t radical departures from last year’s models, they bring a handful of notable upgrades. All three run on Qualcomm’s Galaxy-centric Snapdragon 8 Elite Gen 5, which delivers improved performance and powers a slew of new AI-based features. This includes the ability to screen unknown calls and edit photos by typing what you want changed, along with an update to Google Gemini that can carry out certain tasks in supported third-party apps, like Uber and DoorDash, on your behalf. Starting at $899.99, the S26 is the smallest and most affordable of the trio. It features a 6.3-inch OLED display, weighs just under 6 ounces, and has a 4,300mAh battery. Its rear cameras include a 50-megapixel main sensor, a 12-megapixel ultrawide, and a 10-megapixel telephoto lens. Like its siblings, it supports a 120Hz refresh rate, starts with 12GB of RAM and 256GB of storage, and includes wireless charging. The $1,099.99 S26 Plus, meanwhile, retains the same features but adds a larger 6.7-inch display and a bigger 4,900mAh battery, which in theory means it could last longer on a single charge than the S26. Still, we’d need to test both phones to see how they compare in reality, especially given the larger display on the Plus model. Then there’s the Galaxy S26 Ultra, which starts at $1,299.99. It’s the largest and most feature-rich phone in the lineup — and the heaviest, weighing close to half a pound. Its 6.9-inch display feels closer to an e-reader’s, and the 5,000mAh battery is the largest of the three phones. Its camera system is the most advanced, as well, offering a 200-megapixel main sensor, a 50-megapixel ultrawide, and two dedicated telephoto lenses with 3x and 5x optical zoom so you can zoom in farther without losing image quality. The Ultra is also the only phone to offer Samsung’s new privacy display feature , which makes it harder for people sitting next to you to see what’s on your screen, if you don’t want them to. It’s also the only model with S Pen support for taking notes. That’s just a glimpse of the main differences between the three models, though. If you want a more detailed look at the hardware defining each phone, the table below breaks down the specs. We’ve also published our hands-on impressions of the Galaxy S26 , S26 Plus , and S26 Ultra for those who want more insight ahead of our full reviews.",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/Samsung-Galaxy-S26-Plus-Lifestyle-Image.jpg?quality=90&strip=all&crop=0%2C10.702825294955%2C100%2C78.59434941009&w=1200"
    },
    {
      "industry": "technology",
      "source": "Ars Technica",
      "title": "Musk has no proof OpenAI stole xAI trade secrets, judge rules, tossing lawsuit",
      "url": "https://arstechnica.com/tech-policy/2026/02/judge-xai-cant-claim-openai-stole-trade-secrets-just-by-hiring-ex-staffers/",
      "published": "2026-02-25T22:09:21+00:00",
      "summary": "Even twisting an ex-employee's text to favor xAI's reading fails to sway judge.",
      "content_text": "But this claim hinges entirely upon xAI proving that OpenAI poached its employees to steal its trade secrets. So, for xAI’s lawsuit to proceed, xAI will need to beef up the evidence base for its other claim, that OpenAI has violated the federal Defend Trade Secrets Act, Lin said. To succeed on that, xAI must prove that OpenAI unlawfully acquired, disclosed, or used a trade secret with xAI’s consent. That will likely be challenging because xAI, at this point, has not offered “any nonconclusory allegations that OpenAI itself acquired, disclosed, or used xAI’s trade secrets,” Lin wrote. All xAI has claimed is that OpenAI induced former employees to share secrets, and so far, nothing backs that claim, Lin said. Tishler noted that the court also rejected an xAI theory that “OpenAI should be responsible for what its new hires did before they arrived” for “the same reason: without evidence that OpenAI directed the theft or actually put the stolen information to use, you cannot hold the company liable.” The strongest evidence that xAI had of employee misconduct, allegedly allowing OpenAI to misappropriate xAI trade secrets, revolves around the departure of one of xAI’s earliest engineers, Xuechen Li. That evidence wasn’t enough, Lin said. xAI alleged that Li gave a presentation to OpenAI that supposedly included confidential information. Li also uploaded “the entire xAI source code base to a personal cloud account,” which he had connected to ChatGPT, Lin noted, after a recruiter sent a message on Signal sharing a link with Li to another unrelated cloud storage location. xAI hoped the Signal messages would shock the court, expecting it to read through the lines the way xAI did. As proof that OpenAI allegedly got access to xAI’s source code, xAI pointed to a Signal message that an OpenAI recruiter sent to Li “four hours after” Li downloaded the source code, saying “nw!” xAI has alleged this message is short-hand for “no way!”—suggesting the OpenAI recruiter was geeked to get access to xAI’s source code. But in a footnote, Lin said that “OpenAI insists that ‘nw’ means ‘no worries,’” and thus is unconnected to Li’s decision to upload the source code to a ChatGPT-linked cloud account.",
      "cover_image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-2259422080-1024x648.jpg"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "Corsair is halting Drop sales after March 25th",
      "url": "https://www.theverge.com/news/884824/corsair-ending-drop-shopping-site",
      "published": "2026-02-25T21:58:53+00:00",
      "summary": "The Drop store, which was acquired by gaming gear giant Corsair in 2023, was a haven for mechanical keyboard enthusiasts and audiophiles to discover and buy hard-to-find gear - sometimes at surprisingly good prices. The company will cease sales after March 25th at 11:59PM PT, which is also the cut off to redeem Drop Rewards. [&#8230;]",
      "content_text": "The Drop store, which was acquired by gaming gear giant Corsair in 2023 , was a haven for mechanical keyboard enthusiasts and audiophiles to discover and buy hard-to-find gear — sometimes at surprisingly good prices. The company will cease sales after March 25th at 11:59PM PT, which is also the cut off to redeem Drop Rewards. After March 31st, the site will be accessible for a limited time to view order history, but all retail functionality will shift to Corsair. Corsair’s marketing manager Andrew Williams confirmed to The Verge that several products featured on Drop will be integrated into Corsair’s site for purchase. Select others will be sold on Amazon and Best Buy. Other products will be end-of-life (EOL), particularly hardware collaborations including other companies. When asked if products like Sennheiser or Koss headphone collaborations would move over to Corsair, Williams didn’t confirm or deny on a product-specific level, but stated that “almost all, if not all, of those will end up not being moved over.” He pointed out that Corsair, as a brand, has its own headsets and other products. However, Williams said “the main ones that are sell well and do well, we’re going to continue on Corsair.” The Drop team was integrated into Corsair when it was acquired, according to Williams, and the company claims that no layoffs are happening as a result of this decision to wind down the site. Corsair shared that existing orders, including preorders, will be fulfilled as planned, and that the company will honor existing warranties on products.",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/chorus/uploads/chorus_asset/file/23195124/jporter_220121_4983_0002.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200"
    },
    {
      "industry": "technology",
      "source": "Ars Technica",
      "title": "The Galaxy S26 is faster, more expensive, and even more chock-full of AI",
      "url": "https://arstechnica.com/gadgets/2026/02/samsung-reveals-galaxy-s26-lineup-with-privacy-display-and-exclusive-gemini-smarts/",
      "published": "2026-02-25T21:41:27+00:00",
      "summary": "Samsung's Galaxy S26 series is available for preorder today and ships on March 11.",
      "content_text": "There used to be countless companies making flagship Android phones, but a combination of factors has narrowed the field over time. Today, Samsung is the undisputed king of the Android device ecosystem with its Galaxy S line. So we can safely assume today’s Unpacked has revealed the most popular Android phones for the next year—the Galaxy S26 Ultra, Galaxy S26+, and Galaxy S26. Samsung didn’t swing for the fences this time around, producing phones with a few cosmetic tweaks and upgraded internals. Meanwhile, Samsung is investing even more in AI, saying the S26 series includes the first “Agentic AI phones.” Despite limited hardware upgrades, the realities of component prices in the age of AI mean the prices of the two cheaper models have gone up by $100 this year. The Ultra remains at an already eye-watering $1,300. Faster and more private Looking at the Galaxy S26 family, you’d be hard-pressed to tell them apart from last year’s phones. The camera surround is different, and the measurements of the smallest and largest phone are ever so slightly different. You probably won’t be able to tell just by looking, but the S26 Ultra has regressed from titanium to aluminum, a reversion Apple also made with its latest high-end phones. This phone also retains its S Pen stylus. Specs at a glance: Samsung Galaxy S26 series Galaxy S26 ($900) Galaxy S26+ ($1,100) Galaxy S26 Ultra ($1,300) SoC Snapdragon 8 Elite Gen 5 (3 nm) Snapdragon 8 Elite Gen 5 (3 nm) Snapdragon 8 Elite Gen 5 (3 nm) Memory 12GB 12GB 12GB, 16GB Storage 256GB, 512GB 256GB, 512GB 256GB, 512GB, 1TB Display 6.3-inch OLED, 10-bit color, 2340×1080, 1-120Hz 6.7-inch OLED, 10-bit color, 3120×1440, 1-120Hz 6.9-inch OLED, 10-bit color, 3120×1440, 1-120Hz, S Pen support Cameras 50MP primary, f/1.8, 1.0 μm; 12MP ultrawide, f/2.2, 1.4 μm, 10MP 3x telephoto, f/2.4, 1.0 μm; 12MP selfie, f/2.2, 1.12 μm 50MP primary, f/1.8, 1.0 μm; 12MP ultrawide, f/2.2, 1.4 μm, 10MP 3x telephoto, f/2.4, 1.0 μm; 12MP selfie, f/2.2, 1.12 μm 200MP primary, f/1.4, 0.6 μm; 50MP ultrawide, f/1.9, 0.7 μm; 10MP 3x telephoto, f/2.4, 1.12 μm; 50MP 5x telephoto, f/2.9, 0.7 μm; 12MP selfie, f/2.2, 1.12 μm Software Android 16 Android 16 Android 16 Battery 4,300 mAh 4,900 mAh 5,000 mAh Connectivity Wi-Fi 7, Bluetooth 5.4, USB-C 3.2, Sub6 5G Wi-Fi 7, Bluetooth 5.4, USB-C 3.2, Sub6 and mmWave 5G Wi-Fi 7, Bluetooth 5.4, USB-C 3.2, Sub6 and mmWave 5G Measurements 71.7×149.6×7.2 mm, 167g 75.8×158.4×7.3 mm, 190g 78.1×163.6×7.9 mm, 214 g These phones will again have the latest Snapdragon flagship processor (in North America, Japan, and China) with customizations exclusive to Samsung. The Snapdragon 8 Elite Gen 5 for Galaxy is a 3 nm chip with third-gen Oryon CPU cores, an Adreno 840 GPU, and a powerful Hexagon NPU for on-device AI processing. Samsung promises double-digit performance gains across the board, which is what we hear every year. Samsung flagship phones have extremely fast hardware, so they benchmark well. However, they also tend to heat up and throttle quickly during sustained use. Perhaps that won’t be as much of a problem with the S26 series. Samsung says it has implemented its largest vapor chamber ever to better control temperatures.",
      "cover_image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/Samsung-Mobile-Galaxy-Unpacked-2026-Galaxy-S26-Series-A-First-Look_dl2-1152x648.jpg"
    },
    {
      "industry": "technology",
      "source": "MIT Technology Review",
      "title": "Roundtables: Why 2026 Is the Year for Sodium-Ion Batteries",
      "url": "https://www.technologyreview.com/2026/02/25/1132873/roundtables-why-2026-is-the-year-for-sodium-ion-batteries/",
      "published": "2026-02-25T21:15:27+00:00",
      "summary": "Listen to the session or watch below Sodium-based batteries could be a cheaper, safer alternative to lithium-ion, and the technology is finally making its way into cars—and energy storage arrays on the grid. Sodium-ion batteries are one of MIT Technology Review&#8217;s 10 Breakthrough Technologies of 2026 list, and this subscriber-only discussion explains why. Watch a&#8230;",
      "content_text": "Listen to the session or watch below Sodium-based batteries could be a cheaper, safer alternative to lithium-ion, and the technology is finally making its way into cars—and energy storage arrays on the grid. Sodium-ion batteries are one of MIT Technology Review's 10 Breakthrough Technologies of 2026 list, and this subscriber-only discussion explains why.",
      "cover_image_url": "https://wp.technologyreview.com/wp-content/uploads/2026/02/MITTR-Roundtables-Zoom-Opening-Overlay.png?resize=1200,600"
    },
    {
      "industry": "technology",
      "source": "Wired",
      "title": "Everyone Speaks Incel Now",
      "url": "https://www.wired.com/story/everyone-speaks-incel-now/",
      "published": "2026-02-25T20:59:50+00:00",
      "summary": "After migrating from misogynist forums to social media feeds, terms like “looksmaxxing” and “mogged” are now impossible to avoid.",
      "content_text": "At the beginning of the year, The Cut kicked off a brief discourse cycle by declaring a new lifestyle trend: “ friction-maxxing .” The idea, in a nutshell, is that people have overconvenienced themselves with apps , AI , and other means of near-instant gratification—and would be better off with increased friction in their daily lives, which is to say those mundane challenges that ask some minor effort of them. Whatever your feelings on that philosophy, the use of “maxxing” as a suffix assumed to be familiar or at least intelligible to most readers of a mainstream news outlet is evidence of another trend: the assimilation of incel terminology across the broader internet . The online ecosystem of incels, or “involuntarily celibate” men, is saturated with this sort of clinical jargon; its aggrieved participants insulate, isolate, and identify themselves through in-group codespeak that is meant to baffle and repel outsiders. So how did non-incels (“normies,” as incels would label them) end up adopting and recontextualizing these loaded words? Slang, no matter its origins, has a viral nature. It tends to break containment and mutate. The buzzword “woke,” as it pertains to our current politics, comes from African American Vernacular English and once referred to an awareness of racial and social injustice—this usage dates to the middle of the 20th century , preceding even the civil rights movement. But the culture wars of this century have turned “woke” into a favorite pejorative of right-wingers, who wield it as a catchall term for anything that threatens their ideology, such as Black pilots or gender-neutral pronouns . Back in 2014, the eruption of the Gamergate harassment campaign set the stage for a different linguistic realignment. An organized backlash to women working in the video game industry, and eventually any sort of diversity or progressivism within the medium, it exposed a vein of reactionary anger that would gain a fuller voice during Donald Trump ’s 2016 presidential campaign. This was a period when many in the digital mainstream got their first taste of the trollish nihilism and invective that fuels toxic message boards such as 4chan and gave rise to a network of anti-feminist manosphere sites collectively known as the “PSL” community: PUAHate (a board for venting about pickup artists, it was shut down soon after the 2014 Isla Vista killing spree carried out by Elliot Rodger, who frequented the forum), SlutHate (a straightforward misogyny hub), and Lookism (where incels viciously critique each other’s appearance). Lookism, named for the idea that prejudice against the less attractive is as common and pernicious as sexism or racism, is the only forum of the PSL trifecta that survives today, and while we don’t know who coined the “maxxing” idiom, it’s the likeliest source for the first verb with this construction. “Looksmaxxing,” which borrows from the role-playing game concept of “ min-maxing ,” or elevating a character’s strengths while limiting weaknesses, became the preferred expression for attempts to improve one’s appearance in pursuit of sex. This could mean something as simple as a style makeover or as extreme as “ bonesmashing ,” a supposed technique of achieving a more defined jaw by tapping it with a hammer. If the 2000s introduced people to pickup lingo like “game” and “negging,” the 2010s ushered in language that extended the Darwinian vision of the dating pool as a cutthroat and strictly hierarchical marketplace. “AMOG,” an initialism for “alpha male of the group,” gave us “mogging,” a display where one man flexes his physical superiority over a rival. An ideally masculine specimen might also be recognized as a “Chad,” who allegedly enjoys his pick of attractive partners, while a Chad among Chads is, of course, a “Gigachad.” Women were disparaged as “female humanoids,” then “femoids,” and finally just “foids.”",
      "cover_image_url": "https://media.wired.com/photos/69964e836a79d569b06431c0/191:100/w_1280,c_limit/Mog-Goblins-and-Why-We-All-Use-Incel-Speak-Now-Culture.jpg"
    },
    {
      "industry": "technology",
      "source": "Ars Technica",
      "title": "Judge doesn't trust DOJ with search of devices seized from Wash. Post reporter",
      "url": "https://arstechnica.com/tech-policy/2026/02/judge-doesnt-trust-doj-with-search-of-devices-seized-from-wash-post-reporter/",
      "published": "2026-02-25T20:53:54+00:00",
      "summary": "Court to search devices itself instead of letting government have full access.",
      "content_text": "Judge should have gone further, press group says Even without being aware of the PPA, the court did not approve the Natanson warrant right away. Porter’s order said the court rejected the government’s first two requests for a search warrant because they were too broad. The court was “concerned about both the scope of the proposed search warrant and the government’s apparent attempt to collect information about Ms. Natanson’s confidential sources,” he wrote. The search warrant ultimately approved by the court was limited to information that Natanson received from Aurelio Luis Perez-Lugones and information related to Perez-Lugones that could be evidence in the case against him. “The government expressly alleged that Ms. Natanson received classified information from Mr. Perez-Lugones,” but its search warrant application did not say whether Natanson herself was a target of the criminal investigation, Porter wrote. “The Court learned that Ms. Natanson was not a focus of the investigation only through press reports published the day the warrant was executed,” he wrote. Porter said the court has to take seriously the government’s claim that the case “involves top secret national security information,” even though the court doesn’t know whether disclosure of the information would cause harm. “The Court takes the government at its word, while acknowledging the well-documented concern that the government has at times overclassified information to avoid embarrassing disclosures rather than to protect genuine secrets,” he wrote. The Freedom of the Press Foundation said that “Judge Porter was right to treat the seizure as a prior restraint and to limit the government from fishing through the irrelevant data it seized to snoop on reporters,” and right to reprimand prosecutors for the omission in their search warrant application. But the order didn’t go far enough, the foundation said. “Judge Porter should have required all of Natanson’s materials seized pursuant to the deceptive warrant application to be returned to her,” the group said. “And he should not have credited the administration’s claims that any of the seized materials posed a national security threat without strict proof—as Judge Porter acknowledged, this administration, even more so than others, has a long track record of falsely claiming national security threats to protect itself from embarrassment and further its political agenda. It has earned zero deference from the judiciary on claims of national security threats, particularly when press freedom is at stake.”",
      "cover_image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/washington-post-building-1152x648-1768424038.jpg"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "The Peace Corps is recruiting volunteers to sell AI to developing nations",
      "url": "https://www.theverge.com/policy/884625/peace-corps-tech-promote-american-ai",
      "published": "2026-02-25T20:45:00+00:00",
      "summary": "For more than six decades, the Peace Corps has represented itself as an agency focused on helping underserved communities around the globe. But a new initiative, called the \"Tech Corps,\" threatens to unravel the agency's original mission by recruiting de facto Silicon Valley salespeople to promote the biggest names in AI - many of which [&#8230;]",
      "content_text": "For more than six decades, the Peace Corps has represented itself as an agency focused on helping underserved communities around the globe. But a new initiative, called the “Tech Corps,” threatens to unravel the agency’s original mission by recruiting de facto Silicon Valley salespeople to promote the biggest names in AI — many of which have ties to President Donald Trump. Established by President John F. Kennedy in 1961, the Peace Corps recruited skilled Americans interested in assisting developing countries in industries like education, healthcare, and agriculture. As noted by the Brookings Institution , the agency was created to “win the hearts and minds” of countries not aligned with the US during the Cold War. Now, the version of diplomacy it will be pushing is the vision of America-made AI tools in a bid to “enhance opportunity and prosperity” in developing countries. As noted on the Tech Corps website , the program will recruit volunteers to “support last-mile adoption of American AI.” Qualifications are broad; the Tech Corps says volunteers must have an associate’s or bachelor’s degree in science, technology, engineering, or mathematics, or relevant work experience. It will place volunteers based on requests from countries in the American AI Exports Program , which is supposed to help foreign businesses “partner with or buy American AI.” One Tech Corps assignment example describes volunteers helping to integrate an AI-powered healthcare system into a local hospital, train staff, and develop privacy protocols. Another describes volunteers working with a country’s ministry of education to “identify gaps in student, teacher, and parent services where AI education tools could be most impactful.” Kelsey Quinn, a project lead and analyst of tech sovereignty and security at the New Lines Institute, tells The Verge that while “it’s not entirely unusual for the Peace Corps to wade into the field of technology,” it’s the “commercial structure” of the Tech Corps that’s different. “This program deploys volunteers to support specific adoption of American AI products that countries have purchased, not just generally increase digital literacy as a skill,” Quinn says. Some of the Peace Corps’ previous tech initiatives have involved teaching STEM skills to girls in Zambia , Thailand , and Albania , as well as offering communication technology training in Vanuatu. But the Tech Corps ties its aid directly to the American AI systems procured by developing countries, as the program’s launch date hinges on the first sales made through the American AI Exports Program, according to its website. Just like the American AI Exports Program, the Tech Corps just seems like another boon to the AI industry . In between dinners with tech CEOs and their donations to a gilded White House ballroom , Trump stood behind OpenAI, Oracle, and SoftBank’s plans to build several data centers across the US. Trump has also signed off on an executive order to discourage states from passing laws to regulate AI. At the same time, Trump has dramatically altered the US government’s system for providing assistance abroad. Last year, the Department of Government Efficiency dismantled the US Agency for International Development , a move that has already led to the deaths of hundreds of thousands of people from infectious diseases and malnutrition, according to the Harvard T.H. Chan School of Public Health . A report from The Atlantic reveals that the Trump administration has plans to cut off funding for seven African nations, while directing funding away from two others. “These Tech Corps recruits will function as on-the-ground promoters for US tech” Questions remain about whether the Tech Corps will even accomplish its goal. China has already laid the groundwork for boosting the adoption of its own AI systems through the country’s Digital Silk Road initiative , which brings Chinese technologies to developing nations, such as Egypt, Zambia, Pakistan, Serbia, Ecuador, and many others. “These Tech Corps recruits will function as on-the-ground promoters for the US tech in these emerging markets where China has already maintained, if not widened, its lead in marketing and in promotion,” Meicen Sun, an assistant professor at the University of Illinois Urbana-Champaign and MIT FutureTech affiliate, tells The Verge . Chinese AI models also have an advantage when it comes to running in areas that don’t have sprawling data centers and the power grid to support more demanding systems. Quinn says these models are already “gaining traction in developing economies because they’re simply just cheaper and can run on local infrastructure.” Microsoft researchers also recently found that the popularity of AI models made by DeepSeek — a Chinese company that develops powerful, efficient AI systems — has surged in Iran, Cuba, and Belarus, as well as across Africa, where Microsoft notes that the China-based Huawei “actively promoted and deployed the platform.” Quinn says failure is “entirely possible” for the Tech Corps, as drastic cuts to aid and reductions in the Bureau of Cyberspace and Digital Policy put it on a “weak institutional foundation.” That, coupled with its ties to the American AI Exports Program, could end up driving countries away. “This combination may very well make target countries suspicious of the Tech Corps and ironically promote more hedging behavior from target countries, the exact opposite of what the administration wants.” And this administration’s primary objective is pretty clear: to make Big Tech partners happy. Follow topics and authors from this story to see more like this in your personalized homepage feed and to receive email updates. Emma Roth Policy Politics Report",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/268374_The_Peace_Corps_is_recruiting_volunteers_to_sell_AI_CVirginia2.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "The White House wants AI companies to cover rate hikes. Most have already said they would.",
      "url": "https://techcrunch.com/2026/02/25/the-white-house-wants-ai-companies-to-cover-rate-hikes-most-have-already-said-they-would/",
      "published": "2026-02-25T20:42:14+00:00",
      "summary": "Many hyperscalers have already made public commitments to cover electricity cost increases.",
      "content_text": "The proliferation of AI data centers plugging into the national electrical grid has helped increase consumer electricity prices, driving up the average national electricity price by more than 6% in the last year. That’s not a good look for the incumbents ahead of this fall’s elections, and President Donald Trump addressed the challenge in his State of the Union speech last night. “We’re telling the major tech companies that they have the obligation to provide for their own power needs,” Trump said. “They can build their own power plants as part of their factory, so that no one’s prices will go up.” The hyperscalers in question don’t need to be told. They have already made public commitments in recent weeks to cover electricity costs by building their own power sources, paying higher rates, or both, part of a broader effort to solve PR problems around data center expansion and win over skeptical communities. On January 11, Microsoft announced its policy “to ensure that the electricity cost of serving our datacenters is not passed on to residential customers.” On January 26, OpenAI committed to “paying its own way on energy, so that our operations don’t increase your energy prices.” On February 11, Anthropic made the same pledge to “cover electricity price increases that consumers face from our data centers.” Yesterday, Google announced the largest battery project in the world to support a data center in Minnesota. What these commitments mean in practice, and who will determine which data centers are responsible for which price increases, remains unknown. The White House has not released the text of the proposed pledge. “A handshake agreement with Big Tech over data center costs isn’t good enough,” Arizona Democratic Senator Mark Kelly said on social media. “Americans need a guarantee that energy prices won’t soar and communities have a say.” Techcrunch event Boston, MA | June 9, 2026 White House spokesperson Taylor Rodgers said that next week, companies will send representatives to formally sign the pledge at the White House. Amazon, Google, Meta, Microsoft, xAI, Oracle, and OpenAI are reportedly among those set to attend. However, none of the companies have confirmed their attendance. Even if tech companies commit to taking on electricity costs, on-site power plants may not be a panacea — they can still have adverse impacts on the surrounding environment , and will stress supply chains for natural gas, turbines, photovoltaics, and batteries, depending on how companies aim to power their compute.",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2025/08/GettyImages-692150220.jpeg?resize=1200,800"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "Trump claims tech companies will sign deals next week to pay for their own power supply",
      "url": "https://www.theverge.com/science/884191/ai-data-center-energy-state-of-the-union-trump",
      "published": "2026-02-25T20:37:25+00:00",
      "summary": "President Donald Trump tried to quell Americans' concerns about rising electricity costs during his State of the Union speech - and now we're learning that the deals he promised could land next week. Trump claimed that he's negotiated a \"rate payer protection pledge\" with major tech companies, which would see them build out or pay [&#8230;]",
      "content_text": "President Donald Trump tried to quell Americans’ concerns about rising electricity costs during his State of the Union speech — and now we’re learning that the deals he promised could land next week. Trump claimed that he’s negotiated a “rate payer protection pledge” with major tech companies, which would see them build out or pay for new electricity generation for their data centers. Leaders from Amazon, Google, Meta, Microsoft, xAI, Oracle and OpenAI are expected to attend a March 4th event to sign the pledge, Fox News reported today . There are very few details at this point on what the pledge entails, nor how companies would be held accountable for following through on any commitments. “Under this bold initiative, these massive companies will build, bring, or buy their own power supply for new AI data centers,” White House spokesperson Taylor Rogers said in an email to The Verge . “We’re telling the major tech companies that they have the obligation to provide for their own power needs,” Trump said during his speech. “They have the obligation to provide for their own power needs” Companies expanding their data centers for generative AI are already trying to do that. Anthropic and Microsoft have made voluntary commitments recently to cover the costs of new power plants built to serve their data centers. But they would need to sign contracts with utilities and grid operators, or local regulators would need to set new policies to keep companies on the hook to fulfill their promises. Meta has inked a 15-year agreement to cover the capital costs of three new gas-fired plants being built in Louisiana to power its largest data center yet. But some residents and consumer advocates are still concerned about how increased demand from the data center could raise fuel and electricity costs. Tech companies have also announced a slew of agreements recently to support the deployment of next-generation nuclear reactors that could power their data centers. But that technology is still in development, and generally not expected to come online until the 2030s. Plans to hook up new fossil fuel-fired plants to the power grid also face delays with gas turbines in short supply . Household electricity bills already increased 13 percent nationally in 2025, according to a December report from advocacy group Climate Power . Rates are ticking up as aging power grids upgrade their infrastructure, and as data centers, factories, and electric vehicles increase power demand. Data center electricity demand alone is expected to double or triple by 2028, according to the Department of Energy . Another obstacle is increasingly in the way of those ambitions: local pushback , which has resulted in tech companies facing construction delays and cancellations for dozens of data center projects across the US . Subsequently, there’s been a wave of promises from tech firms to address community concerns. Soaring electricity rates also became a key issue in state races Democrats won last year, including Governor Abigail Spanberger’s victory in Virginia. Spanberger, whose state is home to the biggest hub for data centers in the world, delivered Democrats’ response to Trump’s address. “As I campaigned for Governor last year, I traveled to every corner of Virginia, and I heard the same pressing concern everywhere: costs are too high,” Spanberger said . “And I know these same conversations are being had all across this country.”",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/STK466_ELECTION_2024_CVirginia_E.jpg?quality=90&strip=all&crop=0%2C9.9676601489831%2C100%2C80.064679702034&w=1200"
    }
  ]
}