{
  "industry": "technology",
  "collected_at": "2026-02-08T05:40:43.527647+00:00",
  "hours": 24,
  "limit": 25,
  "count": 25,
  "items": [
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Substack confirms data breach affects users' email addresses and phone numbers",
      "url": "https://techcrunch.com/2026/02/05/substack-confirms-data-breach-affecting-email-addresses-and-phone-numbers/",
      "published": "2026-02-08T04:34:50+00:00",
      "summary": "<p>Article URL: <a href=\"https://techcrunch.com/2026/02/05/substack-confirms-data-breach-affecting-email-addresses-and-phone-numbers/\">https://techcrunch.com/2026/02/05/substack-confirms-data-breach-affecting-email-addresses-and-phone-numbers/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46931347\">https://news.ycombinator.com/item?id=46931347</a></p> <p>Points: 12</p> <p># Comments: 4</p>",
      "content_text": "Newsletter platform Substack has confirmed a data breach in an email to users. The company said that in October, an “unauthorized third party” accessed user data, including email addresses, phone numbers, and other unspecified “internal metadata.” Substack specified that more sensitive data, such as credit card numbers, passwords, and other financial information, was unaffected. In an email sent to users, Substack chief executive Chris Best said that the company identified the issue in February that allowed someone to access its systems. Best said that Substack has fixed the problem and started an investigation. “I’m reaching out to let you know about a security incident that resulted in the email address and phone number from your Substack account being shared without your permission,” said Best in the email to users. “I’m incredibly sorry this happened. We take our responsibility to protect your data and your privacy seriously, and we came up short here.” It’s not clear what exactly the issue was with its systems, and the scope of the data that was accessed. It’s also not yet known why the company took five months to detect the breach, or if it was contacted by hackers demanding a ransom. TechCrunch asked the company for more details, and we will update our story if we hear back. Substack did not say how many users are affected. The company said that it doesn’t have any evidence that users’ data is being misused, but did not say what technical means, such as logs, it has to detect evidence of abuse. However, the company asked users to take caution with emails and texts without any particular indicators or direction. On its website, Substack says that its site has more than 50 million active subscriptions, including 5 million paid subscriptions — a milestone it reached last March . In July 2025, the company raised $100 million in Series C funding led by BOND and The Chernin Group (TCG), with participation from a16z, Klutch Sports Group CEO Rich Paul, and Skims co-founder Jens Grede. Techcrunch event Boston, MA | June 23, 2026",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2024/05/substack-on-a-phone.jpg?resize=1200,800"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Homeland Security Spying on Reddit Users",
      "url": "https://www.kenklippenstein.com/p/homeland-security-spies-on-reddit",
      "published": "2026-02-08T03:59:24+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.kenklippenstein.com/p/homeland-security-spies-on-reddit\">https://www.kenklippenstein.com/p/homeland-security-spies-on-reddit</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46931213\">https://news.ycombinator.com/item?id=46931213</a></p> <p>Points: 57</p> <p># Comments: 13</p>",
      "content_text": "Homeland security field agents are scouring the social media site Reddit, monitoring the communications of law-abiding Americans critical of the agency. The spying is revealed in a January intelligence bulletin produced by the Border Patrol and leaked to me. The subject of the report is Reddit user “ Budget-Chicken-2425 ,” who is not a narco-trafficker, gang member, or terrorist. Just someone concerned about federal overreach. The report centers on Budget-Chicken’s call for a protest near a Border Patrol facility in Edinburg, Texas. Though the report acknowledges that anti-ICE protests throughout Texas have been “generally lawful” and that there’s no evidence of any threat posed by Budget-Chicken’s call, any protest whatsoever near the border patrol facility is said to “warrant continued monitoring.” Screenshot of leaked Border Patrol intelligence bulletin To quote directly from the intelligence bulletin: “At this time, there is no specific reporting of planned violence targeting DHS personnel or facilities linked to this protest call; however, any demonstration in proximity to USBP [United States Border Patrol] RGV [Rio Grande Valley] facilities may present operational, safety, and reputational risks that warrant continued monitoring.” Budget-Chicken’s offending Reddit post was on the r/RioGrandeValley channel. Titled “Join me in protest against ICE,” the post is just a few sentences long, calling on “neighbors, family and community” to “be witnesses and to spread awareness” by protesting a Border Patrol station. Innocuous as this may seem, it is what the intelligence bulletin regards as a threat. At one point the bulletin inadvertently reveals the “intelligence collection requirements” driving this surveillance—a window into how the federal government justifies this kind of social media snooping on Americans. According to these requirements, much of the work is sanctioned under so-called “Force Protection,” a military term for safeguarding troops from enemy attack. By repurposing this battlefield concept, homeland security is treating a Reddit thread like a hostile environment. But the homeland security spies are interested in more than just Budget-Chicken. They are using Reddit to gauge the vibe of the country at large and what they think of immigration authorities like themselves. (They could simply consult public polling , which most recently suggests that almost two-thirds of Americans believe immigration enforcement has “gone too far.”) In a section titled “Pattern, Trend, and Relationship Analysis,” the bulletin gives a sense of the sheer volume of data homeland security collects to generate a big-picture view of what’s going on in the country. One specific priority asks: “What groups or individuals are responsible for, or are associated with, border violence and what is the intended impact to CBP [Customs and Border Protection] personnel and operations?” (That ultimately feeds into the creation of databases and watchlists of offending Americans.) The bulletin says homeland security is tracking three social trendsf in particular: Social Media-Driven Mobilization Symbolic Targeting of Government Facilities A Statewide Baseline of Mobilization Potential Leaked intelligence bulletin In other words, the government is building a sociological profile of political discontent. The bulletin notes that these protests are “perception-driven”—meaning they are motivated by “generalized concerns about rights” rather than specific incidents. In the logic of national security, a lack of a specific trigger makes the public more unpredictable and therefore calls for “situational awareness.” But it’s the granularity of the monitoring that is most absurd. To determine the “threat” posed by Budget-Chicken-2425, analysts didn’t just look at the protest call; they scoured the user’s entire digital footprint. The bulletin notes that Chicken “frequently participates in various community discussions,” listing their interests in r/Texans (comparing the team to the Cleveland Browns), r/movies (discussing the film Almost Famous ), r/stephenking (sharing book collections), and r/FuckImOld (reminiscing about 1970s television production logos). Leaked intelligence bulletin The disconnect between the “lawful” reality of the protest and the agency’s internal panic is most visible in a “BOLO” (Be On The Look Out) alert included in the appendix. Despite the bulletin’s own admission that there is “no specific reporting of planned violence,” the Rio Grande Valley Sector Operations Center issued the following warning to its agents: “It is recommended that all agents wear their ballistic armor, utilize long arms, and if possible, work in groups.” National security brain has so infected the immigration authorities that they now treat a four-sentence Reddit post about “spreading awareness” as though it were the secret messages of al Qaeda. It is a system designed to find threats everywhere, even if it has to look for them in a subreddit about Stephen King novels. Leave a comment Share — Edited by William M. Arkin",
      "cover_image_url": "https://substackcdn.com/image/fetch/$s_!9abv!,w_1200,h_675,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb0fe9948-1e39-4d37-92b0-b89619c807a7_1536x1024.png"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "India has changed its startup rules for deep tech",
      "url": "https://techcrunch.com/2026/02/07/india-has-changed-its-startup-rules-for-deep-tech/",
      "published": "2026-02-08T03:30:00+00:00",
      "summary": "India is adjusting startup rules to help more of its deep tech startups with funding and long-term success.",
      "content_text": "Deep tech startups in sectors such as space, semiconductors, and biotech take far longer to mature than conventional ventures. Because of that India is adjusting its startup rules, and mobilizing public capital, hoping to help more of them make it to commercial products. This week, the Indian government updated its startup framework, doubling the period for which deep tech companies are treated as startups to 20 years and raising the revenue threshold for startup-specific tax, grant, and regulatory benefits to ₹3 billion (about $33.12 million), from ₹1 billion (around $11.04 million) previously. The change aims to align policy timelines with the long development cycles typical of science- and engineering-led businesses. The change also forms part of New Delhi’s effort to build a long-horizon deep tech ecosystem by combining regulatory reform with public capital, including the ₹1 trillion (around $11 billion) Research, Development and Innovation Fund (RDI), announced last year. That fund is intended to expand patient financing for science-led and R&D-driven companies. Against that backdrop, U.S. and Indian venture firms later came together to launch the India Deep Tech Alliance , $1 billion-plus private investor coalition that includes Accel, Blume Ventures, Celesta Capital, Premji Invest, Ideaspring Capital, Qualcomm Ventures, and Kalaari Capital, with chipmaker Nvidia acting as an adviser . For founders, these changes may fix what some see as an artificial pressure point. Under the previous framework, companies often risked losing startup status while still pre-commercial, creating a “false failure signal” that judged science-led ventures on policy timelines rather than technological progress, said Vishesh Rajaram, founding partner at Speciale Invest, an Indian deep tech venture capital firm. “By formally recognizing deep tech as different, the policy reduces friction in fundraising, follow-on capital, and engagement with the state, which absolutely shows up in a founder’s operating reality over time,” Rajaram told TechCrunch. Still, investors say access to capital remains a more binding constraint, particularly beyond the early stages. “The biggest gap has historically been funding depth at Series A and beyond, especially for capital-intensive deep tech companies,” Rajaram said. That is where the government’s earlier RDI fund is meant to play a complementary role. “The real benefit of the RDI framework is to increase the funding available to deep tech companies at early and growth stages,” said Arun Kumar, managing partner at Celesta Capital. By routing public capital through venture funds with tenors similar to private capital, he said, the fund is designed to address chronic gaps in follow-on funding without altering the commercial criteria that govern private investment decisions. Techcrunch event Boston, MA | June 23, 2026 Siddarth Pai, founding partner at 3one4 Capital and co-chair of regulatory affairs at the Indian Venture and Alternate Capital Association, said India’s deep tech framework avoids a “graduation cliff” that has historically cut companies off from support just as they scale. These policy changes come as the RDI fund is beginning to take shape operationally, Pai said, with the first batch of fund managers identified and the process of selecting venture and private equity managers under way. While private capital for deep tech already exists in India — particularly in areas such as biotech — Pai told TechCrunch the RDI Fund is intended to act as a nucleus around which greater capital formation can occur. Unlike a traditional fund-of-funds, he noted, the vehicle is also designed to take direct positions and provide credit and grants to deep tech startups. India’s deep tech funding grows In terms of scale, India remains an emerging rather than dominant deep tech market. Indian deep tech startups have raised $8.54 billion in total to date, but recent data point to renewed momentum. Indian deep tech startups raised $1.65 billion in 2025, a sharp rebound from $1.1 billion in each of the previous two years after funding peaked at $2 billion in 2022, per Tracxn. The recovery suggests growing investor confidence, particularly in areas aligned with national priorities such as advanced manufacturing, defence, climate technologies, and semiconductors. “Overall, the pickup in funding suggests a gradual move toward longer-horizon investing,” said Neha Singh, co-founder of Tracxn. In comparison, U.S. deep tech startups raised about $147 billion in 2025, more than 80 times the amount deployed in India that year, while China accounted for roughly $81 billion, data from Tracxn shows. The disparity highlights the challenge India faces in building capital-intensive technologies, even with its wealth of engineering talent. So the hope is that these moves by the Indian government will lead to more investor participation over the medium term. Image Credits: Jagmeet Singh / TechCrunch A longer-term signal For global investors, New Delhi’s framework change is being read as a signal of longer-term policy intent rather than a trigger for immediate shifts in allocation. “Deep tech companies operate on seven- to twelve-year horizons, so regulatory recognition that stretches the lifecycle gives investors greater confidence that the policy environment will not change mid-journey,” said Pratik Agarwal, a partner at Accel. While he said the change would not alter allocation models overnight or eliminate policy risk entirely, it increased investor comfort that India is thinking about deep tech on longer time horizons. “The change shows that India is learning from the U.S. and Europe on how to create patient frameworks for frontier building,” Agarwal told TechCrunch. Whether the move will reduce the tendency of Indian startups to shift their headquarters overseas as they scale remains an open question. The extended runway strengthens the case for building and staying in India, Agarwal said, though access to capital and customers still matters. Over the past five years, he added, India’s public markets have shown a growing appetite for venture-backed tech companies , making domestic listings a more credible option than in the past. That, in turn, could ease some of the pressure on deep tech founders to incorporate overseas, even if access to procurement and late-stage capital will continue to shape where companies ultimately scale. For investors backing long-horizon technologies, the ultimate test will be whether India can deliver globally competitive outcomes. The real signal, Kumar of Celesta Capital said, would be the emergence of a critical mass of Indian deep tech companies succeeding on the world stage. “It would be great to see ten globally competitive deep tech companies from India achieve sustained success over the next decade,” he said, describing that as the benchmark he would look for in assessing whether India’s deep tech ecosystem is maturing.",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2024/02/india-space-getty.jpg?w=1200"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Bye Bye Humanity: The Potential AMOC Collapse",
      "url": "https://thatjoescott.com/2026/02/03/bye-bye-humanity-the-potential-amoc-collapse/",
      "published": "2026-02-08T03:20:33+00:00",
      "summary": "<p>Article URL: <a href=\"https://thatjoescott.com/2026/02/03/bye-bye-humanity-the-potential-amoc-collapse/\">https://thatjoescott.com/2026/02/03/bye-bye-humanity-the-potential-amoc-collapse/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46931026\">https://news.ycombinator.com/item?id=46931026</a></p> <p>Points: 28</p> <p># Comments: 23</p>",
      "content_text": "It’s been talked about for a while (in fact I covered this 6 years ago), but a lot of things have been said about the potential collapse of the Atlantic Gulf Stream, also known as the Atlantic Meridional Overturning Circulation or AMOC. This global current that keeps Europe and North America (but especially Europe) from being covered in ice is increasingly under threat of slowing or even stopping. The results would be catastrophic. TRANSCIPT: Let’s start with the dismal outlook. Back in 2021, a study in Nature Geosciences showed that the AMOC was the weakest it’s been in more than 1,000 years. The study looked at 11 indicators like deep-sea sediments and ocean temperature patterns going all the way back to 400 C.E. Nine indicators showed a consistent pattern of the AMOC weakening. A more recent study from 2024 found that the abyssal [uh·bi·sl] limb of the AMOC in the North Atlantic is weakening. Also published in Nature Geosciences, the study used mooring observations and hydrographic data from multiple sources in the North Atlantic. So, the AMCO has an upper cell and a deep-sea cell that is underneath it. The upper cell moves warm water from the South Atlantic Ocean to the North Atlantic, where it cools down, sinks, and then flows back down south. The deep-sea cell of colder water at Antarctica’s edge is called the abyssal cell. This Antarctic Bottom Water is the… wait… bottom water? Anyway, this water is the coldest and densest water mass of the oceans. But the study found that the northward movement of the Antarctic Bottom Water at 16 degrees North weakened by around 12 percent from 2000 to 2020. This weakening is associated with observed warming throughout the deep Western Atlantic Ocean. That means an increase in deep-sea heat and rising sea levels in the region. Another study in 2024 showed that a collapse of the AMOC before the year 2100 was unlikely. But when climate models were run to the years 2300 and 2500, they showed that a tipping point for an AMOC collapse would likely happen within a few decades. The researchers found that if carbon emissions keep rising, 70 percent of the model runs lead to a collapse. An intermediate level of emissions showed a 37 percent chance of collapse in the model runs. And even if there were low emissions in the future, an AMOC collapse happened in 25 percent of the models. One of the study’s authors found the results shocking because he thought the chance of the AMOC collapsing from global warming was less than 10 percent. Professor Stefan Rahmstorf told The Guardian: “These numbers are not very certain, but we are talking about a matter of risk assessment where even a 10% chance of an Amoc collapse would be far too high. We found that the tipping point where the shutdown becomes inevitable is probably in the next 10 to 20 years or so.” Scientists started seeing warning signs of a tipping point about five years ago. In fact, observations in the far North Atlantic are already showing a downward trend that is consistent with the models’ projections. Even in intermediate- and low-emission models, the AMOC slows a ton by 2100 and completely collapses afterward. The scientists say this shows the shutdown risk is more serious than many people realize. There’s also a patch of cold water south of Greenland that has resisted the Atlantic Ocean’s overall warming for more than a century. It’s been a mystery as to why. Well, researchers from the University of California, Riverside say only one explanation fits both salinity patterns and observed ocean temperatures. And it’s that the AMOC is slowing down. Using a century’s worth of data, the researchers reconstructed changes in the circulation system and compared them with around 100 different climate models. Only the models that simulated a weakened AMOC matched real-world data. Models that assumed a stronger circulation didn’t even come close. One of the study’s authors said: “It’s a very robust correlation. If you look at the observations and compare them with all the simulations, only the weakened-AMOC scenario reproduces the cooling in this one region.” The researchers also found that the AMOC’s weakening correlates with decreased salinity. That’s another signal that less warm, salty water is being moved northward. It’s not just climate models that are showing an eventual collapse. We also have “fingerprints” from satellite images. Recent high-resolution images show water temperature increasing over time. The researchers say it proves that the AMOC weakening is underway. But you know what. Maybe the AMOC collapse isn’t a pressing problem. Several studies offer a more optimistic view of the situation. For example, a study published in Nature last year showed that the AMOC is resilient to extreme greenhouse gases. Researchers said that upwelling in the Southern Ocean, driven by consistent winds, helps sustain a weakened AMOC in climate models. This upwelling has to be balanced by downwelling in the Atlantic or Pacific. The AMOC can only collapse if a Pacific Meridional Overturning Circulation forms. In almost all their models, a PMOC did emerge, but Southern Ocean upwelling overcame any destabilizing effects of it. They predict that an AMOC is unlikely this century. But they say that projections should focus on ocean circulation changes beyond the North Atlantic to fully understand AMOC’s future. Then there’s a paper published in Nature Communications in January 2025 that found that the AMOC hasn’t declined in the last 60 years. One of the study’s authors said: “Our paper says that the Atlantic overturning has not declined yet. That doesn’t say anything about its future, but it doesn’t appear the anticipated changes have occurred yet.” The study is a stark contrast to a 2018 study that said the AMOC had declined over the last 70 years. That research relied on ocean surface temperature measurements to track how the AMOC changed. But measuring surface temperature doesn’t work that well. Researchers for the latest study used new data from 24 climate-earth models created by the World Climate Research Program. They found that recent surface temperature data didn’t accurately reconstruct the AMOC. They took things further by looking at air-sea heat fluxes, which is the exchange of heat from the ocean to the atmosphere. They found that a stronger AMOC releases more heat from the ocean into the air over the North Atlantic. The study shows that air-sea heat flux anomalies in the North Atlantic are closely linked to the AMOC and that it hasn’t weakened from 1963 to 2017. Ready for more positive news? Well, a study by Caltech researchers found that even though the AMOC will weaken because of climate change, it will do so less than predicted. The researchers developed a simplified physical model based on how density differences and the AMOC’s depth are related. They included real-world measurements of the current’s strength collected over 20 years from monitoring arrays and other observation products in the Atlantic basin. The team found that the AMOC will only weaken by about 18 to 43 percent by the end of the 21st century. Yeah, 43 percent is a lot, but it’s nowhere near what other climate models project. As the study’s lead author said: “Our results imply that, rather than a substantial decline, the AMOC is more likely to experience a limited decline over the 21st century—still some weakening, but less drastic than previous projections suggest.” The researchers say that some of the extreme AMOC weakening projections came from biases in how climate models simulate the ocean’s current state, especially its density stratification. The AMOC has collapsed before. It was about 12,900 years ago. That’s when the rapid melting of the frozen Lake Agassiz in North America caused vast amounts of freshwater to move into the sea. This led to fluctuations in temperature of 10 to 15 degrees Celsius in the Northern Hemisphere within a decade. A comet impact may have caused the event, leading to 1,300 years of freezing, a period of time known as the Younger Dryas. The same thing would happen today if the AMOC were to collapse. Scientists predict that some European cities would see a five to 15 degree Celsius drop in temperatures within a few decades after the AMOC shuts down. But Europe wouldn’t be the only place affected. The Atlantic Ocean would rise by 70 centimeters, submerging many coastal areas and cities. And in the Southern Hemisphere, regions would get warmer than they already are. An AMOC collapse is definitely something to be concerned about. In fact, it’s so serious that Iceland declared it a security risk in November 2025. The country’s government is looking at what policies and research are needed, and work has already started on a disaster preparedness policy. Some of the risks being evaluated include energy, food security, infrastructure, and transportation. Iceland isn’t waiting for definitive, long-term research. It’s acting now because it believes in not “if” but “when.” The will-it-won’t-it collapse of the AMOC is something to keep an eye on. But there are other pressing climate change issues to address in the near term, such as food security, ecosystem degradation, and rising disease rates.",
      "cover_image_url": "http://thatjoescott.com/wp-content/uploads/2026/02/bye-bye-humanity-the-potential-a.jpg"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Beyond agentic coding",
      "url": "https://haskellforall.com/2026/02/beyond-agentic-coding",
      "published": "2026-02-08T01:55:09+00:00",
      "summary": "<p>Article URL: <a href=\"https://haskellforall.com/2026/02/beyond-agentic-coding\">https://haskellforall.com/2026/02/beyond-agentic-coding</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46930565\">https://news.ycombinator.com/item?id=46930565</a></p> <p>Points: 48</p> <p># Comments: 9</p>",
      "content_text": "I'm generally pretty pro-AI with one major exception: agentic coding. My consistent impression is that agentic coding does not actually improve productivity and deteriorates the user's comfort and familiarity with the codebase. I formed that impression from: my own personal experiences Every time I use agentic coding tools I'm consistently unimpressed with the quality of the results. my experiences interviewing candidates I allow interview candidates to use agentic coding tools and candidates who do so consistently performed worse than other candidates, failing to complete the challenge or producing incorrect results 1 . This was a huge surprise to me at first because I expected agentic coding to confer an unfair advantage but ‚Ä¶ nope! research studies Studies like the Becker study and Shen study show that users of agentic coding perform no better and sometimes worse when you measure productivity in terms of fixed outcomes rather than code velocity/volume. I don't believe agentic coding is a lost cause, but I do believe agentic coding in its present incarnation is doing more harm than good to software development. I also believe it is still worthwhile to push on the inadequacies of agentic coding so that it empowers developers and improves code quality. However, in this post I'm taking a different tack: I want to present other ways to leverage AI for software development. I believe that agentic coding has so captured the cultural imagination that people are sleeping on other good and underexplored solutions to AI-assisted software development. The master cue I like to design tools and interfaces from first principles rather than reacting to industry trends/hype and I've accrued quite a few general design principles from over a decade of working in DevProd and also an even longer history of open source projects and contributions. One of those design principles is my personal \"master cue\", which is: A good tool or interface should keep the user in a flow state as long as possible This principle isn't even specific to AI-assisted software development, and yet still highlights why agentic coding sometimes misses the mark. Both studies and developer testimonials show that agentic coding breaks flow and keeps developers in an idle/interruptible holding pattern more than ordinary coding. For example, the Becker study took screen recordings and saw that idle time approximately doubled: I believe we can improve AI-assisted coding tools (agentic or not) if we set our north star to ‚Äúpreserve flow state‚Äù. Calm technology Calm technology is a design discipline that promotes flow state in tools that we build. The design principles most relevant to coding are: tools should minimize demands on our attention Interruptions and intrusions on our attention break us out of flow state. tools should be built to be ‚Äúpass-through‚Äù A tool is not meant to be the object of our attention; rather the tool should reveal the true object of our attention (the thing the tool acts upon), rather than obscuring it. The more we use the tool the more the tool fades into the background of our awareness while still supporting our work. tools should create and enhance calm (thus the name: calm technology) A state of calm helps users enter and maintain flow state. Non-LLM examples of calm technology Engineers already use ‚Äúcalm‚Äù tools and interfaces as part of our work and here are a couple of examples you're probably already familiar with: Inlay hints IDEs (like VSCode) can support inlay hints that sprinkle the code with useful annotations for the reader, such as inferred type annotations: These types of inlay hints embody calm design principles because: they minimize demands on our attention They exist on the periphery of our attention, available for us if we're interested but unobtrusive if we're not interested. they are built to be ‚Äúpass-through‚Äù They don't replace or substitute the code that we are editing. They enhance the code editing experience but the user is still in direct contact with the edited code. The more we use type hints the more they fade into the background of our awareness and the more the code remains the focus of our attention. they create and enhance calm They promote a sense of calm by informing our understanding of the code passively . As one of the Calm Technology principles puts it: ‚ÄúTechnology can communicate, but doesn't need to speak‚Äù. File tree previews Tools like VSCode or GitHub's pull request viewer let you preview at a glance changes to the file tree, like this: You might think to yourself ‚Äúthis is a very uninteresting thing to use as an example‚Äù but that's exactly the point. The best tools (designed with the principles of calm technology) are pervasive and boring things that we take for granted (like light switches) and that have faded so strongly into the background of our attention that we forget they even exist as a part of our daily workflow (also like light switches). File tree previews: minimize demands on our attention They're there if we need the information, but easy to ignore (or even forget they exist) if we don't use them. are built to be ‚Äúpass-through‚Äù When we interact with the file tree viewer we are interacting directly with the filesystem and the interaction between the representation (the viewer) and the reality (the filesystem) feels direct, snappy, and precise. The more we use the viewer the more the representation becomes indistinguishable from the reality in our minds. create and enhance calm We do not need to constantly interact with the file tree to gather up-to-date information about our project structure. It passively updates in the background as we make changes to the project and those updates are unobtrusive and not attention-grabbing. Chat-based coding agents are not calm We can think about the limitations of chat-based agentic coding tools through this same lens: they place high demands on our attention The user has to either sit and wait for the agent to report back or do something else and run the LLM in a semi-autonomous manner. However, even semi-autonomous sessions prevent the user from entering flow state because they have to remain interruptible. they are not built to be ‚Äúpass-through‚Äù Chat agents are a highly mediated interface to the code which is indirect (we interact more with the agent than the code), slow (we spend a lot of time waiting), and imprecise (English is a dull interface ). they undermine calm The user needs to constantly stimulate the chat to gather new information or update their understanding of the code (the chat agent doesn't inform the user's understanding passively or quietly). Chat agents are also fine-tuned to maximize engagement. Prior art for calm design Inline suggestions from GitHub Copilot One of the earliest examples of an AI coding assistant that begins to model calm design principles is the OG AI-assistant: GitHub Copilot's support for inline suggestions , with some caveats I'll go into. This does one thing really well: However, by default these inline suggestions violate other calm technology principles: they demand our attention By default Copilot presents the suggestions quite frequently and the user has to pause what they're doing to examine the output of the suggestion. After enough times the user begins to condition themselves into regularly pausing and waiting for a suggestion which breaks them out of a flow state. Now instead of being proactive the user's been conditioned by the tool to be reactive. they undermine calm GitHub Copilot's inline suggestion interface is visually busy and intrusive. Even if the user ignores every suggestion the effect is still disruptive: suggestions appear on the user's screen in the center of their visual focus and the user has to decide on the spot whether to accept or ignore them before proceeding further. The user also can't easily passively absorb information presented in this way: understanding each suggestion requires the user's focused attention. ‚Ä¶ buuuuut these issues are partially fixable by disabling the automatic suggestions and requiring them to be explicitly triggered by Alt + \\ . However, unfortunately that also disables the next feature, which I like even more: Next edit suggestions (also from GitHub Copilot) Next edit suggestions are a related GitHub Copilot feature that display related follow-up edits throughout the file/project and let the user cycle between them and possibly accept each suggested change. They behave like a ‚Äúsuper-charged find and replace‚Äù: These suggestions do an amazing job of keeping the user in a flow state: they minimize demand on the user's attention The cognitive load on the user is smaller than inline suggestions because the suggestions are more likely to be bite-sized (and therefore easier for a human to review and accept). they're built to be ‚Äúpass-through‚Äù Just like inline suggestions, next edit suggestions still keep the user in close contact with the code they are modifying. they create and enhance calm Suggestions are presented in an unobtrusive way: they aren't dumped in the dead center of the user's attention and they don't demand immediate review. They exist on the periphery of the user's attention as code suggestions that the user can ignore or focus on at their leisure. AI-assisted calm technology I believe there is a lot of untapped potential in AI-assisted coding tools and in this section I'll sketch a few small examples of how we can embody calm technology design principles in building the next generation of coding tools. Facet-based project navigation You could browse a project by a tree of semantic facets. For example, if you were editing the Haskell implementation of Dhall the tree viewer might look like this prototype I hacked up 2 : The goal here is to not only provide a quick way to explore the project by intent, but to also improve the user's understanding of the project the more they use the feature. \"String interpolation regression\" is so much more informative than dhall/tests/format/issue2078A.dhall 3 . Also, the above video is based on a real tool and not just a mock. You can find the code I used to generate that tree of semantics facets here and I'll write up another post soon walking through how that code works. Automated commit refactor You could take an editor session, a diff, or a pull request and automatically split it into a series of more focused commits that are easier for people to review. This is one of the cases where the AI can reduce human review labor (most agentic coding tools create more human review labor). There is some prior art here but this is still a nascent area of development. File lens You could add two new tools to the user's toolbar or context menu: ‚ÄúFocus on‚Ä¶‚Äù and ‚ÄúEdit as‚Ä¶‚Äù. ‚ÄúFocus on‚Ä¶‚Äù would allow the user to specify what they're interested in changing and present only files and lines of code related to their specified interest. For example, if they want to focus on ‚Äúcommand line options‚Äù then only related files and lines of code would be shown in the editor and other lines of code would be hidden/collapsed/folded. This would basically be like ‚ÄúZen mode‚Äù but for editing a feature domain of interest. ‚ÄúEdit as‚Ä¶‚Äù would allow the user to edit the file or selected code as if it were a different programming language or file format. For example, someone who was new to Haskell could edit a Haskell file ‚Äúas Python‚Äù and then after finishing their edits the AI attempts to back-propagate their changes to Haskell. Or someone modifying a command-line parser could edit the file ‚Äúas YAML‚Äù and be presented with a simplified YAML representation of the command line options which they could modify to add new options. Conclusion This is obviously not a comprehensive list of ideas, but I wrote this to encourage people to think of more innovative ways to incorporate AI into people's workflows besides just building yet another chatbot. I strongly believe that chat is the least interesting interface to LLMs and AI-assisted software development is no exception to this.",
      "cover_image_url": "https://haskellforall.com/imgs/logo.jpg"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "The kids 'picked last in gym class' gear up for Super Bowl",
      "url": "https://techcrunch.com/2026/02/07/the-kids-picked-last-in-gym-class-gear-up-for-super-bowl/",
      "published": "2026-02-08T01:26:49+00:00",
      "summary": "YouTube CEO Neal Mohan is expected to be there. Apple's Tim Cook, too.",
      "content_text": "The Super Bowl is happening in Silicon Valley this Sunday, and the Patriots-Seahawks game at Levi’s Stadium is going to be packed with tech money. YouTube CEO Neal Mohan is expected to be there. Apple’s Tim Cook, too. (He has become a Super Bowl fixture since Apple Music began sponsoring the halftime show several years ago.) Longtime VC Venky Ganesan from Menlo Ventures gave the New York Times a quote about the whole thing, saying the Super Bowl in the Bay Area is “tech billionaires who got picked last in gym class paying $50,000 to pretend they’re friends with the guys who got picked first.” Added Ganesan, “And for the record, I, too, was picked last in gym class.” Ganesan could likely afford a $50,000 ticket if he needed one. Menlo went all-in on Anthropic, setting up a $100 million fund with the AI company in summer 2024 to invest in other AI startups. The firm has also joined numerous funding rounds for Anthropic itself, both through its flagship fund and various special purpose vehicles. (Anthropic is reportedly expected to close a $20 billion round of funding next week at a post-money valuation of $350 billion.) Tickets are expensive across the board, averaging almost $7,000 according to the Times (with some last-minute seats still available on StubHub for closer to $3,600, according to a quick glance at the ticket reseller site). Only a quarter go to the general public; the rest are distributed to NFL teams. Of all ticket buyers, the largest group (27%) is coming from Washington State for the Seahawks, who’ve won just one Super Bowl in franchise history compared with the Patriots’ six titles, all with Tom Brady at quarterback. Google , OpenAI, Anthropic , Amazon , and Meta are splashing out for competing ads about whose AI is best for customers, so maybe their respective CEOs will show up, too. Other than Amazon’s Andy Jassy, who reportedly splits his time between Seattle and Santa Monica, all of them have homes within an hour or so of Sunday’s game. This is just the third time the Bay Area has hosted the Super Bowl. The first time was in 1985 at Stanford Stadium, the original football stadium at Stanford University, where the 49ers beat the Dolphins. The second took place 10 years ago at Levi’s Stadium, when the Broncos beat the Panthers. Techcrunch event Boston, MA | June 23, 2026",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2026/01/tim-coook-apple-tv-GettyImages-2235568147-1.jpg?w=1024"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "GitHub - localgpt-app/localgpt",
      "url": "https://github.com/localgpt-app/localgpt",
      "published": "2026-02-08T01:26:38+00:00",
      "summary": "<p>I built LocalGPT over 4 nights as a Rust reimagining of the OpenClaw assistant pattern (markdown-based persistent memory, autonomous heartbeat tasks, skills system).<p>It compiles to a single ~27MB binary — no Node.js, Docker, or Python required.<p>Key features:<p>- Persistent memory via markdown files (MEMORY, HEARTBEAT, SOUL markdown files) — compatible with OpenClaw's format - Full-text search (SQLite FTS5) + semantic search (local embeddings, no API key needed) - Autonomous heartbeat runner that checks tasks on a configurable interval - CLI + web interface + desktop GUI - Multi-provider: Anthropic, OpenAI, Ollama etc - Apache 2.0<p>Install: `cargo install localgpt`<p>I use it daily as a knowledge accumulator, research assistant, and autonomous task runner for my side projects. The memory compounds — every session makes the next one better.<p>GitHub: <a href=\"https://github.com/localgpt-app/localgpt\" rel=\"nofollow\">https://github.com/localgpt-app/localgpt</a> Website: <a href=\"https://localgpt.app\" rel=\"nofollow\">https://localgpt.app</a><p>Would love feedback on the architecture or feature ideas.</p> <hr /> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46930391\">https://news.ycombinator.com/item?id=46930391</a></p> <p>Points: 113</p> <p># Comments: 32</p>",
      "content_text": "You can’t perform that action at this time.",
      "cover_image_url": "https://opengraph.githubassets.com/c38deaf1c88ca218c6349981981ff72613e3a1a96f348c8105c0da8cee09d972/localgpt-app/localgpt"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "Former Tumblr head Jeff D’Onofrio steps in as acting CEO at the Washington Post",
      "url": "https://www.theverge.com/tech/875433/tumblr-jeff-donofrio-ceo-washington-post-layoffs",
      "published": "2026-02-07T23:41:23+00:00",
      "summary": "After what can generously be called a contentious tenure as the CEO of The Washington Post, Will Lewis is stepping down following mass layoffs this week. Jeff D'Onofrio, former CEO of Tumblr from 2017 to 2022, will step in as acting CEO and publisher. D'Onofrio has been CFO at the Post since June of last [&#8230;]",
      "content_text": "After what can generously be called a contentious tenure as the CEO of The Washington Post , Will Lewis is stepping down following mass layoffs this week. Jeff D’Onofrio, former CEO of Tumblr from 2017 to 2022, will step in as acting CEO and publisher. D’Onofrio has been CFO at the Post since June of last year, meaning he’s had a front row seat to Jeff Bezos’ dismantling of the once storied paper for the last nine months. D’Onofrio’s resume doesn’t include extensive experience in traditional news media, nor many notable success stories. He was briefly the general manager of Yahoo News while it was still a Verizon property, before shifting his focus solely to Tumblr. Under his leadership, Tumblr tried to clean up its image by banning adult content, but its traffic fell by 30 percent . Yahoo had purchased Tumblr for $1.1 billion in 2013. By 2019, it was sold to Automatic, the owner of WordPress, reportedly for less than $3 million .",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/gettyimages-490730284.jpg?quality=90&strip=all&crop=0%2C14.560455138261%2C100%2C70.879089723477&w=1200"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "Super Bowl LX ads: all AI everything",
      "url": "https://www.theverge.com/entertainment/874504/super-bowl-lx-ads-big-game",
      "published": "2026-02-07T23:10:44+00:00",
      "summary": "Super Bowl LX is nearly here, with the Seattle Seahawks taking on the New England Patriots. While Bad Bunny will be the star of the halftime show, AI could be the star of the commercial breaks, much like crypto was a few years ago. Last year’s Super Bowl featured a Google Gemini ad that fumbled [&#8230;]",
      "content_text": "Super Bowl LX is nearly here, with the Seattle Seahawks taking on the New England Patriots. While Bad Bunny will be the star of the halftime show, AI could be the star of the commercial breaks, much like crypto was a few years ago . Last year’s Super Bowl featured a Google Gemini ad that fumbled a Gouda cheese stat , and this year’s game is already slated to include an ad for Anthropic’s AI platform that takes jabs at its competitors, namely OpenAI. AI-generated ads could make an appearance, too. Super Bowl LX is set to kick off at 6:30PM ET/3:30PM PT on Sunday, February 8th at Levi’s Stadium in Santa Clara, California.",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/chorus/uploads/chorus_asset/file/24008751/acastro_STK099_NFL_01.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "FDA Intends to Take Action Against Non-FDA-Approved GLP-1 Drugs",
      "url": "https://www.fda.gov/news-events/press-announcements/fda-intends-take-action-against-non-fda-approved-glp-1-drugs",
      "published": "2026-02-07T22:25:03+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.fda.gov/news-events/press-announcements/fda-intends-take-action-against-non-fda-approved-glp-1-drugs\">https://www.fda.gov/news-events/press-announcements/fda-intends-take-action-against-non-fda-approved-glp-1-drugs</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46928810\">https://news.ycombinator.com/item?id=46928810</a></p> <p>Points: 112</p> <p># Comments: 232</p>",
      "content_text": "For Immediate Release: February 06, 2026 Statement From: Martin A Makary, M.D., M.P.H. Commissioner of Food and Drugs - Food and Drug Administration Today, the U.S. Food and Drug Administration is announcing its intent to take decisive steps to restrict GLP-1 active pharmaceutical ingredients (APIs) intended for use in non-FDA-approved compounded drugs that are being mass-marketed by companies — including Hims & Hers and other compounding pharmacies — as similar alternatives to FDA-approved drugs. These actions are aimed to safeguard consumers from drugs for which the FDA cannot verify quality, safety, or efficacy. We take seriously any potential violations of the Federal Food, Drug, and Cosmetic Act. The FDA is also taking steps to combat misleading direct-to-consumer advertising and marketing following warning letters that were sent in the fall of 2025. In promotional materials, companies cannot claim that non-FDA-approved compounded products are generic versions or the same as drugs approved by FDA. They also cannot state compounded drugs use the same active ingredient as the FDA-approved drugs or that compounded drugs are clinically proven to produce results for the patient. The FDA will use all available compliance and enforcement tools within its authorities to address unsubstantiated claims and associated public health concerns. Entities engaged in the manufacture, distribution, or marketing of unapproved compounded GLP-1 products should be aware that failure to adequately address any violations may result in legal action without further notice, including, without limitation, seizure and injunction. Related Information ### Boilerplate The FDA, an agency within the U.S. Department of Health and Human Services, protects the public health by assuring the safety, effectiveness, and security of human and veterinary drugs, vaccines and other biological products for human use, and medical devices. The agency also is responsible for the safety and security of our nation’s food supply, cosmetics, dietary supplements, radiation-emitting electronic products, and for regulating tobacco products. Content current as of: 02/06/2026",
      "cover_image_url": "https://www.fda.gov/themes/custom/preview/img/FDA-Social-Graphic.png"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "NBA star Giannis Antetokounmpo joins Kalshi as an investor",
      "url": "https://techcrunch.com/2026/02/07/nba-star-giannis-antetokounmpo-joins-kalshi-as-an-investor/",
      "published": "2026-02-07T21:30:41+00:00",
      "summary": "Antetokounmpo is the first NBA player to directly invest in Kalshi.",
      "content_text": "Giannis Antetokounmpo of the Milwaukee Bucks announced Friday that he has joined prediction market Kalshi as a shareholder, making him the first NBA player to invest directly in the company. “The internet is full of opinions. I decided it was time to make some of my own,” said the two-time NBA MVP in a social media post . “Today, I’m joining Kalshi as a shareholder. We all on Kalshi now.” The announcement has not gone over well on social media. On Reddit , for example, one user described it as “literally a conflict of interest,” while another described Kalshi as “cancerous” and yet another wondered, “is this even allowed.” According to The Athletic , the NBA’s recent collective bargaining agreement allows players to advertise and take stakes of up to 1% in sports betting companies, as long as they’re not promoting league-related wagers. Kalshi said it will partner with Antetokounmpo on marketing and live events — and in accordance with the company’s “strict terms of service that ban insider trading and market manipulation,” he will not be allowed to trade on markets related to the NBA.",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-2186559456.jpg?w=1024"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "GOG is already working on native Linux support",
      "url": "https://www.theverge.com/tech/875425/gog-native-linux-support",
      "published": "2026-02-07T20:23:46+00:00",
      "summary": "GOG recently posted a job listing for a senior software engineer focused on porting its Galaxy desktop client to Linux. Now, in reply to a Reddit AMA, the GOG team has revealed that work on native Linux support is already underway. It's still too early to give an ETA, according to the Reddit thread. But [&#8230;]",
      "content_text": "“We’re at a very early stage right now, but we of course see the rising popularity and importance of this OS for gamers and agree on its value. We’ve started the recruitment process for a Senior Engineer who would help handle it. When it comes to specifics, we can’t share much at the moment. Nonetheless, the work has started, and this will appear on GOG one day. That much we can say.”",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2025/12/gog-logo.webp?quality=90&strip=all&crop=0,10.945238432149,100,78.109523135701"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Notepad on my Windows PC broke thanks to Microsoft's servers",
      "url": "https://www.windowscentral.com/microsoft/windows-11/windows-locked-me-out-of-notepad-is-the-thin-client-era-ruining-pcs",
      "published": "2026-02-07T19:51:56+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.windowscentral.com/microsoft/windows-11/windows-locked-me-out-of-notepad-is-the-thin-client-era-ruining-pcs\">https://www.windowscentral.com/microsoft/windows-11/windows-locked-me-out-of-notepad-is-the-thin-client-era-ruining-pcs</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46927098\">https://news.ycombinator.com/item?id=46927098</a></p> <p>Points: 141</p> <p># Comments: 173</p>",
      "content_text": "A couple of weeks ago, I found that I couldn't open Notepad on my desktop PC. It wasn't because Windows 11 had crashed, but rather, Microsoft told me it wasn't \"available in (my) account\". It turned out that an error (0x803f8001) with Microsoft Store's licensing service stopped me from opening a few first-party apps, including the Snipping Tool. Yes, even the app I usually use to screenshot error messages was busted. Ironic. Now, I'm usually a fairly level-headed Windows enthusiast who can relate to users who both love and loathe Microsoft's operating system, but I couldn't open Notepad.exe — are we serious? You've probably all seen the memes: it's called \"This PC\" now, and not \"My Computer\" anymore. It's usually easy to laugh off as a disgruntled conspiracy, but I can see why it trends when the themings of Software as a Service (SaaS) are creeping into the most basic Windows apps. I was locked out of Notepad due to Microsoft's server bug — is this the 'Thin Client' era? - YouTube Watch On After all, Notepad is supposed to be the absolute barebones, most ultra-basic app in the entire OS. Well, it was, before Microsoft added Copilot and users started looking for a way to disable the unusual AI addition . Sure, you can still type C:\\Windows\\notepad.exe into 'Run' with Windows + R for a legacy fallback, but many perhaps wouldn't know about it. I'm still a Windows guy, and I always will be. Nevertheless, I can't ignore that Windows 11 regularly feels less like an operating system and more like a thin client; just a connection to Microsoft's cloud with fewer options for you to act as the administrator of your own PC. This PC vs. My Computer I was completely locked out of the modern Notepad (Image credit: Ben Wilson | Windows Central) To be clear, I don't have major problems with the default, out-of-box experience (OOBE) of Windows 11. In fact, it doesn't take me long to make changes when installing fresh copies on new desktop builds. Default pins on the Start menu don't matter because I barely use it, and disabling ads is straightforward enough . The major points pretty much boil down to: Uninstalling OneDrive : The web app is fine for manual backups, but I definitely don't want my files automatically synced. Creating a local account : Microsoft keeps making it harder, but I'll always use workarounds. After that, I don't take issue with the normal desktop — unless something unexpectedly breaks. Our Editor-in-Chief, Daniel Rubino, said it best, \"People don’t hate change. They hate surprise. \" It was certainly a surprise to lose access to my plain text editor, loaded up with more than what an extended (Windows + V) clipboard would be useful for. Nobody asked for this. So, is the solution to look for open‑source Notepad clones? Maybe for some enthusiasts, but that's just another app to add to a growing Winget list , and I'd rather Microsoft stay true to its word about walking back Windows 11's AI overload . I can't abide by comments on social platforms suggesting people \"just use a debloater\" on a new Windows PC, either — we shouldn't have to. I generally avoid recommending Windows debloat scripts from GitHub to anyone in the first place. That, and I generally avoid recommending Windows debloat scripts from GitHub to anyone in the first place. Windows can be adjusted to your liking if you follow the right guides , and while you can inspect open-source code for yourself and generally trust some well-respected coders on that platform, it's a strange solution that needn't exist. Stop nudging your users towards danger I built my PC around the 9800X3D, but is it really mine? (Image credit: Ben Wilson | Windows Central) I'm not naive enough to think Windows is Microsoft's top priority. Cloud computing and Microsoft 365 are far more valuable than a consumer-level operating system, though Microsoft does have a staggering lead over the competition — one that would be absurd to jeopardize. Still, my problems with Notepad and Snipping Tool are a raindrop in the Pacific Ocean of Microsoft's broader plans, but I don't want first-party apps asking for authentication from its servers — nor do I want our readers to download the first debloat script they find on the web. There are justifications for Microsoft adding elements of its cloud business to Windows, but I wish it wouldn't force it in a way that locks people into an online-only experience. My PC should be entirely functional without an Internet connection — especially when I need a few scribbles from Notepad. AI is undoubtedly the future, at least in some capacity. Even if Satya Nadella says artificial intelligence needs to prove its worth , there's no believable chance that it's going away, especially now that Copilot is so deeply ingrained in practically everything Microsoft owns. Still, if online-only services are all active by default and Windows 12 is ultimately an agentic AI OS , I wouldn't be surprised if more people stick with a debloated Windows 11, just as others did with Windows 10 . Do you think the next version of Windows will return some control back to the user, or will it be even more Internet-dependent? Join us on Reddit at r/WindowsCentral to share your insights and discuss our latest news, reviews, and more.",
      "cover_image_url": "https://cdn.mos.cms.futurecdn.net/wPJcMXqD44weR2UTFWd8fJ-1920-80.jpg"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "I went back to Linux and it was a mistake",
      "url": "https://www.theverge.com/report/875077/linux-was-a-mistake",
      "published": "2026-02-07T19:30:00+00:00",
      "summary": "It's a complete coincidence that I installed Linux around the same time as my colleagues Nathan Edwards and Stevie Bonifield. A few months ago, I decided to breathe new life into a 2019 Dell XPS 15 that had been collecting dust for a couple of years. Despite its (at the time) high-end Core i7 CPU [&#8230;]",
      "content_text": "It’s a complete coincidence that I installed Linux around the same time as my colleagues Nathan Edwards and Stevie Bonifield. A few months ago, I decided to breathe new life into a 2019 Dell XPS 15 that had been collecting dust for a couple of years. Despite its (at the time) high-end Core i7 CPU and 32GB of RAM, Windows was frustratingly slow on it. The fan was constantly at full throttle even when the machine was idle, and it regularly failed to install updates. So in early 2024, I gave up and switched to an M1 MacBook Pro. But I wanted to give my oldest child something to practice typing on. Plus, I’d been trying to find a suitable distraction-free writing solution . (Spoiler: this laptop was not the solution I was looking for .) So I installed Ubuntu. Again. See, before the MacBook and before the Dell XPS, I was a Linux user. I first installed Ubuntu in 2006 on a ThinkPad X40. And it remained my primary OS across three different laptops and 13 years. My Ubuntu desktop in 2007. Despite some... let’s call them quirks (Wi-Fi didn’t work out of the box on that X40), I was happy with Linux for a long time. I dual-booted Windows out of necessity, and often had a work-issued MacBook on hand. But those were for testing apps or specific tasks, like editing video. 99 percent of my life was spent in Ubuntu. That is, until about 2017. As I got older, tinkering with my laptop changed from a hobby to something that got in the way of me pursuing my other hobbies. I had rediscovered my love of making music, and, try as it might, Ubuntu Studio just wasn’t cutting it. I was spending more time in Ableton Live , which meant more time in Windows, until in 2019, I bought the aforementioned XPS and switched over completely. A lot has changed in the 20 years since I first installed Linux, and even in the seven years since I last used it. It’s now arguably the best platform for PC gaming . There are genuinely great photo-editing apps, such as Darktable , which was introduced in 2009. And there are even viable, commercial options for making music in Bitwig and Reaper , all of which have improved dramatically since 2019. Darktable is a capable RAW photo editor, but it’s no Lightroom. The Ubuntu installation process hasn’t changed much, but the interface is prettier. I put the ISO on a thumb drive and briefly considered wiping the XPS completely. I chickened out at the last minute and decided to dual-boot, which meant repartitioning my hard drive. Right off the bat, I was reminded that Linux hasn’t completely overcome its fiddly nature. The fingerprint reader didn’t work. A minor annoyance I decided to ignore. More concerning was that Ubuntu failed to install updates because of an EFI partition issue specific to the XPS 15 that was also causing problems under Windows. I managed to get them installed, but I’m not convinced I didn’t create a ticking time bomb by deleting essential files. Ubuntu also refused to mount my Windows partition for the first month or so, before suddenly working for unknown reasons. Stevie similarly admitted to “ rage quitting and going to bed ” when they couldn’t get Ubuntu to connect to a second SSD. Nathan had his own bizarre problems where his CachyOS refused to acknowledge his mouse clicks . He was also hit with option paralysis, trying to choose between four bootloaders and thirteen desktop environments. For some, this is part of the appeal, the infinite customizability, the power to bend your computer to your every whim and make it truly yours. Ubuntu doesn’t throw its customizability in your face, the way Cachy does, but it’s still there if you want it. The problem, as I encountered during my days of meticulously tweaking desktops and crafting my dream note-taking tool using Obsidian , is that you’re building a house of cards. One tiny thing in some seemingly inconsequential library or plugin could cause the whole system to come crumbling down. I don’t want to be wasting my time tweaking and troubleshooting when I already have things that work. Even if you keep it simple, as I did on my XPS, you’re hardly guaranteed smooth sailing. Ubuntu is one of the best-supported Linux distros. And, while my specific XPS 15 isn’t one of them, Dell has sold many Ubuntu-certified laptops over the years, so I figured I was in the clear — wrong. Several apps, whether from the official Ubuntu App Center, downloaded snaps , or .deb packages, quietly failed to install. No obvious error, no nothing. I had to open the terminal and install them from there to see what went wrong. Installing software on Linux is somehow even more confusing than it was 20 years ago. Bitwig proves that Linux can do it all these days. Even the things that worked weren’t seamless. I got Steam up and running, but it took hours , required installing outdated 32-bit libraries, and it crashed along the way. Oh, and Steam games refused to recognize my audio interface; they would only play through the laptop’s speakers. CachyOS or Bazzite would have provided a smoother Steam experience. But it’s also illustrative of the problem. Saying you use Linux is almost meaningless because there are so many different flavors. Thankfully, Bitwig recognized my audio interface, but it was fickle about which MIDI controllers it wanted to work with (an issue I’d never had on the Mac version), and crashed when I first tried to run it. Also, while the native Bitwig instruments are solid, I missed my favorite soft synths and effects, like Arturia’s Pigments , which is Windows- and Mac-only. There were a handful of other minor annoyances. If my laptop went to sleep (which it only seems to do when it’s plugged in, go figure), it refused to reconnect to the external hard drive I have hooked up, and it stopped recognizing the SD card reader. To get them working again, I have to reboot. It also doesn’t wake on input from a Bluetooth mouse or keyboard. I have to open the lid to wake it when it’s hooked up to my dock and an external monitor. If I just need a web browser, Ubuntu works like a charm. I had no issues with my Wi-Fi or with Firefox. My XPS now runs faster and quieter than it did with Windows. And I got Obsidian up and running with minimal fuss. I used Ubuntu as my primary work machine for a few days, and everything was fine. But I can do most of my job from the confines of a web browser. It’s everything else. I can game on my Linux machine, but the experience is smoother on my Switch or PS4. I can make music in Bitwig or Reaper on Ubuntu, but Ableton on macOS is easier to use and supports all my VSTs. GIMP and Darktable offer solid image editing, but let’s be honest, they’re nowhere near as powerful as Lightroom and Photoshop. Linux can do all the things now — sometimes better than Windows. But for all it does, it always feels like there’s a better option. Follow topics and authors from this story to see more like this in your personalized homepage feed and to receive email updates. Terrence O'Brien Analysis Linux Report Tech",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/Screenshot-From-2026-01-15-13-38-29.png?quality=90&strip=all&crop=0%2C10.839398969906%2C100%2C78.321202060188&w=1200"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Brookhaven Lab's RHIC concludes 25-year run with final collisions",
      "url": "https://www.hpcwire.com/off-the-wire/brookhaven-labs-rhic-concludes-25-year-run-with-final-collisions/",
      "published": "2026-02-07T19:07:27+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.hpcwire.com/off-the-wire/brookhaven-labs-rhic-concludes-25-year-run-with-final-collisions/\">https://www.hpcwire.com/off-the-wire/brookhaven-labs-rhic-concludes-25-year-run-with-final-collisions/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46926576\">https://news.ycombinator.com/item?id=46926576</a></p> <p>Points: 71</p> <p># Comments: 58</p>",
      "content_text": "<p>Article URL: <a href=\"https://www.hpcwire.com/off-the-wire/brookhaven-labs-rhic-concludes-25-year-run-with-final-collisions/\">https://www.hpcwire.com/off-the-wire/brookhaven-labs-rhic-concludes-25-year-run-with-final-collisions/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46926576\">https://news.ycombinator.com/item?id=46926576</a></p> <p>Points: 71</p> <p># Comments: 58</p>",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "New York lawmakers propose a three-year pause on new data centers",
      "url": "https://techcrunch.com/2026/02/07/new-york-lawmakers-propose-a-three-year-pause-on-new-data-centers/",
      "published": "2026-02-07T18:23:04+00:00",
      "summary": "Although the bill’s prospects are uncertain, New York is at least the sixth state to consider pausing construction of new data centers.",
      "content_text": "New Yorker state lawmakers have introduced a bill that would impose a moratorium of at least three years on permits tied to the construction and operation of new data centers. While the bill’s prospects are uncertain, Wired reports that New York is at least the sixth state to consider pausing construction of new data centers. As tech companies plan to spend ever-increasing amounts of money to build AI infrastructure, both Democrats and Republicans have expressed concerns about the impact those data centers might have on surrounding communities. Studies have also linked data centers to increased home electricity bills . Critics include progressive Senator Bernie Sanders, who has called for a national moratorium , as well as conservative Florida Governor Ron De Santis, who said data centers will lead to “higher energy bills just so some chatbot can corrupt some 13 year old kid online.” More than 230 environmental groups including Food & Water Watch, Friends of the Earth, and Greenpeace recently signed an open letter to Congress calling for a national moratorium on the construction of new data centers. Eric Weltman of Food & Water Watch told Wired that the New York bill — sponsored by state senator Liz Krueger and assemblymember Anna Kelles, both Democrats — was “our idea.” Data center pauses have also been proposed by Democrats in Georgia, Vermont, and Virginia, while Republicans sponsored similar bills in Maryland and Oklahoma. According to Politico , Krueger described her state as “completely unprepared” for the “massive data centers” that are “gunning for New York.” “It’s time to hit the pause button, give ourselves some breathing room to adopt strong policies on data centers, and avoid getting caught in a bubble that will burst and leave New York utility customers footing a huge bill,” she said. Techcrunch event Boston, MA | June 23, 2026 Last month, New York Governor Kathy Hochul announced a new initiative called Energize NY Development, which her office said would both modernize the way large energy users (i.e., data centers) would connect to the grid while also requiring them to “pay their fair share.”",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2025/06/GettyImages-2217198328.jpeg?resize=1200,800"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Speed up responses with fast mode",
      "url": "https://code.claude.com/docs/en/fast-mode",
      "published": "2026-02-07T18:08:23+00:00",
      "summary": "<p>Article URL: <a href=\"https://code.claude.com/docs/en/fast-mode\">https://code.claude.com/docs/en/fast-mode</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46926043\">https://news.ycombinator.com/item?id=46926043</a></p> <p>Points: 163</p> <p># Comments: 155</p>",
      "content_text": "Fast mode is in research preview . The feature, pricing, and availability may change based on feedback. Fast mode delivers faster Opus 4.6 responses at a higher cost per token. Toggle it on with /fast when you need speed for interactive work like rapid iteration or live debugging, and toggle it off when cost matters more than latency. Fast mode is not a different model. It uses the same Opus 4.6 with a different API configuration that prioritizes speed over cost efficiency. You get identical quality and capabilities, just faster responses. What to know: Use /fast to toggle on fast mode in Claude Code CLI. Also available via /fast in Claude Code VS Code Extension. Fast mode for Opus 4.6 pricing starts at $30/150 MTok. Fast mode is available at a 50% discount for all plans until 11:59pm PT on February 16. Available to all Claude Code users on subscription plans (Pro/Max/Team/Enterprise) and Claude Console. For Claude Code users on subscription plans (Pro/Max/Team/Enterprise), fast mode is available via extra usage only and not included in the subscription rate limits. This page covers how to toggle fast mode , its cost tradeoff , when to use it , requirements , and rate limit behavior . Toggle fast mode Toggle fast mode in either of these ways: Type /fast and press Tab to toggle on or off Set \"fastMode\": true in your user settings file Fast mode persists across sessions. For the best cost efficiency, enable fast mode at the start of a session rather than switching mid-conversation. See understand the cost tradeoff for details. When you enable fast mode: If you’re on a different model, Claude Code automatically switches to Opus 4.6 You’ll see a confirmation message: “Fast mode ON” A small ↯ icon appears next to the prompt while fast mode is active Run /fast again at any time to check whether fast mode is on or off When you disable fast mode with /fast again, you remain on Opus 4.6. The model does not revert to your previous model. To switch to a different model, use /model . Understand the cost tradeoff Fast mode has higher per-token pricing than standard Opus 4.6: Mode Input (MTok) Output (MTok) Fast mode on Opus 4.6 (<200K) $30 $150 Fast mode on Opus 4.6 (>200K) $60 $225 Fast mode is compatible with the 1M token extended context window. When you switch into fast mode mid-conversation, you pay the full fast mode uncached input token price for the entire conversation context. This costs more than if you had enabled fast mode from the start. Decide when to use fast mode Fast mode is best for interactive work where response latency matters more than cost: Rapid iteration on code changes Live debugging sessions Time-sensitive work with tight deadlines Standard mode is better for: Long autonomous tasks where speed matters less Batch processing or CI/CD pipelines Cost-sensitive workloads Fast mode vs effort level Fast mode and effort level both affect response speed, but differently: Setting Effect Fast mode Same model quality, lower latency, higher cost Lower effort level Less thinking time, faster responses, potentially lower quality on complex tasks You can combine both: use fast mode with a lower effort level for maximum speed on straightforward tasks. Requirements Fast mode requires all of the following: Not available on third-party cloud providers : fast mode is not available on Amazon Bedrock, Google Vertex AI, or Microsoft Azure Foundry. Fast mode is available through the Anthropic Console API and for Claude subscription plans using extra usage. Extra usage enabled : your account must have extra usage enabled, which allows billing beyond your plan’s included usage. For individual accounts, enable this in your Console billing settings . For Teams and Enterprise, an admin must enable extra usage for the organization. Fast mode usage is billed directly to extra usage, even if you have remaining usage on your plan. This means fast mode tokens do not count against your plan’s included usage and are charged at the fast mode rate from the first token. Admin enablement for Teams and Enterprise : fast mode is disabled by default for Teams and Enterprise organizations. An admin must explicitly enable fast mode before users can access it. If your admin has not enabled fast mode for your organization, the /fast command will show “Fast mode has been disabled by your organization.” Enable fast mode for your organization Admins can enable fast mode in: Handle rate limits Fast mode has separate rate limits from standard Opus 4.6. When you hit the fast mode rate limit or run out of extra usage credits: Fast mode automatically falls back to standard Opus 4.6 The ↯ icon turns gray to indicate cooldown You continue working at standard speed and pricing When the cooldown expires, fast mode automatically re-enables To disable fast mode manually instead of waiting for cooldown, run /fast again. Research preview Fast mode is a research preview feature. This means: The feature may change based on feedback Availability and pricing are subject to change The underlying API configuration may evolve Report issues or feedback through your usual Anthropic support channels. See also",
      "cover_image_url": "https://claude-code.mintlify.app/_next/image?url=%2F_mintlify%2Fapi%2Fog%3Fdivision%3DConfiguration%26appearance%3Dsystem%26title%3DSpeed%2Bup%2Bresponses%2Bwith%2Bfast%2Bmode%26description%3DGet%2Bfaster%2BOpus%2B4.6%2Bresponses%2Bin%2BClaude%2BCode%2Bby%2Btoggling%2Bfast%2Bmode.%26logoLight%3Dhttps%253A%252F%252Fmintcdn.com%252Fclaude-code%252Fo69F7a6qoW9vboof%252Flogo%252Flight.svg%253Ffit%253Dmax%2526auto%253Dformat%2526n%253Do69F7a6qoW9vboof%2526q%253D85%2526s%253D536eade682636e84231afce2577f9509%26logoDark%3Dhttps%253A%252F%252Fmintcdn.com%252Fclaude-code%252Fo69F7a6qoW9vboof%252Flogo%252Fdark.svg%253Ffit%253Dmax%2526auto%253Dformat%2526n%253Do69F7a6qoW9vboof%2526q%253D85%2526s%253D0766b3221061e80143e9f300733e640b%26primaryColor%3D%25230E0E0E%26lightColor%3D%2523D4A27F%26darkColor%3D%25230E0E0E%26backgroundLight%3D%2523FDFDF7%26backgroundDark%3D%252309090B&w=1200&q=100"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "GitHub - Momciloo/fun-with-clip-path",
      "url": "https://github.com/Momciloo/fun-with-clip-path",
      "published": "2026-02-07T17:45:27+00:00",
      "summary": "<p>Two clip-paths, over the navigation:<p>- The first clip-path is a circle (top-left corner) - The second clip-path is a polygon, that acts like a ray (hardcoded, can be improved)<p>The original work by Iventions Events <a href=\"https://iventions.com/\" rel=\"nofollow\">https://iventions.com/</a> uses JavaScript, but I found CSS-only approach more fun<p>Here's a demo and the codebase: <a href=\"https://github.com/Momciloo/fun-with-clip-path\" rel=\"nofollow\">https://github.com/Momciloo/fun-with-clip-path</a></p> <hr /> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46925811\">https://news.ycombinator.com/item?id=46925811</a></p> <p>Points: 79</p> <p># Comments: 16</p>",
      "content_text": "Momciloo/fun-with-clip-path You can’t perform that action at this time.",
      "cover_image_url": "https://opengraph.githubassets.com/dce3ce8309a33d283e584fd11ec3763d23a9e5080bc8e20e0bfd7e1294171736/Momciloo/fun-with-clip-path"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Jonathan Whiting",
      "url": "https://jonathanwhiting.com/writing/blog/games_in_c/",
      "published": "2026-02-07T17:45:25+00:00",
      "summary": "<p>Article URL: <a href=\"https://jonathanwhiting.com/writing/blog/games_in_c/\">https://jonathanwhiting.com/writing/blog/games_in_c/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46925808\">https://news.ycombinator.com/item?id=46925808</a></p> <p>Points: 188</p> <p># Comments: 172</p>",
      "content_text": "Why I Write Games in C (yes, C) I am an unusual beast. All my solo project games I've been making recently have been written in 'vanilla' C. Nobody does this. So I think it might be interesting to explain why I do. Dry programming language opinions incoming, you have been warned . What I need from a language There's some things which are non-negotiable. First of, it has to be reliable. I can't afford to spend my time dealing with bugs I didn't cause myself. A lot of my games were written for flash, and now flash is dying. I do not want to spend my time porting old games to new platforms, I want to make new games. I need a platform that I am confident will be around for a while. Similarly I want to avoid tying myself to a particular OS, and ideally I'd like to have the option of developing for consoles. So it's important that my programming language is portable, and that it has good portable library support. What I want from a language The strongest thing on my desired, but not required list is simplicity. I find looking up language features, and quirky 'clever' api's incredibly tiring. The ideal language would be one I can memorize, and then never have to look things up. Dealing with bugs is huge creative drain. I want to produce less bugs, so I want strict typing, strong warning messages and static code analysis. I want bugs to be easier to find, so I want good debuggers and dynamic analysis. I'm not interesting in high-def realism, but I do still care a bit about performance. Having more cycles available broadens the palette of things you can do. It's particularly interesting to explore what is possible with modern, powerful computers if you aren't persuing fidelity. Even more than that I care about the speed of the compiler. I am not a zen master of focus, and waiting 10+ seconds is wasteful, yes, but more importantly it breaks my flow. I flick over to Twitter and suddenly 5+ minutes are gone. I am not an OOP convert. I've spent most of my professional life working with classes and objects, but the more time I spend, the less I understand why you'd want to combine code and data so rigidly. I want to handle data as data and write the code that best fits a particular situation. The Alternatives C++ is still the most common language for writing games, and not without reason. I still do almost all of my contract work in it. I dislike it intensely. C++ covers my needs, but fails my wants badly. It is desperately complicated. Despite decent tooling it's easy to create insidious bugs. It is also slow to compile compared to C. It is high performance, and it offers features that C doesn't have; but features I don't want, and at a great complexity cost. C# and Java have similar issues. They are verbose and complex beasts, and I am searching for a concise, simple creature. They both do a lot to railroad a programmer into a strongly OOP style that I am opposed to. As per most higher level languages they have a tendency to hide away complexity in a way that doesn't actually prevent it from biting you. I like Go a lot. In many ways it is C revisited, taking into account what has be learnt in the long years since it was released. I would like to use it, but there are big roadblocks that prevent me. The stop-the-world garbage collection is a big pain for games, stopping the world is something you can't really afford to do. The library support for games is quite poor, and though you can wrap C libs without much trouble, doing so adds a lot of busy work. It is niche enough that I worry a little about long term relevance. It would be nice to make things for the web, but it feels like a terrifyingly fast moving enviroment. It is particularly scary with the death of flash. I really dislike javascript, it is so loose that I marvel that people are able to write big chunks of software in it. I have no interest in trying. Haxe feels much more promising than most alternatives. If I do web stuff again I'll be diving in here. There is some good library support. I am a little concerned by its relative youth, will it last? I don't much have else to say about it though, I've only dabbled with the surface. Some people just say screw it, I'll write my own language, the language I want to use. I admire this, and sometimes I toy with the idea of doing the same. It feels like too much to throw away all existing library support, and taking full responsibilty for future compatibility. It is also very difficult, and when it comes down to it I would rather be making games than programming languages. Why C is still my best fit C is dangerous, but it is reliable. A very sharp knife that can cut fingers as well as veg, but so simple it's not too hard to learn to use it carefully. It is fast, and when it comes to compilation I can't think of anything faster. It can be made to run on just about anything. Usually this is relatively easy. It is hard to imagine a time when this won't be the case. The library and tooling support is strong and ongoing. I say this with some sadness, but it is still the language for me. I absolutely DO NOT mean to say \"hey, you should use C too\". I full appeciate preferences here are pretty specific and unusual. I have also already written more 'vanilla' C code than most, and this certainly is part of my comfort. So yeah, that's it :-) Back to main site",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "xorvoid",
      "url": "https://xorvoid.com/sectorc.html",
      "published": "2026-02-07T17:39:53+00:00",
      "summary": "<p>Article URL: <a href=\"https://xorvoid.com/sectorc.html\">https://xorvoid.com/sectorc.html</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46925741\">https://news.ycombinator.com/item?id=46925741</a></p> <p>Points: 246</p> <p># Comments: 48</p>",
      "content_text": "SectorC: A C Compiler in 512 bytes SectorC ( github ) is a C compiler written in x86-16 assembly that fits within the 512 byte boot sector of an x86 machine. It supports a subset of C that is large enough to write real and interesting programs. It is quite likely the smallest C compiler ever written. In a base64 encoding, it looks like this: 6gUAwAdoADAfaAAgBzH/6DABPfQYdQXoJQHr8+gjAVOJP+gSALDDqluB+9lQdeAG/zdoAEAfy+gI AegFAYnYg/hNdFuE9nQNsOiqiwcp+IPoAqvr4j3/FXUG6OUAquvXPVgYdQXoJgDrGj0C2nUGV+gb AOsF6CgA68Ow6apYKfiD6AKrifgp8CaJRP7rrOg4ALiFwKu4D4Srq1fonP9ewz2N/HUV6JoA6BkA ieu4iQRQuIs26IAAWKvD6AcAieu4iQbrc4nd6HkA6HYA6DgAHg4fvq8Bra052HQGhcB19h/DrVCw UKroWQDoGwC4WZGrW4D/wHUMuDnIq7i4AKu4AA+ridirH8M9jfx1COgzALiLBOucg/j4dQXorf/r JIP49nUI6BwAuI0G6wyE0nQFsLiq6wa4iwarAduJ2KvrA+gAAOhLADwgfvkx2zHJPDkPnsI8IH4S weEIiMFr2wqD6DABw+gqAOvqicg9Ly90Dj0qL3QSPSkoD5TGidjD6BAAPAp1+eu86Ln/g/jDdfjr slIx9osEMQQ8O3QUuAACMdLNFIDkgHX0PDt1BIkEMcBaw/v/A8H9/yvB+v/34fb/I8FMAAvBLgAz wYQA0+CaANP4jwCUwHf/lcAMAJzADgCfwIUAnsCZAJ3AAAAAAAAAAAAAAAAAAAAAAAAAAAAAVao= Supported language A fairly large subset is supported: global variables, functions, if statements, while statements, lots of operators, pointer dereference, inline machine-code, comments, etc. All of these features make it quite capable. For example, the following program animates a moving sine-wave: int y; int x; int x_0; void sin_positive_approx() { y = ( x_0 * ( 157 - x_0 ) ) >> 7; } void sin() { x_0 = x; while( x_0 > 314 ){ x_0 = x_0 - 314; } if( x_0 <= 157 ){ sin_positive_approx(); } if( x_0 > 157 ){ x_0 = x_0 - 157; sin_positive_approx(); y = 0 - y; } y = 100 + y; } int offset; int x_end; void draw_sine_wave() { x = offset; x_end = x + 314; while( x <= x_end ){ sin(); pixel_x = x - offset; pixel_y = y; vga_set_pixel(); x = x + 1; } } int v_1; int v_2; void delay() { v_1 = 0; while( v_1 < 50 ){ v_2 = 0; while( v_2 < 10000 ){ v_2 = v_2 + 1; } v_1 = v_1 + 1; } } void main() { vga_init(); offset = 0; while( 1 ){ vga_clear(); draw_sine_wave(); delay(); offset = offset + 1; if( offset >= 314 ){ // mod the value to avoid 2^16 integer overflow offset = offset - 314; } } } Screenshot But, how? When I started thinking about SectorC, I had just finished Deobfuscating OTCC with a lot of its ideas freshly loaded into my head. I also just had some healthy doses of justine.lol and Tom7 to inspire the absurdity of it all. Did I think I would succeed? I suspected NO. Fit an entire C compiler in 510 bytes of instruction memory? Good luck (sarcasm). Tokenizing The first problem came quickly. In C, the tokenizer/lexer alone seems larger than one 512 byte sector! We need to consume an arbitrary stream of bytes and produce ‚Äútokens‚Äù. For example: int main() { if( a < 5 ){ func(); } } Would be consumed and converted into: 'int' TOKEN_KEYWORD_INT 'main' TOKEN_IDENTIFIER '(' TOKEN_LPAREN ')' TOKEN_RPAREN '{' TOKEN_LBRACE 'if' TOKEN_KEYWORD_IF '(' TOKEN_LPAREN 'a' TOKEN_IDENTIFIER '<' TOKEN_OPERATOR '5' TOKEN_NUMBER ')' TOKEN_RPAREN '{' TOKEN_LBRACE 'func' TOKEN_IDENTIFIER '(' TOKEN_LPAREN ')' TOKEN_RPAREN ';' TOKEN_SEMI '}' TOKEN_RBRACE '}' TOKEN_RBRACE We need to specifically recognize keywords , identifiers , operators , and numbers . And then we need to convert numbers from string to integer with something like atoi() : int atoi(const char *s) { int n = 0; while (1) { char c = *s++; if (!c) break; n = 10 * n + (c - '0'); } return n; } I wrote a fairly straight-forward and minimalist lexer and it took >150 lines of C code. A crude estimate of the same code in x86-16 would require 300-450 bytes minimum (e.g. a simple add ax,bx instruction encodes as 2 bytes). And this doesn‚Äôt include any symbol table, recursive-descent parser, code-generator, branch-patching, etc. No Chance. So, naturally ‚Ä¶ I continued. Always pick the losers. The lolz are more fun that way. Big Insight #1 Big Insight #1 came while thinking about other languages such as Forth. The tokenizer in Forth is nearly trivial. Every token is simply space-delimited. Every token is just called a WORD and nothing is special (slight lie). Hmm, how about a C that does that? So dreamed up a C that is technically still a C, is probably turing-complete, and will definitely make every code maintainer terrified. üòè I will call it the Barely C Programming Language : int done , a , b , c , p , cond ; int(main)(){while(!done){ a = b - c ; *(int*) p = b - c ; a = *(int*) p ; if(cond) a = b - 45 ; }} Here we have spacing strategically placed to create ‚Äúmega-tokens‚Äù For example: int(main)(){while(!done){ is one such \"mega-token\". In a sense, we actually have a language more like: VAR_BEGIN done AND a AND b AND c AND p AND cond END MAIN_BEGIN a = b - c END DEREF p = b - c END a = DEREF p END COND a = b - 45 END MAIN_END But, a normal C compiler will also recognize it as C! Even after using space-delimiters, we still have a lot of tokens and need to find more ways to minimize the tokenizer. What is essential? Well it‚Äôs quite hard to avoid the atoi() if we want to actually have integer literals. What else do we need? How about nothing. Big Insight #2 Big Insight #2 is that atoi() behaves as a (bad) hash function on ordinary text. It consumes characters and updates a 16-bit integer. Hashes are perhaps the holy-grail of computer-science. With a good hash, we can just side-step all the hard problems by trading them for an even harder problem (hash collisions), and then we just ignore that harder problem. Brilliant. (sticks fingers in ears) ü§™ So we have this: Token Type Meaning of atoi() Integer Literal uint16 number Keyword token ‚Äùenum‚Äù value Identifier hash value into a 64K array Implementing Barely C The first implementation of Barely C fit in 468 bytes. It was a simple recursive-descent parser over the atoi tokens. There was no symbol table of any kind. Variables simply access a 64K segment using the hash value. Codegen is emitted somewhat similar to OTCC , using ax as the result register and shuffling values to the stack and then to cx for binary operators. Minimizing with Byte-Threaded Code In an attempt to steal every good idea Forth ever had, I then dreamed up what I will call ‚Äúbyte-threaded-code‚Äù. Since a sector is 512 bytes, if we simply align address on a 2-byte boundary, we can do addressing with a single byte! We can have a series of ‚Äúgadgets‚Äù and do forth-style threading: bits 16 cpu 386 jmp 0x07c0:entry entry: push cs pop ds lea si,operations next: xor ax,ax lodsb add ax,ax push next jmp ax putch: mov ah,0x01 mov al,bl mov dx,0 int 0x14 ret align 2 hang: jmp hang align 2 print_F: mov bx,'F' jmp putch align 2 print_G: mov bx,'G' jmp putch operations: db 17 ; print_F db 20 ; print_G db 17 ; print_F db 17 ; print_F db 20 ; print_G db 17 ; print_F db 17 ; print_F db 17 ; print_F db 20 ; print_G db 16 ; hang Annoyingly, nasm won‚Äôt let you do something like db print_F/2 so I had to write a custom little assembler to do it. Alas, this idea didn‚Äôt work out. In 512 bytes, the overhead of this Forth-style computation model doesn‚Äôt pay for itself. There are a lot of little overheads: 2 byte alignment, extra ret instructions, calling other ‚Äúthreads‚Äù, the next function, etc. The byte-threaded version of Barely C ended up at the same size as the straight-forward version However, the idea is fun and I decided to document it anyways in the event that someone else finds utility. Minimizing the Straight-Forward version Instead, I returned to the straight-forward version and minimized it as much as possible. From 468 bytes ‚áí 303 bytes (165 bytes saving): 510 - 303 ‚áí 207 spare bytes to use for new features! Some tricks: Reorganize code to allow ‚Äúfall-through‚Äù instead of jmp or call Use tail-calls via jmp wherever possible Perform call-fusion (e.g. call tok_next2 instead of call tok_next; call tok_next ) Utilize stosw and lodsw extensively Eliminate machine code tables for cheaper inline stosw versions Prefer cmp ax,imm over cmp bx,imm Keep jump offsets within 8-bits to encode more efficiently Look Ma, A Real C! As it turns out, a lot can be accomplished in 200 bytes if you already have a tokenizer, parser, and code-generator in 300 bytes. With these 200 bytes, Barely C became a proper C: Arbitrarily nested if statement block with an arbitrary expression condition Arbitrarily nested while statement block with an arbitrary expression condition Lots of operators: + , - , * , & , | , ^ , << , >> , == , != , < , > , <= , >= Grouping expressions: ( expression ) Function definitions and recursive function calls (using func() as a hash value into a symbol table at segment 0x3000) A special asm statement for inline machine-code Single-line // comments Multi-line /* comments A trick to do ‚Äúspace-injection‚Äù before semicolons to make code look more normal The biggest enabler here is the binary_oper_tbl which allows for a very cheap way to add lots of operations. Each operator is simply a <16-bit token-value> <16-bit-machine-code> pair, costing just 4 bytes.The above 14 operators cost just 56 bytes plus a little overhead to scan the table. Grammar Here's the full grammer specification: program = (var_decl | func_decl)+ var_decl = \"int\" identifier \";\" func_decl = \"void\" func_name \"{\" statement* \"}\" func_name = <identifier that ends in \"()\" with no space> statement = \"if(\" expr \"){\" statement* \"}\" | \"while(\" expr \"){\" statement* \"}\" | \"asm\" integer \";\" | func_name \";\" | assign_expr \";\" assign_expr = deref? identifier \"=\" expr deref = \"*(int*)\" expr = unary (op unary)? unary = deref identifier | \"&\" identifier | \"(\" expr \")\" | indentifier | integer op = \"+\" | \"-\" | \"&\" | \"|\" | \"^\" | \"<<\" | \">>\" | \"==\" | \"!=\" | \"<\" | \">\" | \"<=\" | \">=\" In addition, both // comment and /* multi-line comment */ styles are supported. (NOTE: This grammar is 704 bytes in ascii, 38% larger than it's implementation!) Inline Machine-Code A programming language without I/O is useless. And, as the C language is defined in an I/O agnostic way, we need some way out. Thus, an asm extension is supported. This allows programs to generate raw x86-16 machine code literals inline. Using asm , programs can access any low-level detail of the machine. This is used extensively in the example code. Error-Handling What is ‚Äúerror-handling‚Äù? ü§£ In traditional C style, we trust the programmer to write correct and well-formed programs. We are certain they are all minor gods and goddesses with the ability of perfection. Obviously, spending bytes on error-checking would be foolish. Surely all will agree that this is a very reasonable standard. For the less divine among us, a lint was also written (that doesn‚Äôt fit in a sector) to detect errors. The author certainly didn‚Äôt require this tool for development. Runtime If C compiler writers were a secret shadow organization like the Free Masons, Illuminati, Lizard Peoples, or Pizzagaters our inner-secret would be ‚ÄúC actually has a runtime‚Äù. SectorC has a runtime under rt/ consisting of two files implemented in C itself: rt/lib.c: A collection of library routines, often coded in inline asm rt/_start.c: The actual entry-point _start() The runtime code is concatenated with program source to construct the full source to compile and run. Examples A few examples are provided that leverage the unique hardware aspects of the x86-16 IBM PC: examples/hello.c: Print a text greeting on the screen writing to memory at 0xB8000 examples/sinwave.c: Draw a moving sine wave animation with VGA Mode 0x13 using an appropriately bad approximation of sin(x) examples/twinkle.c: Play ‚ÄúTwinkle Twinkle Little Star‚Äù through the PC Speaker (Warning: LOUD) Conclusion It seems fitting to end an article with ‚Äútakeaways‚Äù or ‚Äúwhat did we learn‚Äù. So.. umm.. what did we learn? Honestly, I‚Äôm not sure. But in the interest of fun, here‚Äôs a Choice Your Own Adventure version of What Did We Learn: What Did We Learn Your Chosen Adventure Things that seem impossible often aren‚Äôt and we should Just Do It anyway Move to the South Pole with absolutely no gear on a Homesteading Mission Software is too bloated these days, we only need a few KBs Go check yourself into the technology hippie-commune of suckless Error checking is overrated Take Elon up on his pitch to be a mars astronaut because Earth really doesn‚Äôt need more software that ignores errors. Anything X can do, C can do better Something like this? link . Monzy, we need a new rap! (call me) That was all gibberish nonsense and thank you for wasting my life (passive-aggression) Feel regret that you wasted the time because there is a lot better content in the world you‚Äôd rather consume and decide to get a therapist to work through your issues with reading nonsense internet gibberish This xorvoid person/robot/AI is ridiculous/absurd/dumb and does arguably pointless things for fun Follow, like, subscribe, ring the bell.. üòÅ (rss)",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "LG’s C5 TV and Anker’s powerful power bank are this week’s best deals",
      "url": "https://www.theverge.com/gadgets/874585/lg-c5-tv-anker-power-bank-deal-sale-super-bowl",
      "published": "2026-02-07T16:00:00+00:00",
      "summary": "It’s the first weekend of February, which can only mean one thing: The Super Bowl (or “Benito Bowl,” if you’re mainly tuning in for the Bad Bunny-led halftime show) is upon us. The game is Sunday, February 8th, at 6:30PM ET on Peacock, NFL Plus, and on live services that include NBC. With that out [&#8230;]",
      "content_text": "It’s the first weekend of February, which can only mean one thing: The Super Bowl (or “ Benito Bowl ,” if you’re mainly tuning in for the Bad Bunny-led halftime show) is upon us. The game is Sunday, February 8th, at 6:30PM ET on Peacock , NFL Plus , and on live services that include NBC. With that out of the way, I’m here to surface this week’s best deals. There were plenty of good ones, from cheap yet extremely handy two-headed USB-C cables and a $20 sci-fi first-person shooter PC game bundle to steep discounts on big OLED TVs that will impress people at your Super Bowl watch party. If you happen to be in the market for a 65-inch OLED 4K TV (can I come over?), the LG C5 is almost half off its original asking price of $2,699. Whether you buy one through Amazon for $1,397 or from Best Buy at $1399.99, there’s a good chance that you can have it installed by kickoff on Sunday. Amazon currently has fast shipping for this TV, and many Best Buy brick-and-mortar locations are likely to have this model in stock. LG’s C-series TVs are some of the most popular OLED models each year. With pixel-level brightness control, they boast excellent black levels and color accuracy, and are an incredible value for movie lovers, gamers, sports fans, and everyone in between. However, if you’re looking for the best of the best when it comes to brightness, Samsung’s pricier S95F beats the C5. And, while LG’s glossy screen looks fantastic, it’ll show more reflections than the S95F’s matte finish. It’s been a while since we featured a good deal on Anker’s Laptop Power Bank , its 25,000mAh / 90Wh powerhouse that peaks at 100W when charging a single device (or up to 165W with two USB-C devices). Now, we’re featuring its best-ever price that’s currently happening at Newegg and through Anker itself. You can buy the black model at its all-time low price of $89.99 ($45 off) directly from Anker with code VergeYWP0QJDE . This power bank’s output speed, capacity, and selection of four ports (three USB-C, one USB-A) are good and all, but it’s the small details that make this one stand out. Notably, it comes with two built-in USB-C cables, one of which retracts while the other doubles as handle. For those who own an iPhone 12 or newer, or a Google Pixel 10, Belkin’s Stage Powergrip is exactly the accessory you may not have realized you wanted. It’s a 9,300mAh battery-equipped gadget that adds a camera grip to your phone, snapping on snugly with MagSafe or Pixelsnap magnets, depending on the device you have. It lets you snap photos with its dedicated shutter button, and the grip can swivel to let you get the types of shots you want without feeling like you’re endangering your precious phone. There’s been a deal happening all week on the black model, which costs $68.39 at Amazon (originally $79.99, the current cost of the blue and sand-colored options). More ways to save this weekend",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/lgc5lede.png?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Al Lowe on model trains, funny deaths and working with Disney",
      "url": "https://spillhistorie.no/2026/02/06/interview-with-sierra-veteran-al-lowe/",
      "published": "2026-02-07T15:52:42+00:00",
      "summary": "<p>Article URL: <a href=\"https://spillhistorie.no/2026/02/06/interview-with-sierra-veteran-al-lowe/\">https://spillhistorie.no/2026/02/06/interview-with-sierra-veteran-al-lowe/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46924790\">https://news.ycombinator.com/item?id=46924790</a></p> <p>Points: 102</p> <p># Comments: 22</p>",
      "content_text": "You know he did other things than Larry, right? We talked with Al Lowe about Leisure Suit Larry, but mostly other stuff. This was Roar’s first interview and he ran out of time before he could dive into what Al is mostly known for. Al Lowe is an American software developer who started making games in the early eighties. His is mostly known for the Leisure Suit Larry games, created with Sierra, but he is also a musical man who loves playing his sax and still plays with his model trains. He has also created lots of other games for Sierra, not only the Larry series. If you think Al is great, but you already know all you need to know about Larry, then this is the article for you. And it is pretty long. Get your coffee now, before continuing… Roar: Have you ever been to Norway? Al: Oh yeah, sure. I have this eyeball right here. It works because I spent a week recuperating in the Department of Ophthalmology at Oslo University Hospital. I had a detached retina when we were flying into Oslo, and went to an ophthalmologist, and she said to get to the hospital right now. The next morning a bunch of Norwegian women started poking around in my eye and fixed it. Al Lowe in 2025, cut from Roars interview. Of course, I was blind in that eye for the next week, and I was already blind in the other eye, so I wandered around in the dark while my wife went sightseeing. I went with her, but I didn’t see any sights. Roar: So you have felt and smelt the beautiful fjords, but never seen them. Al: Well, on an earlier trip, we did take the railroad to Bergen. Over the mountain plateau [Hardangervidda]. So we spent some time there, and that was fun. Roar: I’ve never actually taken that trip myself, but I heard it’s wonderful. Al: It was not for us. We got to the top, up in the top of the mountains. And the electricity went off. And so, we parked in the middle of this beautiful snowy scene for three hours, before they finally got the power back on. Then we went on our journey but we had already missed the connecting rides. We had booked a trip on a boat through the fjords, and a trip on the cog railway. We missed all of them. So be sure you go when there is electricity. Roar: Yeah, we like to joke about that in Norway, the richest country in the world and the lousiest trains. I guess that is a good segue, because you do like trains, don’t you? At least model trains? Al: Yes, I do. I’ve been a model railroader for most of my life. I got my first model train when I was 2 years old, and my dad wouldn’t let me play with it. So he ran it around the Christmas tree and I had to watch. But eventually, when I was 9 or 10, I got interested in HO size trains, and built a layout in our basement. Completely by myself, I had no idea what I was doing or anything, but I just learned as I went, piddled around and figured it out. I went to the hobby shop sometime and would say stuff like: How do you glue this stuff to this stuff? And they’d give me some special glue and and teach me how. Al Lowe put trains in his educational game Donald Duck’s Playground. Photo: MobyGames . So, it was fun. I learned a lot, and I proved that I could do things with my hands. I ended up learning a lot about wiring and electricity. And scenery, I did stuff with papier-mâché. I took old tomato sticks from our garden, cut them in different heights. I wrapped old screen door wire over the top of them and coated that with plaster to make mountains and stuff. So I did a lot of stuff, mostly things I’d read about in magazines, but I think it helped me understand that I could learn something if I didn’t already know it. Roar: Did you add any miniature figures or models and stuff like that in the scenery as well? Al: Sure, yeah, lots of stuff. In fact, I built buildings out of balsa wood. And I read an article in a magazine that said if you hold your X-Acto knife at just the right angle and drag it across the balsa wood, you can make clapboard siding, you know, where the siding overlaps. I also built doors from scraps of wood. Anyway, it was fun. It’s a fun hobby, and it served me for ten years at first. Then I went to college and started playing music, and I forgot about it. But then when my son was 10 years old I thought: I liked doing this when I was 10, maybe he will. So we went to the hobby shop and bought a bunch of trains. Of course he wasn’t interested at all, but I got hooked again! So for the last 35 years I’ve been doing N-scale trains, where the locomotive is about five inches long. Roar: So your son is 45, then? Al: Yes! Roar: Yes, then you could have been my dad, because I’m 46. Al: Well, you’re the right age, then, I guess. National Model Railroad Association Magazine. Photo: E-bay . Model trains are not what they used to be Roar: I was sent a link to the National Model Railroad Association where you are mentioned as a member of the board of directors of the 4th Division of the Pacific Northwest region. Al: The Pacific Northwest Region of the NMRA, yeah. I did that for 10 years at least. Then I was assistant director for another 10 years so I worked at that organization for about 20 years. But recently I said: That’s enough, let some new blood come in. So we got new people now, and they’re much better, so I’m happy. Roar: But you didn’t get any money for it? Al: No, no, it’s a volunteer organization. At first we did a paper newsletter every month. We would print and fold and staple and stamp and address-label hundreds of newsletters. I did that for a couple years until I finally convinced everybody that we’ve got to go online. So then I made a PDF, posted it, and we sent out an email. That was much better, but we still had a few die-hard people that would download it and print it out every time. I was part of this for 24 years. Roar: Are there any physical magazines about model trains today? Al: Yes. Well, the man who ran one of the biggest ones just passed away and so his magazine’s gone now, but there’s still a magazine called N Scale, and it’s housed right here in the suburb of Seattle. It is distributed all over the world. And then there’s the National Model Railroad Association, which publishes a magazine every month, all over the world. And then there’s Model Railroader and Railroad Model Craftsman. I know there’s one for each of the scales, for HO and S and O and G for garden railroads. N Scale Railroading. Photo: N Scale . Al: It’s an old hobby. A weird hobby, in that it follows my path. Most people get interested in trains when they’re little kids. And then they grow out of it in high school. Then later in life, they realize: I got time and money now. I could get back in it, this is a good hobby. And it really is. It’s great socially, because the group that I’m in have a layout that’s portable and we can bring it to shows. So 6-8 times a year we pack everything up into a trailer, move it to a place, and set it up. And we run trains all weekend, we talk with the public and hundreds of people come by and see my work. Then we pack it up and put it back in the trailer and then wait for the next show. So it doesn’t get old because it’s so long between shows. It’s really fun. Roar: I guess there’s not that much development in the field, so even though you haven’t done it for 20 years, it’s not that hard to just continue where you left off? Al: Actually, it has changed completely with computers. So you couldn’t be more wrong! Actually, you were correct for 40 years at least. But in the 90s people started developing computerized means of running the trains. And in the mid-90s, the National Model Railroad Association set up standards and said: If you follow all these rules then your stuff can run on anybody’s system. And that’s when it really changed over. About 2001 or 2002, we went from the old system, which was a rheostat that controlled the amount of voltage applied to the rails, just like a volume control. So we went from a potentiometer that changed the amount of power to the tracks to constantly powered tracks, with a computer chip inside each locomotive. It has a number address that you can set. Then on your remote control you can type in the number of the locomotive you want to drive, turn a knob, and it sends a radio signal through the air to a computer that’s permanently affixed to the layout. And that computer multiplexes a serial signal on top of the power, just kind of like your old phone modems where they overlaid a signal on top of the regular phone lines. It then tells the locomotive to go this fast, back up, or sound the whistle and so on. Roar: I guess for you this was not a bad thing at all, because you love that kind of stuff. Al: Oh, no, right? It was right up my the alley there. N Scale is really small. Photo: Freddie10538 (public domain). Roar: I have some experience painting Warhammer miniature figures. Did you paint miniatures yourself? Al: Yeah. The trouble with N Scale is that the people are very small. It’s a challenge to paint. We always tell everybody that the hard part is getting the eyelashes right. Roar: And that’s at least one thing that does not get easier with age. Al: No, no, that’s true. But I have lots of special tools for that. I have an optimizer that has LED lights built around it, so I can zoom right in on things and paint the tiny little stuff. Roar: Pretending to be a cyborg. Al: Yeah, well … getting the help I need. Al Lowe and music Roar: Music is very important to you. And people wouldn’t think about music when they hear your name. But for me, I got this mp3-file in the late 90s with the title: «Al Lowe – For Your Thighs Only». And that played the music from Leisure Suit Larry. So then I learned the name. And of course, I had played these games earlier, but never picked up on the name. Al: I hope you paid me a royalty. Roar: Yeah, of course! So for me you were a musician before I found out that you had also created these games. Al: Well, I was involved with a lot of Sierra products in one way or another. I worked on their spelling program when they had a word processor. I worked on their assembler for Apple II. I worked on … God, just a lot of different stuff. 26 products in 16 years. Al Lowe’s website. Oh, yeah, I should plug my website . Tell people, if they have gotten this far, they should visit my website. There’s lots more stuff there. If you enjoy this, you’ll enjoy that. Roar: Yeah, I believe I will. Your kids, did they go for music, game developing, or anything else in their father’s footsteps? Al: Of course not. They’re my kids. Roar: Since you’re a saxophone player, I would guess that you can play a lot of other instruments as well? Al: Well, I was a high school band director, so I had to learn to play every instrument somewhat. So, I can pick up a viola and play badly, and I can play a trombone or French horn, but my real instrument is the saxophone and also flute and clarinet. Roar: So, what kind of music do you like? Al: Big Band Jazz. But I play as much as I listen. I tend to watch films and read books, but I play music. I’m in two big bands, where I play lead alto sax. And they meet every week for a couple hours in the evening. And I’m in two saxophone quartets that meet a little less regularly, but we play classical saxophone music, and also some pop songs and things, but it’s just four saxophones, and so we get a lot of interesting work. That’s real fun. Roar: I’ve been looking at this YouTube video , where you played in a band called The David Hasselhoff Big Band. Al: Yes, the group is ironically named. Al Lowe and The David Hasselhoff Big Band. Photo: Youtube . Roar: Metallica had been covered by a band of four cello players. Would it be possible to do something like that with saxophones? Al: It would absolutely be possible, but the question is, why would you do that? Roar: That is the better question. I guess being Norwegian, I don’t really feel like any music should be without some kind of growling. Al Lowe and television Roar: Do you watch any modern series on television? Al: We watch a lot of Scandinavian noir shows. A lot of Danish and a couple of Norwegian shows. And we watch them in Norwegian with subtitles. We don’t want to use the English voiceover, because they are usually not very good. Roar: No, I would guess not. Al: It is easier and better just to listen to the Norwegian and not know what you’re saying, but be able to read the subtitles and get the gist. I can’t think of a title right now, but we’ve got a long list on Netflix. Roar: Yeah. Today it’s very easy to consume whatever you want to consume. Even though you don’t necessarily own the stuff you watch. Al: From very early on I didn’t want to own any movies or TV shows. We never bought anything. I always felt like I would rather just rent and see something new – see more stuff, instead of looking at the same thing over and over. And so, I didn’t build up a collection of VHS or DVDs. I was «streaming» when that was still analog. We got discs in the mail, you know. That was pretty advanced. Roar: Yeah like Netflix, the original Netflix? Al: Yeah. Leisure Suit Larry. Photo: Mobygames. Al Lowe and humor Roar: What about humor? The modern humor scene versus the old humor scene. Some things that were acceptable to say back in the eighties are not any more, and so on, and we even rewrite old things to make them more «modern». Do you have any views about this issue, being an old humorist yourself? Al: Well, I said and did things in the eighties’ games that I would never dream of doing now. Let’s put it that way. On the other hand, violence seems to be quite popular. I don’t understand it. We get scared of humor, but yet, you just can’t blow a person up any more than we do now. It is an odd situation isn’t it? Roar: Yep. There has also been so much nudity in the last 10 years in all kinds of movies, and very explicit nudity. Al: Yeah, which I enjoy. I have no problem with that. Roar: Yeah, yeah, I won’t complain. Al: Well, all in all, I guess my opinion is that things have gotten better. How long has it been since I wrote Larry 1? 35 years? No, 38 years. So in the 38 years since I wrote Larry 1, I think all of that stuff has gotten better, with the exception of violence. I’m not happy with that. I think we show too much, too often, and too strongly and too dramatically. Roar: It’s very brutal now. Al: But, you know, I think all these things are a pendulum, right? You know, it’s gonna swing over here, and then eventually it’s gonna swing back. Typical human behaviour. Roar: Yes, that’s true. Al: We’ll see what happens. Well, I won’t see what happens. You can see what happens. Gelfling Adventure. Photo: Hardcoregaming . Old Sierra games Roar: Yes. Um, I guess I also have to talk to you about the old Sierra games. And these might not be the most original questions. Al: Well, I only want original questions….. Roar: You had, among other things, been working on the Dark Crystal Game: Gelfling Adventure. Al: Yeah, I did. Roar: And growing up when I did, I loved watching that old Dark Crystal movie. It was pretty scary when I was eight or nine, but man, what a wonderful movie. Al: It was. Yeah, it sure was. And Roberta [Williams] was delighted when Jim Henson asked her to do a computer game based on Dark Crystal. And I was delighted, because after the artists had done all the backgrounds and things for Dark Crystal, Sierra asked me to spin off an educational version of it. We called it the Gelfling Adventure, and it wasn’t a big hit, but it was sure fun to work on. Roar: I would guess if you could get it today, in a big box, it would be worth a fortune. Al: I used to have a copy, but I think I sold it on eBay when I cleaned out my attic. I had an attic full of games, and finally I just put them on eBay and sold them off one at a time. Roar: You did? Was it that collection Metal Jesus Rocks bought? Al: Ah, no, that was not a game, that was stuff from Sierra. We should tell people what we’re referring to – there’s a YouTube channel ca",
      "cover_image_url": "https://spillhistorie.no/wp-content/uploads/2026/02/al-lowe-topp.png"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "The second-gen AirTags are a scatterbrain’s best friend",
      "url": "https://www.theverge.com/tech/875314/airtags-second-gen-review-item-tracker",
      "published": "2026-02-07T15:30:00+00:00",
      "summary": "In a somewhat controversial Vergecast episode, I declared that AirTags are a superior product to iPads. The iPad lovers roasted me across social media. I have heard and respect their opinions. But, I'm sorry - the second-gen AirTag has only deepened my conviction that the humble, $29 item tracker is one of Apple's most helpful [&#8230;]",
      "content_text": "In a somewhat controversial Vergecast episode , I declared that AirTags are a superior product to iPads. The iPad lovers roasted me across social media. I have heard and respect their opinions. But, I’m sorry — the second-gen AirTag has only deepened my conviction that the humble, $29 item tracker is one of Apple’s most helpful gizmos. The new updates focus more on making a good thing better. There are upgraded ultra-wideband and Bluetooth chips that help extend the range by about 1.5 times. The chime has a new, higher pitch and is 50 percent louder thanks to a redesigned speaker. And you can now use Precision Finding for AirTags on an Apple Watch, provided you have at least a Series 9 or Ultra 2. Before Apple announced the refresh last week, my spouse and I owned seven AirTags and were planning on buying a few more. Both of us struggle with ADHD. We have wily cats who love to bat things under couches and beds and behind shelving. Without AirTags, we’d probably spend an hour a day running up and down the 42 stairs of our four-story townhome trying to find our essentials. We have AirTags on our key fobs, TV remotes, wallets, car glove boxes, and inside our Canada Goose jackets. (They were expensive and it is arctically cold on the East Coast right now, okay?) But we also have plenty of complaints about our AirTags. The other day, my spouse lost their keys in a car lot and spent 90 minutes looking for them. The chime was way too low, and the AirTag was struggling to connect over Bluetooth. The pudgy disc shape is annoying for wallets unless you buy one specially made to fit it. (And those wallets tend to be ugly .) They scuff easily. If your cat bats it down a staircase, there’s a good chance it comes flying apart, and you have to race the unhinged scamp so he doesn’t eat the battery. The second-gen AirTags don’t fix most of that. They’re still the same shape, size, and design. (And, thankfully, price.) My review unit scuffed two seconds after I stuck it in the accompanying key ring. But the louder chime and extended range made a huge difference in testing. You’ve got a new ultra-wideband chip, upgraded Bluetooth chip, redesigned speaker, IP67, and NFC. I pitted the second-gen AirTag against an original one with a freshly replaced battery. I quickly noticed how the extended range improved connectivity. Connecting to my phone was always much faster than the original, regardless of distance. That speed meant the chime rang sooner and the signal for Precision Finding appeared quicker. The vertical range improved, too. AirTags are best if you’re looking for things in horizontal space. They really struggle if you’re on a different floor — a perpetual problem in our townhouse. The original AirTag was fine if I was one floor up or down from my desired object. But two or more floors? Forget it. The best I could hope for was the Precision Finding screen telling me the tag was “far” or, more likely, unreachable. Not so for the second-gen AirTag. I stuck it on the ground floor of my house and ran up all the way to the fourth floor. Not only could I reliably get a quick connection for Precision Finding, to my surprise, I could also easily hear the chime. (If everything on my street were dead silent, I could maybe hear the original AirTag from the same position, but often couldn’t.) This was true when I stuffed the AirTags under blankets and between couch cushions, too. My favorite new feature, however, was the ability to use Precision Finding from the Apple Watch. It’s not intuitive to set up. Instead of using the Find Items app, you have to add a button to the Apple Watch control panel. It works well. When I’m doing chores around the house, I often leave my phone behind. If something with the original AirTag on it is missing, I have to first find my phone and then use Precision Finding on it to find the lost item. I never lose my Apple Watch because it’s always on me. (Honestly, the most-used function on my Apple Watch is finding my phone.) As with the original AirTags, the new versions still have unwanted tracker alerts. Apple’s announcement post for the new model argues that the louder chime makes AirTags less attractive for nefarious purposes. Apple makes a point during setup to say that these devices are not meant for tracking people or pets — unlike Tile — but bad actors won’t listen . I haven’t had this unit long enough to see how the louder chime and extended range impact this feature in an organically triggered scenario. I’ll report back once I have. Adding AirTag Precision Finding to the Apple Watch was my absolute favorite upgrade. I now want to upgrade all our perfectly good original AirTags. At minimum, that’ll set us back $200. While you can recycle old AirTags for free, you don’t get a store credit, and I hate the idea of e-waste when it’s affordable to replace the coin cell battery. My father-in-law has declared he has no such qualms and plans to update all eight of his AirTags immediately. I suggest a more reasonable compromise if you’re an AirTag power user like me. Be patient, identify the four items you tend to lose most, get a four-pack when they’re on sale, and then repurpose the original ones for items you use (or lose) less frequently but still want to track. Ultimately, the iPad lovers have vociferously reminded me that AirTags are not for everyone. I’m inherently jealous of you geniuses who always seem to know where your things are. But if you’re like me, plagued with the executive dysfunction of a goldfish, the second-gen AirTags are a meaningful update — even if it’s not a flashy one. Follow topics and authors from this story to see more like this in your personalized homepage feed and to receive email updates. Victoria Song Accessory Reviews Gadgets Reviews Tech",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/268280_AirTags_2_AKrales_0003.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "[2602.05192] First Proof",
      "url": "https://arxiv.org/abs/2602.05192",
      "published": "2026-02-07T15:25:49+00:00",
      "summary": "<p>Article URL: <a href=\"https://arxiv.org/abs/2602.05192\">https://arxiv.org/abs/2602.05192</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46924591\">https://news.ycombinator.com/item?id=46924591</a></p> <p>Points: 133</p> <p># Comments: 76</p>",
      "content_text": "BibTeX formatted citation ×",
      "cover_image_url": "/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "StrongDM Software Factory",
      "url": "https://factory.strongdm.ai/",
      "published": "2026-02-07T15:05:56+00:00",
      "summary": "<p>See also <a href=\"https://simonwillison.net/2026/Feb/7/software-factory/\" rel=\"nofollow\">https://simonwillison.net/2026/Feb/7/software-factory/</a></p> <hr /> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46924426\">https://news.ycombinator.com/item?id=46924426</a></p> <p>Points: 195</p> <p># Comments: 347</p>",
      "content_text": "We built a Software Factory : non-interactive development where specs + scenarios drive agents that write code, run harnesses, and converge without human review. The narrative form is included below. If you'd prefer to work from first principles, I offer a few constraints & guidelines that, applied iteratively, will accelerate any team toward the same intuitions, convictions 1 , and ultimately a factory 2 of your own. In kōan or mantra form: Why am I doing this? (implied: the model should be doing this instead) In rule form: Code must not be written by humans Code must not be reviewed by humans Finally, in practical form: If you haven't spent at least $1,000 on tokens today per human engineer, your software factory has room for improvement The StrongDM AI Story On July 14th, 2025, Jay Taylor and Navan Chauhan joined me (Justin McCarthy, co-founder, CTO) in founding the StrongDM AI team. The catalyst was a transition observed in late 2024: with the second revision of Claude 3.5 (October 2024), long-horizon agentic coding workflows began to compound correctness rather than error. Compounding correctness vs compounding error By December of 2024, the model's long-horizon coding performance was unmistakable via Cursor's YOLO mode . Prior to this model improvement, iterative application of LLMs to coding tasks would accumulate errors of all imaginable varieties (misunderstandings, hallucinations, syntax, version DRY violations, library incompatibility, etc). The app or product would decay and ultimately \"collapse\": death by a thousand cuts, etc. Together with YOLO mode, the updated model from Anthropic provided the first glimmer of what we now refer to internally as non-interactive development or grown software. Find Knobs, Turn To Eleven \"These go to 11\" In the first hour of the first day of our AI team, we established a charter which set us on a path toward a series of findings (which we refer to as our \"unlocks\"). In retrospect, the most important line in the charter document was the following: Hands off! Initially it was just a hunch. An experiment. How far could we get, without writing any code by hand? Not very far! At least: not very far, until we added tests. However, the agent, obsessed with the immediate task, soon began to take shortcuts: return true is a great way to pass narrowly written tests, but probably won't generalize to the software you want. Tests were not enough. How about integration tests? Regression tests? End-to-end tests? Behavior tests? From Tests to Scenarios and Satisfaction One recurring theme of the agentic moment: we need new language. For example, the word \"test\" has proven insufficient and ambiguous. A test, stored in the codebase, can be lazily rewritten to match the code. The code could be rewritten to trivially pass the test. We repurposed the word scenario to represent an end-to-end \"user story\", often stored outside the codebase (similar to a \"holdout\" set in model training), which could be intuitively understood and flexibly validated by an LLM. Synthetic scenario curation and shaping interface Because much of the software we grow itself has an agentic component, we transitioned from boolean definitions of success (\"the test suite is green\") to a probabilistic and empirical one. We use the term satisfaction to quantify this validation: of all the observed trajectories through all the scenarios, what fraction of them likely satisfy the user? Validating Scenarios in the Digital Twin Universe In previous regimes, a team might rely on integration tests, regression tests, UI automation to answer \"is it working?\" We noticed two limitations of previously reliable techniques: Tests are too rigid - we were coding with agents, but we're also building with LLMs and agent loops as design primitives; evaluating success often required LLM-as-judge Tests can be reward hacked - we needed validation that was less vulnerable to the model cheating The Digital Twin Universe is our answer: behavioral clones of the third-party services our software depends on. We built twins of Okta, Jira, Slack, Google Docs, Google Drive, and Google Sheets, replicating their APIs, edge cases, and observable behaviors. With the DTU, we can validate at volumes and rates far exceeding production limits. We can test failure modes that would be dangerous or impossible against live services. We can run thousands of scenarios per hour without hitting rate limits, triggering abuse detection, or accumulating API costs. Digital Twin Universe: behavioral clones of Okta, Jira, Google Docs, Slack, Drive, and Sheets (click to enlarge) Unconventional Economics Our success with DTU illustrates one of the many ways in which the Agentic Moment has profoundly changed the economics of software. Creating a high fidelity clone of a significant SaaS application was always possible, but never economically feasible. Generations of engineers may have wanted a full in-memory replica of their CRM to test against, but self-censored the proposal to build it. They didn't even bring it to their manager, because they knew the answer would be no. Those of us building software factories must practice a deliberate naivete : finding and removing the habits, conventions, and constraints of Software 1.0 . The DTU is our proof that what was unthinkable six months ago is now routine. Read Next Principles : what we believe is true about building software with agents Techniques : repeated patterns for applying those principles Products : tools we use daily and believe others will benefit from Thank you for reading. We wish you the best of luck constructing your own Software Factory.",
      "cover_image_url": "https://factory.strongdm.ai/images/og-image.png"
    }
  ]
}