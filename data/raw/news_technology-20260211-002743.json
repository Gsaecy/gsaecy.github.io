{
  "industry": "technology",
  "collected_at": "2026-02-10T16:28:45.105678+00:00",
  "hours": 24,
  "limit": 25,
  "count": 25,
  "items": [
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "Singapore says China-backed hackers targeted its four largest phone companies",
      "url": "https://techcrunch.com/2026/02/10/singapore-china-backed-hackers-targeted-largest-phone-companies-salt-typhoon/",
      "published": "2026-02-10T16:19:39+00:00",
      "summary": "The Singaporean government said the China-backed hackers gained \"limited access to critical systems\" run by the country's top four telecommunication giants, but said they did not disrupt services or steal customers' data.",
      "content_text": "Singapore’s government has blamed a known Chinese cyber-espionage group for targeting four of its top telecommunication companies as part of a months-long attack. In a statement Monday , Singapore confirmed for the first time that the hackers, known as UNC3886, targeted the country’s telecoms infrastructure, including its largest companies: Singtel, StarHub, M1, and Simba Telecom. The government previously said that it was responding to an unspecified attack on its critical infrastructure. While the intruders were able to breach and access some systems, they did not disrupt services or access personal information, said K. Shanmugam, the country’s coordinating minister for national security. Google-owned cybersecurity unit Mandiant previously linked UNC3886 as an espionage group likely working on behalf of China. The Chinese government is known to conduct regular cyber-espionage operations, as well as prepositioning for disruptive attacks ahead of an anticipated invasion of Taiwan, which Beijing has routinely denied, per Reuters . UNC3886 is known for exploiting zero-day vulnerabilities in routers, firewalls, and virtualized environments, where cybersecurity tools that are designed to spot malware cannot typically reach. The hacking group has targeted the defense, technology, and telecom industries across the U.S. and the Asia-Pacific region. In the case of the attack on Singapore’s top telcos, Shanmugam said the hackers used advanced tools, like rootkits, to gain long-term persistence to their systems. “In one instance, they were able to gain limited access to critical systems but did not get far enough to have been able to disrupt services,” according to the government’s statement. Per Reuters, the telcos said in a joint statement that the companies regularly face distributed denial-of-service and other malware attacks. “We adopt defence-in-depth mechanisms to protect our networks and conduct prompt remediation when any issues are detected,” the statement read. The attacks on Singapore’s telcos follow similar but distinctly different attacks on hundreds of telecoms companies around the world in recent years, including in the United States . Multiple governments have linked these attacks to a China-backed group dubbed Salt Typhoon. Singapore said the attack carried out by UNC3886 has “not resulted in the same extent of damage as cyberattacks elsewhere,” referring to the Salt Typhoon hacks.",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2026/02/singapore-2152482714.jpg?resize=1200,800"
    },
    {
      "industry": "technology",
      "source": "Wired",
      "title": "ICE Is Expanding Across the US at Breakneck Speed. Here’s Where It’s Going Next",
      "url": "https://www.wired.com/story/ice-expansion-across-us-at-heres-where-its-going-next/",
      "published": "2026-02-10T16:05:43+00:00",
      "summary": "ICE plans to lease offices throughout the US as part of a secret, months-long expansion campaign. WIRED is publishing dozens of these locations.",
      "content_text": "By early October, the ICE surge team was working through the government shutdown, even as other critical government work was put on hold. Days after the shutdown began, GSA was still awarding leases. On October 6, 2025, a signed internal memorandum stated that GSA should “approve of all new lease housing determinations associated with ICE hiring surge,” in light of ICE’s “urgent” space requirements and the purported impact of delays on the agency’s ability to “meet critical immigration enforcement deadlines.” On October 9, the same day that Trump announced in a cabinet meeting that the government would be making “permanent” cuts from “Democrat programs” during the shutdown, GSA received a list from OPLA with requests for office locations, including expansions and new leases, in 41 cities around the country. In a memorandum dated October 29, 2025, a representative from Homeland Security Investigations—one of the two major departments within ICE, along with ERO, and tasked with a wide range of investigative work in cases ranging from human trafficking to art theft—asked GSA’s office of general counsel to engage in nationwide lease acquisition on behalf of DHS “using the unusual and compelling urgency justification,” in accordance with Trump’s executive immigration order. “If HSI cannot effectively obtain office space in a timely manner, HSI will be adversely impacted in accomplishing its mission—a mission that is inextricably tied to the Administration’s priority in protecting the American People Against Invasion,” the memorandum states. By early November, according to documents viewed by WIRED, 19 projects had already been awarded in cities around the US, including Nashville, Tennessee; Dallas, Texas; Sacramento, California; and Tampa, Florida. Multiple projects were days away from being awarded in Miami, Florida; Pittsburgh, Pennsylvania; and New Orleans, Louisiana, among others, and emergency requests for short-term space had been made in eight cities, including Atlanta, Georgia; Baltimore, Maryland; Boston, Massachusetts; and Newark, New Jersey. In documents viewed by WIRED, ICE has repeatedly outlined its expansion to cities around the US. The September memorandum citing “unusual and compelling urgency” for office expansion states that OPLA will be “expanding its legal operations” into Birmingham, Alabama; Fort Lauderdale, Fort Myers, Jacksonville, and Tampa, Florida; Des Moines, Iowa; Boise, Idaho; Louisville, Kentucky; Baton Rouge, Louisiana; Grand Rapids, Michigan; St. Louis, Missouri; Raleigh, North Carolina; Long Island, New York; Columbus, Ohio; Oklahoma City, Oklahoma; Pittsburgh, Pennsylvania; Charleston and Columbia, South Carolina; Nashville, Tennessee; Richmond, Virginia; Spokane, Washington and Coeur d’Alene, Idaho; and Milwaukee, Wisconsin. The memorandum also states that the existing offices are at maximum capacity and will “require additional space” to accommodate the new employees hired. At the time, the memo states that OPLA had selected almost 1,000 attorneys to hire. Months after the “surge” began, ICE’s expansion to American cities is well underway, according to documentation viewed by WIRED. The table below gives a detailed listing of planned ICE lease locations as of January, and includes current ICE offices that are set to expand and new spaces the agency is poised to occupy. It does not include more than 100 planned ICE locations across many states—including California, New York, and New Jersey—where WIRED has not viewed every specific address.",
      "cover_image_url": "https://media.wired.com/photos/6986655d619933b7c3265278/191:100/w_1280,c_limit/politics_ice_offices_sprawl.jpg"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "Microsoft wants to rewire data centers to save space",
      "url": "https://www.theverge.com/science/876083/microsoft-ai-data-center-superconductor",
      "published": "2026-02-10T16:01:40+00:00",
      "summary": "Microsoft wants to design more efficient data centers using materials that allow electricity to flow with zero resistance. If these new materials, called high-temperature superconductors, can make it to market, Microsoft thinks it could be a game changer for how data centers and the energy infrastructure they connect to are built. Tech companies are facing [&#8230;]",
      "content_text": "Microsoft wants to design more efficient data centers using materials that allow electricity to flow with zero resistance. If these new materials, called high-temperature superconductors, can make it to market, Microsoft thinks it could be a game changer for how data centers and the energy infrastructure they connect to are built. Tech companies are facing backlash over how much power generative AI demands, delays connecting to power grids that lack the infrastructure to meet those demands, and the impact construction of new data centers has on local residents. High-temperature superconductors (HTS) could potentially shrink the amount of space needed for a data center and the transmission lines feeding it power. “Microsoft is exploring how this technology could make electrical grids stronger and reduce the impact data centers have on nearby communities,” Microsoft GM of Global Infrastructure Marketing Alistair Speirs wrote in a blog published today. “The future data center will be superconducting” Today’s centers — and most of our energy infrastructure — rely on old-school copper wires, which conduct electricity pretty efficiently. HTS cables, however, can move an electrical current with zero resistance, slashing the amount of energy lost in the process. It also allows for cables that are lighter and more compact. You’ll already find HTS in MRI machines, and more recently used in short stretches for power lines in dense metropolitan areas including Paris and Chicago . So far, though, their use has been limited in part because HTS cables have been more complicated and expensive to use in energy systems than copper cables. To reach zero resistance, the HTS would need to be cooled to very low temperatures — likely using liquid nitrogen. And the HTS “tape” that forms the basis of superconducting cables is typically made with rare-earth barium copper oxide material. While a superconducting cable only requires a small amount of rare-earth material, the supply chain for the rare earth element is still largely concentrated in China . A bigger challenge, experts tell The Verge , will be increasing the manufacturing capacity for this tape enough for it to be affordable. That’s starting to change as a result of the power demands of generative AI . In recent years, tech companies have fueled research into the development of a nuclear fusion power plant , long considered the holy grail of clean energy. Much of the HTS tape manufactured today goes toward fusion research, and growth in that department has managed to lower costs for the material. “That actually helped the supply chain and manufacturer variety, and even some of the costing of HTS … for us to, like, oh, ‘Well, let’s think about that. Now things have changed a little bit,’” says Husam Alissa, director of systems technology at Microsoft. Microsoft is primarily interested in using HTS in two ways, Alissa tells The Verge . Inside a data center, smaller cables would allow for more flexibility in how the electrical rooms and racks of hardware are laid out. With funding from Microsoft, Massachusetts-based superconducting company VEIR demonstrated last year that HTS cables at a data center could deliver the same amount of power with about a 10x reduction in cable dimension and weight compared to conventional alternatives. “The future data center will be superconducting … High power, more efficient, more compact,” says Ziad Melhem, professor in practice in the physics department at Lancaster University who sits on the editorial board for the Superconductivity Global Alliance. (Melhem disclosed that he previously worked at Oxford Instruments, which supplied Microsoft with components for its quantum computer system.) Outside of the data center, Microsoft is open to working with energy companies to support the buildout of long-distance power lines using HTS. Expanding transmission lines has been one of the biggest bottlenecks when it comes to updating the power grid, connecting data centers, and building out more power supply. The process of getting approvals to build such large infrastructure across multiple jurisdictions can be long and arduous. With power lines made from HTS, the amount of space needed for those power lines could shrink significantly. While overhead transmission lines might spread out over an area around 70 meters in width, superconducting cables might only need 2 meters of clearance, according to the Microsoft blog. The smaller area ostensibly shaves down the time and cost needed for construction. “This is an obvious evolution of the use of this technology,” says Dennis Whyte, a professor of nuclear science and engineering at MIT. Whyte has not worked directly with Microsoft, but co-leads an effort to build a fusion machine called SPARC that’s a collaboration between MIT and a company called Commonwealth Fusion Systems that’s received funding from Bill Gates’ Breakthrough Energy Ventures . The additional interest in HTS for data centers could also allow fusion companies get more of the material for less money, helping propel advances in nuclear fusion technology as well. Microsoft has separately inked a deal with another company developing a fusion power plant in Washington state. “It’s come full circle,” Whyte says. Follow topics and authors from this story to see more like this in your personalized homepage feed and to receive email updates. Justine Calma Analysis Energy Microsoft Report Science Tech",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/gettyimages-2243425240.jpg?quality=90&strip=all&crop=0%2C10.737892056687%2C100%2C78.524215886627&w=1200"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "Vega raises $120M Series B to rethink how enterprises detect cyber threats",
      "url": "https://techcrunch.com/2026/02/10/vega-raises-120m-series-b-to-rethink-how-enterprises-detect-cyber-threats/",
      "published": "2026-02-10T16:00:00+00:00",
      "summary": "Vega Security raised $120 million Series B, bringing its valuation to $700 million, in a round led by Accel. The company aims to rethink how enterprises detect cybersecurity threats.",
      "content_text": "Modern enterprises generate enormous amounts of security data, but legacy tools like Splunk still require companies to store all of it in one place before they can detect threats – a slow and costly process that’s increasingly breaking down in cloud environments where volumes are exploding and data lives everywhere. AI cybersecurity startup Vega Security wants to flip that approach by running security where the data already lives, implementing in cloud services, data lakes, and existing storage systems. And the two-year-old firm just raised a $120 million Series B round to scale that vision, TechCrunch has exclusively learned. Led by Accel with participation from Cyberstarts, Redpoint, and CRV, the new round nearly doubles Vega’s valuation to $700M and brings its total funding to $185 million, money the startup will use to further develop its AI-native security operations suite, beef out its go-to-market team, and expand globally. Shay Sandler, co-founder and CEO of Vega, told TechCrunch that the current operating model of the SIEM (security information and event management) — the dominant technology in this domain for the last two decades — is not only “crazy expensive,” but is also increasingly causing AI-native security operations to fail. In complex cloud environments, he says, the current model often increases exposure to threat actors. “Vega has defined a new operating model that enables organizations to leverage the full potential of their enterprise data to achieve incident response readiness, without all the complexity, the cost, the drama,” Shay Sandler, co-founder and CEO of Vega, told TechCrunch. “We want to simply enable them to reach AI-native detection response capability anywhere the data is, at scale.” Like so many cybersecurity founders, Sandler did his time in the Israeli military’s cybersecurity unit before being one of the founding employees behind Granulate, which Intel acquired for $650 million in 2022. After a year at Intel, Sandler decided to “do it big time in the cybersecurity world.” That pedigree is partly what attracted the attention of Andrei Brasoveanu, a partner at Accel. But it was also Vega’s ambitious approach to security management in a market that is already dominated by one player: Splunk. Techcrunch event Boston, MA | June 23, 2026 Brasoveanu told TechCrunch that legacy SIEM companies like Splunk, which Cisco acquired in 2024 for $28 billion, have been criticized in recent years because their solutions are difficult to scale. They fail at processing the insane rise of data volumes driven by AI. “Splunk and every contender since has always centralized the data, but by doing that you essentially hold the customer hostage,” Brasoveanu said. However, sometimes it’s easier to hate the status quo than do the work of making a switch to a better alternative, a quandary any startup attempting to breach enterprise budgets understands. That’s why Sandler says Vega’s “North Star” was to not only build a solution that is more cost effective and better at threat detection, but “to make it no drama, as simple as possible for the biggest, most complex enterprises in the world to adopt it within minutes.” Vega’s approach seems to be working. The 100-person startup has already signed multi-million-dollar contracts with banks, healthcare companies, and Fortune 500 firms, including cloud-heavy companies like Instacart. “The only reason they would do that with a two-year-old startup is because the problem is so painful and other solutions on the market require an unrealistic expectation that the enterprise change the way they operate or do two years of data migrations,” Sandler said. “Vega enables them to just plug and play and achieve immediate detection response value.”",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2026/02/Shay-Sandler-Eli-Rosen.-Credit-Ohad-Kab.jpeg?resize=1200,802"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "What to know about Netflix’s landmark acquisition of Warner Bros.",
      "url": "https://techcrunch.com/2026/02/10/what-to-know-about-netflixs-landmark-acquisition-of-warner-bros/",
      "published": "2026-02-10T15:56:21+00:00",
      "summary": "Learn more about Netflix's acquisition of Warner Bros., considered the most historic megadeal in Hollywood, as it continues to develop.",
      "content_text": "​If you thought 2025 couldn’t get any crazier, the streaming world had one more surprise up its sleeve before the year ended. Netflix, already the largest streaming platform with over 325 million subscribers, took a bold step by acquiring Warner Bros. ’ film and television studios, as well as HBO, HBO Max, and other assets. The deal, announced in early December, will bring together some of the most legendary franchises, such as Game of Thrones, Harry Potter, and DC Comics properties, among others, all under one roof. ​The scale of this megadeal has stunned industry observers. Not only is it historic in its size, but it is also predicted to disrupt Hollywood as we know it. We’re here to break down exactly what’s happening with the Netflix–WBD deal, including the latest developments, what’s at stake, and what could come next. What has happened so far? ​This all started back in October when Warner Bros. Discovery (WBD) revealed it was exploring a potential sale after receiving unsolicited interest from several major players in the industry. For years, WBD has struggled under the weight of billions of dollars in debt, compounded by declining cable viewership and fierce competition from streaming platforms. These financial pressures forced the company to consider major strategic changes, including selling its entertainment assets to one of its rivals. ​The bidding process quickly became competitive. Several major players saw the potential in acquiring the media giant. Paramount and Comcast emerged as serious contenders, with Paramount initially viewed as the frontrunner. Techcrunch event Boston, MA | June 23, 2026 But ultimately, WBD’s board determined that Netflix’s offer was the most attractive, despite Paramount offering approximately $108 billion in cash. Paramount’s bid aimed to acquire the entire company, while Netflix’s offer focused specifically on the film, television, and streaming assets. Additionally, Netflix recently amended its agreement to an all-cash offer at $27.75 per WBD share, further reassuring investors and paving the way for the deal to proceed. The deal is valued at approximately $82.7 billion. A fierce bidding war Even after Netflix emerged as the preferred buyer, tensions with Paramount remained high, as the rival company continued to pursue Warner Bros.’ assets. ​Paramount persisted in its attempts to acquire WBD for several months. Still, the board repeatedly rejected its offers, citing concerns about Paramount’s heavy debt load and the increased risk associated with its proposal. The board noted that Paramount’s offer would have left the combined company burdened with $87 billion in debt, a risk they were unwilling to take. In January, Paramount filed a lawsuit seeking more information about the Netflix deal. A month later, the company sought to sweeten its deal by announcing it would offer a $0.25 per share “ticking fee” to WBD shareholders for each quarter the deal fails to close by December 31, 2026. It also said it would pay the $2.8 billion breakup fee if Netflix backs out. The company continues to assert that its offer is far superior. Regulatory hurdles Image Credits: Bryce Durbin/TechCrunch Given the unprecedented scale and market impact of the deal, regulatory scrutiny is intense and remains a significant obstacle to closing the transaction. Earlier this week, it was reported that Netflix co-CEO Ted Sarandos is scheduled to testify before a U.S. Senate committee about the deal, a move that highlights just how seriously lawmakers are taking these concerns. In November, prominent lawmakers — Senators Elizabeth Warren, Bernie Sanders, and Richard Blumenthal — voiced their concerns to the Justice Department’s Antitrust Division , warning that such a massive merger could have serious consequences for consumers and the industry at large. The senators argue that the merger could give the new media giant excessive market power, enabling it to raise prices for consumers and stifle competition. Should regulators block the acquisition, Netflix would be obligated to pay a $5.8 billion breakup fee . It remains unclear whether Warner Bros. would remain an independent company or revisit previous acquisition proposals. Concerns within the industry ​Reactions from the entertainment industry have been largely negative. The Writers Guild of America has been among the most vocal critics, demanding that the merger be blocked on antitrust grounds. Additionally, insiders worry that the acquisition will squeeze independent creators and diverse voices out of the spotlight, ultimately narrowing the range of stories that get told. There are also widespread concerns about potential job losses and lower wages. For creators and theaters, uncertainty remains around release windows. Netflix co-CEO Ted Sarandos has stated that all films planned for theatrical release through Warner Bros. will continue as scheduled. However, he also hinted that, over time, release windows may be shortened, with movies coming to streaming platforms sooner than before. What should subscribers know? Image Credits: Thibault Penin / Unsplash ​What does all this mean if you’re a Netflix or HBO Max subscriber? Netflix executives have reassured viewers that HBO’s operations will remain largely unchanged in the near term. At this stage, the company says it’s too early to make any definitive announcements about potential bundles or app integration. Regarding pricing, Sarandos has stated that no immediate changes will occur during the regulatory approval period. However, subscribers should be aware that Netflix has historically raised subscription prices regularly, so price increases are possible once the acquisition is finalized. Netflix tends to hike its rates every year or two. When is the deal expected to close? The Netflix–WBD deal is not yet final. A WBD stockholder vote is expected around April, with the deal anticipated to close 12 to 18 months after that vote. However, regulatory approvals are still pending, and scrutiny could shape the final outcome. Stay tuned…",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2022/12/AdobeStock_500522615_Editorial_Use_Only-scaled-1.webp?resize=1200,768"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "Google expands tools to let users remove sensitive data about themselves from Search",
      "url": "https://techcrunch.com/2026/02/10/google-remove-sensitive-personal-data-from-search/",
      "published": "2026-02-10T15:54:48+00:00",
      "summary": "Users will be able to more easily request the removal of results that include private information or non-consensual explicit imagery.",
      "content_text": "As part of Safer Internet Day, Google unveiled updates to its “Results about you” and non-consensual explicit image removal tools. The “Results about you” tool already allows users to remove Search results containing their phone number, email address, or home address, and it can now also be used to request the removal of results that include information such as a driver’s license, passport, or Social Security number, the company said in the update Tuesday. You can access the tool in the Google app by tapping your Google account photo and selecting “Results about you.” If it’s your first time using the tool, you need to click “Get started” and then add the personal contact information you want to monitor. You’ll then be prompted to add your government ID numbers. Existing users can add their ID numbers and select which ones they want to monitor. Once you’ve confirmed your details, Google will automatically monitor Search results and notify you if it finds results that contain your information. Google notes that removing this information from Search doesn’t remove it from the web entirely, but that doing so can help keep your information private. Image Credits: Google The update is rolling out in the U.S. over the coming days. Google plans to bring it to additional regions in the future. The tech giant also announced that it’s making it easier to request the removal of non-consensual explicit images on Search. Now, users just have to click on the three dots on an image, select “remove result,” then tap “It shows a sexual image of me.” Additionally, you no longer need to report images one by one, as you can now select and submit requests for multiple images from Search results using a single form. Users can also track the status of all requests in one place within the “Results about you” hub. “We understand that removing existing content is only part of the solution,” Google wrote in its blog post . “For added protection, the new process allows you to opt-in to safeguards that will proactively filter out any additional explicit results that might appear in similar searches.” Techcrunch event Boston, MA | June 23, 2026",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2025/08/GettyImages-2198713751.jpg?w=1024"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "Trump EPA reportedly seeks to revoke landmark air pollution rule",
      "url": "https://techcrunch.com/2026/02/10/trump-epa-reportedly-seeks-to-revoke-landmark-air-pollution-rule/",
      "published": "2026-02-10T15:47:13+00:00",
      "summary": "EPA administrator Lee Zeldin is reportedly expected to repeal the 2009 \"endangerment finding\" that underpins U.S. climate regulatory efforts.",
      "content_text": "The Trump administration’s EPA administrator, Lee Zeldin, is looking to repeal the 2009 “endangerment finding” that found greenhouse gases pose a threat to human health and welfare, possibly as early as this week, the Wall Street Journal reports . The EPA finding had set the legal basis for federal regulation of six greenhouse gases, including carbon dioxide and methane, and it has been unsuccessfully challenged since it was first instituted. The move is almost certain to attract a number of lawsuits, and it could be years before the matter is settled. The EPA’s move will only affect tailpipe emissions for cars and trucks, though it’s expected that the Trump administration will use it to unwind regulations in other sectors like power plants and industrial facilities. Legacy automakers, which pushed Trump to weaken fuel efficiency rules , notably did not push for the EPA to repeal the endangerment finding. Tesla went further, asking the EPA to maintain the finding , saying it was “based on a robust factual and scientific record.” If the Trump administration is successful, the U.S. will be increasingly out of step with regulations in other advanced economies. Companies that do business across borders will need to develop different approaches for each market, increasing costs. Automakers, in particular, are facing a future in which they’ll be forced to serve bifurcated markets, at least in the near term. Regulatory whiplash in the U.S. coupled with increasing competition from China has cost automakers tens of billions of dollars . American automakers’ reliance on fossil fuel-powered trucks, in particular, has painted the domestic industry into a corner, providing addictive profits that distract from future-proofing their fleets in advance of seemingly inevitable competition from Chinese marques. Techcrunch event Boston, MA | June 23, 2026 The Trump administration told the WSJ it expects the policy change to save more than $1 trillion, though it didn’t provide any evidence to back that figure up. Climate change is expected to cost far more. The Congressional Budget Office found that nearly $1 trillion worth of real estate is threatened by rising sea levels, and mortality rates in the U.S. could be 2% higher if global warming isn’t abated . Another study, published in 2024, found that climate change could trim global GDP by 17% by 2050, the equivalent of $38 trillion per year.",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2025/03/GettyImages-2201520437.jpeg?resize=1200,800"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "Jeffrey Epstein’s digital cleanup crew",
      "url": "https://www.theverge.com/report/876081/jeffrey-epstein-files-seo-google-digital-footprint-emails",
      "published": "2026-02-10T15:45:00+00:00",
      "summary": "In between hobnobbing with royalty and world leaders and abusing children and young women, Jeffrey Epstein appears to have been googling himself regularly. Across several batches of documents related to the convicted sex offender made public, we see Epstein shoot off emails to associates, complaining that his digital footprint includes factual information about his crimes. [&#8230;]",
      "content_text": "In between hobnobbing with royalty and world leaders and abusing children and young women, Jeffrey Epstein appears to have been googling himself regularly. Across several batches of documents related to the convicted sex offender made public, we see Epstein shoot off emails to associates, complaining that his digital footprint includes factual information about his crimes. i want the google page cleaned ( November 5th, 2010 ) mike „ can you olcan up my wiki page ( April 18th, 2011 ) Any way to clean up my wiki page ( September 17th, 2013 ) Epstein regularly directed his gripes at Al Seckel, a fixer type who appears over and over in Epstein files and promises to bury news articles and other content that mentions his abuse. But Seckel didn’t do it alone. Over thousands of documents, it’s clear that many people — SEO consultants, contacts in the sciences, and even unrelated acquaintances — helped to obscure Epstein’s past whenever someone searched for him online. Even after Epstein had pleaded guilty to procuring a child for prostitution, making him a registered sex offender, his network was happy to do him favors and reputation management firms took him on as a client. Reputation management services aren’t necessarily a tool to cover up crimes — it’s a standard public relations practice — but in Epstein’s case, the agencies would have known about his abuse, given that that’s what they were being hired to try to minimize. In October 2010, Seckel laid out an overview of the group’s plan of attack to defend Epstein’s reputation online. The situation, as Seckel described it, was that a search surfaced “over 75+ pages of derogatory material,” and that someone would be “very hard pressed to find any ‘positive’ references.” To “balance the only one-sided negative opinion that has been spread over a wide birth on the Internet,” Seckel said, the team would need to flood the zone with content they can control, specifically pointing to spinning up websites with original content related to Epstein’s connections to science and charities. Seckel would be the unpaid “team leader” along with other unpaid consultants. A person named Michael Keesling would be paid $25,000 to purchase and host web domains, contract a “Phillipine Crew” that would spread flattering links around the web, and other tasks. An unidentified group of “hackers” would be paid $2,500. A “Stephanie Horenstein (Fred Horenstein)” would be paid $2,500 to leave positive comments on Epstein-related news articles. The Verge ’s messages sent to a Michael Keesling working in SEO out of West Hollywood went unanswered. During this time frame, Epstein and his team discussed editing his Wikipedia page to remove mentions of him being a sex offender and pedophile, replacing his mugshot with other pictures, and pushing down news articles they didn’t like. Among the recently released files are several wire transfers to Keesling, totaling $22,500. Elsewhere, Keesling also mentioned receiving “over [$20,000] in cash” from Seckel without a receipt. By 2013, Epstein was in the market for another “good reverse [SEO] person,” as he wrote in an email, and was recommended someone named Tyler Shears. Within a few weeks, Shears was laying out a 30-day plan — quoting $125 per hour — and starting with tactics like beefing up content around a different Jeffrey Epstein “in order to help remove some of the negative results for our Epstein.” By February 2014, Epstein’s accountant noted that Shears had billed over $50,000 and that Epstein was “unsure of what has changed” since he hired Shears. A person with the same name and company did not respond to The Verge ’s request for comment. Epstein also shopped around for reputation management firms and according to emails was turned down multiple times. In 2010, Seckel forwarded to Epstein an exchange he had with a company called Infuse Creative. “We have no problem helping someone who is innocent of accusations or a true victim of circumstance, but if there is truth to these allegations and the conviction, I’m afraid we’d have to pass,” Gregory Markel, founder of the firm, wrote to Seckel. “Do you personally know how much of these allegations are true?” Elsewhere, an associate whose name is redacted in files told Epstein that Reputation.com couldn’t represent him “because of [his] background” but that another firm, Integrity Defenders, would (files also include a paid invoice for $2,449 from Integrity Defenders ). “Please ask him or anyone else not to click on any of the negative links EVER again as that can keep them lingering on the first page,” an account manager at the firm wrote in an email. The website for Integrity Defenders is inactive and The Verge was unable to contact the company. When it came to attempting to conceal Epstein’s crimes online, many people — paid and unpaid, knowingly or unthinkingly — assisted in the whitewashing. In 2010, after Seckel and his team spun up websites focused on Epstein’s ties to science and his philanthropy in an effort to drown out media coverage, the fixer started emailing acquaintances, asking for a favor. The request was simple: Would they link to Epstein’s sites on their own websites? Seckel asked scientists associated with UCLA , multiple physicists , and others in the scientific community. The idea, at least from an SEO perspective , is that getting valuable links from trustworthy sources like academic institutions would signal to Google that Epstein’s new sites should be surfaced to anyone searching for him. Part of how Google decides which pages to rank highly in search results is by looking at whether other sites link to a page; Epstein’s camp appears to have been trying to push down negative search results by securing valuable links from outside entities. One of the acquaintances who agreed to add links back to Epstein’s sites is Mark Tramo, an adjunct professor in the department of neurology at UCLA who told SFGate that he had “not heard anything [about] statutory rape or minors being involved, I never saw him with young girls, never visited the island, never flew in his planes.” But in 2010, Tramo was eager to acquiesce to Seckel’s request for links. “What splendid ideas!” he responded , before going on to describe the “support” (including anonymously) Epstein had provided for his work. “Links all over the world and at major institutions going up,” Seckel wrote in another thread to Epstein . Even after Epstein pleaded guilty to procuring a girl under 18 for prostitution, the industry he snaked his way into was happy to keep up the ruse. Follow topics and authors from this story to see more like this in your personalized homepage feed and to receive email updates. Mia Sato Policy Politics Report Science",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/268334_Jeffrey_Epsteins_digital_clean_up_crew_CVirginia.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Hello Entire World · Entire",
      "url": "https://entire.io/blog/hello-entire-world/",
      "published": "2026-02-10T15:44:47+00:00",
      "summary": "<p>Article URL: <a href=\"https://entire.io/blog/hello-entire-world/\">https://entire.io/blog/hello-entire-world/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46961345\">https://news.ycombinator.com/item?id=46961345</a></p> <p>Points: 22</p> <p># Comments: 11</p>",
      "content_text": "",
      "cover_image_url": "https://entire.io/entire-funding-announcement-even-logos.png"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Bazzite Postmortem",
      "url": "https://ba.antheas.dev/bazzite-postmortem.html",
      "published": "2026-02-10T15:40:42+00:00",
      "summary": "<p>Article URL: <a href=\"https://ba.antheas.dev/bazzite-postmortem.html\">https://ba.antheas.dev/bazzite-postmortem.html</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46961261\">https://news.ycombinator.com/item?id=46961261</a></p> <p>Points: 43</p> <p># Comments: 9</p>",
      "content_text": "",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "This vintage-looking camera has been updated for shooting the stars",
      "url": "https://www.theverge.com/tech/876264/om-system-om-3-astro-digital-camera-stars-astrophotography",
      "published": "2026-02-10T15:22:53+00:00",
      "summary": "Just over a year after OM System launched its vintage-styled OM-3 Micro Four Thirds mirrorless camera, the company has announced a new version with a handful of upgrades catering to astrophotography. The new OM-3 Astro will be available starting in March 2026 for $2,499.99, which is $500 more expensive than the standard model. Both can [&#8230;]",
      "content_text": "Just over a year after OM System launched its vintage-styled OM-3 Micro Four Thirds mirrorless camera , the company has announced a new version with a handful of upgrades catering to astrophotography. The new OM-3 Astro will be available starting in March 2026 for $2,499.99, which is $500 more expensive than the standard model. Both can be used to photograph the night sky, but the Astro introduces hardware improvements to enhance the colors of red nebulae – a popular subject for astrophotographers. The most notable upgrade to the OM-3 Astro is a new infrared cut filter positioned in front of the camera’s 20.37-megapixel stacked back-illuminated sensor featuring “optical characteristics optimally tuned to achieve approximately 100% transmission of Hα wavelengths.” Nearly every digital camera uses an IR filter to reduce infrared light and improve color accuracy, but the one in the OM-3 Astro specifically lets deep-red Hydrogen-alpha light get through, which is what gives red nebulae their distinct colors and shapes. One of the OM-3’s best features when it launched last year was a dedicated control dial for quickly accessing simulated film look profiles, and the ability to create your own easily accessible custom profiles. The OM-3 Astro comes with three of those profiles pre-programmed for astrophotography. Color1 is optimized for enhancing images of red nebulae, Color2 is optimized for night sky photography that combines stars and landscapes, and Color3 is designed for handheld star photography, according to PetaPixel . The Astro variant of the OM-3 also carries forward other features found on the standard model that are useful for capturing stars and celestial objects. Its Starry Sky AF locks onto stars using autofocus. It’s not as fast as standard autofocus, but will save you having to zoom in on the sky and make focus adjustments manually. There’s also a Night Vision mode for the OM-3’s LCD preview screen. It will enhance everything on the display with exaggerated contrast so you can properly frame a shot or make focus adjustments in the middle of the night. The mode will also reduce the intensity of the display to help preserve your night vision so you can operate the camera without needing to reach for a flashlight.",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/om3_astro1.jpg?quality=90&strip=all&crop=0%2C10.709735616043%2C100%2C78.580528767913&w=1200"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Disruption with some GitHub services",
      "url": "https://www.githubstatus.com/incidents/wkgqj4546z1c",
      "published": "2026-02-10T15:12:11+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.githubstatus.com/incidents/wkgqj4546z1c\">https://www.githubstatus.com/incidents/wkgqj4546z1c</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46960743\">https://news.ycombinator.com/item?id=46960743</a></p> <p>Points: 57</p> <p># Comments: 17</p>",
      "content_text": "Subscribe to updates for Disruption with some GitHub services via email and/or text message. You'll receive email notifications when incidents are updated, and text message notifications whenever GitHub creates or resolves an incident. VIA SMS: Afghanistan (+93) Albania (+355) Algeria (+213) American Samoa (+1) Andorra (+376) Angola (+244) Anguilla (+1) Antigua and Barbuda (+1) Argentina (+54) Armenia (+374) Aruba (+297) Australia/Cocos/Christmas Island (+61) Austria (+43) Azerbaijan (+994) Bahamas (+1) Bahrain (+973) Bangladesh (+880) Barbados (+1) Belarus (+375) Belgium (+32) Belize (+501) Benin (+229) Bermuda (+1) Bolivia (+591) Bosnia and Herzegovina (+387) Botswana (+267) Brazil (+55) Brunei (+673) Bulgaria (+359) Burkina Faso (+226) Burundi (+257) Cambodia (+855) Cameroon (+237) Canada (+1) Cape Verde (+238) Cayman Islands (+1) Central Africa (+236) Chad (+235) Chile (+56) China (+86) Colombia (+57) Comoros (+269) Congo (+242) Congo, Dem Rep (+243) Costa Rica (+506) Croatia (+385) Cyprus (+357) Czech Republic (+420) Denmark (+45) Djibouti (+253) Dominica (+1) Dominican Republic (+1) Egypt (+20) El Salvador (+503) Equatorial Guinea (+240) Estonia (+372) Ethiopia (+251) Faroe Islands (+298) Fiji (+679) Finland/Aland Islands (+358) France (+33) French Guiana (+594) French Polynesia (+689) Gabon (+241) Gambia (+220) Georgia (+995) Germany (+49) Ghana (+233) Gibraltar (+350) Greece (+30) Greenland (+299) Grenada (+1) Guadeloupe (+590) Guam (+1) Guatemala (+502) Guinea (+224) Guyana (+592) Haiti (+509) Honduras (+504) Hong Kong (+852) Hungary (+36) Iceland (+354) India (+91) Indonesia (+62) Iraq (+964) Ireland (+353) Israel (+972) Italy (+39) Jamaica (+1) Japan (+81) Jordan (+962) Kenya (+254) Korea, Republic of (+82) Kosovo (+383) Kuwait (+965) Kyrgyzstan (+996) Laos (+856) Latvia (+371) Lebanon (+961) Lesotho (+266) Liberia (+231) Libya (+218) Liechtenstein (+423) Lithuania (+370) Luxembourg (+352) Macao (+853) Macedonia (+389) Madagascar (+261) Malawi (+265) Malaysia (+60) Maldives (+960) Mali (+223) Malta (+356) Martinique (+596) Mauritania (+222) Mauritius (+230) Mexico (+52) Monaco (+377) Mongolia (+976) Montenegro (+382) Montserrat (+1) Morocco/Western Sahara (+212) Mozambique (+258) Namibia (+264) Nepal (+977) Netherlands (+31) New Zealand (+64) Nicaragua (+505) Niger (+227) Nigeria (+234) Norway (+47) Oman (+968) Pakistan (+92) Palestinian Territory (+970) Panama (+507) Paraguay (+595) Peru (+51) Philippines (+63) Poland (+48) Portugal (+351) Puerto Rico (+1) Qatar (+974) Reunion/Mayotte (+262) Romania (+40) Russia/Kazakhstan (+7) Rwanda (+250) Samoa (+685) San Marino (+378) Saudi Arabia (+966) Senegal (+221) Serbia (+381) Seychelles (+248) Sierra Leone (+232) Singapore (+65) Slovakia (+421) Slovenia (+386) South Africa (+27) Spain (+34) Sri Lanka (+94) St Kitts and Nevis (+1) St Lucia (+1) St Vincent Grenadines (+1) Sudan (+249) Suriname (+597) Swaziland (+268) Sweden (+46) Switzerland (+41) Taiwan (+886) Tajikistan (+992) Tanzania (+255) Thailand (+66) Togo (+228) Tonga (+676) Trinidad and Tobago (+1) Tunisia (+216) Turkey (+90) Turks and Caicos Islands (+1) Uganda (+256) Ukraine (+380) United Arab Emirates (+971) United Kingdom (+44) United States (+1) Uruguay (+598) Uzbekistan (+998) Venezuela (+58) Vietnam (+84) Virgin Islands, British (+1) Virgin Islands, U.S. (+1) Yemen (+967) Zambia (+260) Zimbabwe (+263) Enter mobile number Enter the OTP sent To receive SMS updates, please verify your number. To proceed with just email click ‘Subscribe’",
      "cover_image_url": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "I Started Programming When I Was 7. I'm 50 Now, and the Thing I Loved Has Changed",
      "url": "https://www.jamesdrandall.com/posts/the_thing_i_loved_has_changed/",
      "published": "2026-02-10T15:08:36+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.jamesdrandall.com/posts/the_thing_i_loved_has_changed/\">https://www.jamesdrandall.com/posts/the_thing_i_loved_has_changed/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46960675\">https://news.ycombinator.com/item?id=46960675</a></p> <p>Points: 107</p> <p># Comments: 82</p>",
      "content_text": "I Started Programming When I Was 7. I'm 50 Now, and the Thing I Loved Has Changed I wrote my first line of code in 1983. I was seven years old, typing BASIC into a machine that had less processing power than the chip in your washing machine. I understood that machine completely. Every byte of RAM had a purpose I could trace. Every pixel on screen was there because I’d put it there. The path from intention to result was direct, visible, and mine. Forty-two years later, I’m sitting in front of hardware that would have seemed like science fiction to that kid, and I’m trying to figure out what “building things” even means anymore. This isn’t a rant about AI. It’s not a “back in my day” piece. It’s something I’ve been circling for months, and I think a lot of experienced developers are circling it too, even if they haven’t said it out loud yet. The era that made me My favourite period of computing runs from the 8-bits through to about the 486DX2-66. Every machine in that era had character. The Sinclair Spectrum with its attribute clash. The Commodore 64 with its SID chip doing things the designers never intended. The NES with its 8-sprite-per-scanline limit that made developers invent flickering tricks to cheat the hardware. And the PC — starting life as a boring beige box for spreadsheets, then evolving at breakneck pace through the 286, 386, and 486 until it became a gaming powerhouse that could run Doom. You could feel each generation leap. Upgrading your CPU wasn’t a spec sheet exercise — it was transformative. These weren’t just products. They were engineering adventures with visible tradeoffs. You had to understand the machine to use it. IRQ conflicts, DMA channels, CONFIG.SYS and AUTOEXEC.BAT optimisation, memory managers — getting a game to run was the game. You weren’t just a user. You were a systems engineer by necessity. And the software side matched. Small teams like id Software were going their own way, making bold technical decisions because nobody had written the rules yet. Carmack’s raycasting in Wolfenstein, the VGA Mode X tricks in Doom — these were people pushing against real constraints and producing something genuinely new. Creative constraints bred creativity. Then it professionalised. Plug and Play arrived. Windows abstracted everything. The Wild West closed. Computers stopped being fascinating, cantankerous machines that demanded respect and understanding, and became appliances. The craft became invisible. But it wasn’t just the craft that changed. The promise changed. When I started, there was a genuine optimism about what computers could be. A kid with a Spectrum could teach themselves to build anything. The early web felt like the greatest levelling force in human history. Small teams made bold decisions because nobody had written the rules yet. That hope gave way to something I find genuinely distasteful. The machines I fell in love with became instruments of surveillance and extraction. The platforms that promised to connect us were really built to monetise us. The tinkerer spirit didn’t die of natural causes — it was bought out and put to work optimising ad clicks. The thing I loved changed, and then it was put to work doing things I’m not proud to be associated with. That’s a different kind of loss than just “the tools moved on.” But I adapted. That’s what experienced developers, human beings, do. The shifts I rode Over four decades I’ve been through more technology transitions than I can count. New languages, new platforms, new paradigms. CLI to GUI. Desktop to web. Web to mobile. Monoliths to microservices. Tapes, floppy discs, hard drives, SSDs. JavaScript frameworks arriving and dying like mayflies. Each wave required learning new things, but the core skill transferred. You learned the new platform, you applied your existing understanding of how systems work, and you kept building. The tool changed; the craft didn’t. You were still the person who understood why things broke, how systems composed, where today’s shortcut became next month’s mess. I’ve written production code in more languages than some developers have heard of. I’ve shipped software on platforms that no longer exist. I’ve chased C-beams off the shoulder of Orion. And every time the industry lurched in a new direction, the experience compounded. You didn’t start over. You brought everything with you and applied it somewhere new. That’s the deal experienced developers made with the industry: things change, but understanding endures. This time is different I say that knowing how often those words have been wrong throughout history. But hear me out. Previous technology shifts were “learn the new thing, apply existing skills.” AI isn’t that. It’s not a new platform or a new language or a new paradigm. It’s a shift in what it means to be good at this. I noticed it gradually. I’d be working on something — building a feature, designing an architecture — and I’d realise I was still doing the same thing I’d always done, just with the interesting bits hollowed out. The part where you figure out the elegant solution, where you wrestle with the constraints, where you feel the satisfaction of something clicking into place — that was increasingly being handled by a model that doesn’t care about elegance and has never felt satisfaction. Cheaper. Faster. But hollowed out. I’m not typing the code anymore. I’m reviewing it, directing it, correcting it. And I’m good at that — 42 years of accumulated judgment about what works and what doesn’t, what’s elegant versus what’s expedient, how systems compose and where they fracture. That’s valuable. I know it’s valuable. But it’s a different kind of work, and it doesn’t feel the same. The feedback loop has changed. The intimacy has gone. The thing that kept me up at night for decades — the puzzle, the chase, the moment where you finally understand why something isn’t working — that’s been compressed into a prompt and a response. And I’m watching people with a fraction of my experience produce superficially similar output. The craft distinction is real, but it’s harder to see from the outside. Harder to value. Maybe harder to feel internally. The abstraction tower Here’s the part that makes me laugh, darkly. I saw someone on LinkedIn recently — early twenties, a few years into their career — lamenting that with AI they “didn’t really know what was going on anymore.” And I thought: mate, you were already so far up the abstraction chain you didn’t even realise you were teetering on top of a wobbly Jenga tower. They’re writing TypeScript that compiles to JavaScript that runs in a V8 engine written in C++ that’s making system calls to an OS kernel that’s scheduling threads across cores they’ve never thought about, hitting RAM through a memory controller with caching layers they couldn’t diagram, all while npm pulls in 400 packages they’ve never read a line of. But sure. AI is the moment they lost track of what’s happening. The abstraction ship sailed decades ago. We just didn’t notice because each layer arrived gradually enough that we could pretend we still understood the whole stack. AI is just the layer that made the pretence impossible to maintain. The difference is: I remember what it felt like to understand the whole machine. I’ve had that experience. And losing it — even acknowledging that it was lost long before AI arrived — is a kind of grief that someone who never had it can’t fully feel. What remains I don’t want to be dishonest about this. There’s a version of this post where I tell you that experience is more valuable than ever, that systems thinking and architectural judgment are the things AI can’t replace, that the craft endures in a different form. And that’s true. When I’m working on something complex — juggling system-level dependencies, holding a mental model across multiple interacting specifications, making the thousand small decisions that determine whether something feels coherent or just works — I can see how I still bring something AI doesn’t. The taste. The judgment. The pattern recognition from decades of seeing things go wrong. AI tools actually make that kind of thinking more valuable, not less. When code generation is cheap, the bottleneck shifts to the person who knows what to ask for, can spot when the output is subtly wrong, and can hold the whole picture together. Typing was never the hard part. But I’d be lying if I said it felt the same. It doesn’t. The wonder is harder to access. The sense of discovery, of figuring something out through sheer persistence and ingenuity — that’s been compressed. Not eliminated, but compressed. And something is lost in the compression, even if something is gained. The fallow period I turned 50 recently. Four decades of intensity, of crafting and finding satisfaction and identity in the building. And now I’m in what I’ve started calling a fallow period. Not burnout exactly. More like the ground shifting under a building you thought that although ever changing also had a permanence, and trying to figure out where the new foundation is. I don’t have a neat conclusion. I’m not going to tell you that experienced developers just need to “push themselves up the stack” or “embrace the tools” or “focus on what AI can’t do.” All of that is probably right, and none of it addresses the feeling. The feeling is: I gave 42 years to this thing, and the thing changed into something I’m not sure I recognise anymore. Not worse, necessarily. Just different. And different in a way that challenges the identity I built around it and doesn’t satisfy in the way it did. I suspect a lot of developers over 40 are feeling something similar and not saying it, because the industry worships youth and adaptability and saying “this doesn’t feel like it used to” sounds like you’re falling behind. I’m not falling behind. I’m moving ahead, taking advantage of the new tools, building faster than ever, and using these tools to help others accelerate their own work. I’m creating products I could only have dreamt of a few years ago. But at the same time I’m looking at the landscape, trying to figure out what building means to me now. The world’s still figuring out its shape too. Maybe that’s okay. Maybe the fallow period is the point. Not something to push through, but something to be in for a while. I started programming when I was seven because a machine did exactly what I told it to, felt like something I could explore and ultimately know, and that felt like magic. I’m fifty now, and the magic is different, and I’m learning to sit with that. Photo by Javier Allegue Barros on Unsplash",
      "cover_image_url": "https://www.jamesdrandall.com/posts/the_thing_i_loved_has_changed/metadata.png"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "YouTubers aren’t relying on ad revenue anymore - here's how some are diversifying",
      "url": "https://techcrunch.com/2026/02/10/youtubers-arent-relying-on-ad-revenue-anymore-heres-how-some-are-diversifying/",
      "published": "2026-02-10T15:06:26+00:00",
      "summary": "YouTubers are no longer just creators, and in some cases, their side businesses are growing faster than their channels.",
      "content_text": "YouTube has become the biggest platform out there, offering tons of opportunities for creators to earn a living. Back in June, the company reported that its creative ecosystem added over $55 billion to the U.S. GDP and created more than 490,000 full-time jobs. However, many YouTubers have reduced their reliance on ad revenue and brand deals. There are several reasons for this shift. First, ad revenue can be unpredictable. With YouTube continually updating its policies, some creators find it challenging to secure ads for their videos, which can negatively impact their earnings. They’ve also realized that income from these streams can vanish unexpectedly. Recognizing the volatility of platform-dependent revenue, many YouTubers are no longer just creators. They’re vertically integrated media companies with parallel businesses, including product lines, brick-and-mortar ventures, and consumer brands that can outlast algorithm changes and policy shifts. In some cases, these side businesses are growing faster and more sustainably than their YouTube channels. MrBeast Image Credits: Beast Industries Jimmy Donaldson, known as MrBeast, who has 442 million subscribers, isn’t just one of the platform’s biggest creators — he’s its most aggressive entrepreneur. In November 2025, for example, The Times reported that the YouTuber is set to open a theme park in Saudi Arabia, with rides inspired by his video content. Among other features, there will supposedly be a game where six players stand on trap doors and must press a button when it lights up or fall down. MrBeast is also venturing into telecommunications. He plans to establish a mobile virtual network operator (MVNO), which could involve partnering with one of the major operators, such as AT&T, T-Mobile, or Verizon. Techcrunch event Boston, MA | June 23, 2026 Additionally, the YouTuber was spotted filing a trademark application for a mobile app that offers banking, financial advisory, and crypto exchange services. In February 2026, MrBeast announced the acquisition of Step , the banking app targeted toward Gen Z users. But there’s so much more. What started with a merchandise store in 2018 — ShopMrBeast — has exploded into a broad business portfolio, including his now three-year-old snack brand, Feastables. Feastables’ initial product was the “MrBeast Bar,” a chocolate bar that generated over $10 million in sales within its first 72 hours, selling over 1 million bars at launch. As of today, Feastables is more profitable than his YouTube content and even his “Beast Games” competition series on Prime Video. In 2024, Feastables generated roughly $250 million in revenue and over $20 million in profit, while his media business lost approximately $80 million. Other ventures include his packaged food brand Lunchly (co-founded with YouTubers Logan Paul and KSI), the toy line MrBeast Lab, MrBeast Burger , and the analytics platform Viewstats . He even attempted to buy the U.S. operations of TikTok by joining the American Investor Consortium, a group of investors led by Employer.com founder Jesse Tinsley. Emma Chamberlain Chamberlain Coffee. Image Credits: Chamberlain Coffee Emma Chamberlain , who rose to fame as a teen vlogger in 2016, now has over 12 million subscribers and has found success in the beverage industry. She launched her coffee brand, Chamberlain Coffee, in 2019, which offers a variety of products, including cold brew, coffee pods, ground and whole bean options, as well as tea and matcha. Notably, other YouTubers have followed suit, such as Jacksepticeye with his Top of the Mornin’ Coffee brand and Philip DeFranco with Wake & Make Coffee. In 2023, Chamberlain Coffee had a significant year, introducing ready-to-drink canned lattes and reaching approximately $20 million in revenue, according to Forbes . The brand recently experienced even more substantial growth, opening its first physical location in January. Previously, it had only an online and retail presence at places like Target, Sprouts, and Walmart. Although Chamberlain Coffee faced some challenges in 2024 due to supplier issues, it’s expected to rebound, with projected revenue growth of over 50% by 2025, reaching more than $33 million, according to Business Insider . The brand is also aiming for profitability by 2026. Logan Paul Image Credits: Cliff Hawkins / Getty Images Logan Paul (23.6 million subscribers) is now known for his wrestling career but was earlier known for numerous controversies, like an infamous 2017 video and an allegedly scammy NFT project , CryptoZoo. He also gained attention through his energy drink brand, Prime, which achieved rapid viral success in 2022. The brand, co-founded by YouTuber KSI, surpassed $1.2 billion in sales in 2023, a figure far exceeding what most content creators earn from views, ads, and brand deals. However, it has since faced declining sales, regulatory scrutiny for its high caffeine content, and lawsuits from business partners. Sales have particularly cooled in the U.K., where revenue dropped by about 70% from 2023 to 2024. Another venture of his, Maverick Apparel, made between $30 million and $40 million in 2020. His brother, Jake Paul, is also involved in various ventures, including co-founding the Anti Fund, which has touted past investments in OpenAI, Anduril, Ramp, and Cognition, among others. The younger Paul also owns a grooming line, called W, and a mobile betting platform called Betr. Ryan’s World Ryan’s World , hosted by 13-year-old Ryan Kaji, is another prominent YouTuber with a staggering following. Ryan rose to fame through his toy reviews and unboxing videos, which have captivated nearly 40 million young viewers. In addition to his YouTube success, Kaji has expanded his brand through a line of toys and apparel that are sold in major retail chains and that reportedly generated over $250 million in revenue in 2020. Kaji and his family have since diversified their ventures, including launching a TV show and an app that provides educational content tailored for children. Rosanna Pansino Image Credits: rosannopansino.com Rosanna Pansino is a popular baker on YouTube known for her baking tutorials and themed treats. With 14.8 million subscribers, she gained fame for her recipes inspired by pop culture, gaming, and movies. Beyond YouTube, Pansino has released several cookbooks that have been well-received, expanding her Nerdy Nummies brand. She also sells baking tools at several retailers, such as Amazon. Other YouTubers have ventured into cookware and food products as additional revenue streams. Notable examples include cook and author Andrew Rea, known by the pseudonym Babish, who launched his Babish Cookware brand in 2021, as well as comedy duo Rhett & Link, who sell MishMash Cereal . Michelle Phan Ipsy founders Jennifer Goldfarb (left), Marcelo Camberos, and Michelle Phan (right) Michelle Phan gained fame in 2007 with her makeup tutorials, becoming one of the first beauty influencers to effectively monetize her content. In addition to her successful YouTube career, she co-founded the beauty subscription service Ipsy , which has become highly popular. Phan also has her own makeup line, EM Cosmetics. Huda Kattan Image Credits: Huda Beauty Huda Kattan founded the globally recognized beauty brand Huda Beauty in 2013. She sold a minority stake to private equity firm TSG Consumer Partners in 2017 but bought it back in June after investor pressure to bring in senior leadership clashed with her vision for the fast-moving brand, which reportedly brings in hundreds of millions of dollars in sales each year. Many influencers have created their own makeup brands. Other well-known makeup brands launched by YouTube influencers include Jeffree Star Cosmetics and Tati Beauty.",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2023/06/money-grab-bryce.jpg?resize=1200,675"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "The US is flirting with its first-ever population decline",
      "url": "https://www.bloomberg.com/news/articles/2026-01-30/trump-immigration-crackdown-could-shrink-us-population-for-first-time",
      "published": "2026-02-10T15:05:24+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.bloomberg.com/news/articles/2026-01-30/trump-immigration-crackdown-could-shrink-us-population-for-first-time\">https://www.bloomberg.com/news/articles/2026-01-30/trump-immigration-crackdown-could-shrink-us-population-for-first-time</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46960624\">https://news.ycombinator.com/item?id=46960624</a></p> <p>Points: 67</p> <p># Comments: 178</p>",
      "content_text": "<p>Article URL: <a href=\"https://www.bloomberg.com/news/articles/2026-01-30/trump-immigration-crackdown-could-shrink-us-population-for-first-time\">https://www.bloomberg.com/news/articles/2026-01-30/trump-immigration-crackdown-could-shrink-us-population-for-first-time</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46960624\">https://news.ycombinator.com/item?id=46960624</a></p> <p>Points: 67</p> <p># Comments: 178</p>",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "Hauler Hero collects $16M for its AI waste management software",
      "url": "https://techcrunch.com/2026/02/10/hauler-hero-collects-16m-for-its-ai-waste-management-software/",
      "published": "2026-02-10T15:00:00+00:00",
      "summary": "Hauler Hero has seen its customer base, revenue, and head count double since the company raised its seed round in 2024.",
      "content_text": "Hauler Hero has picked up fresh funds as demand for its AI-powered waste management software continues to pile up. The New York-based startup raised $16 million in a Series A round led by Frontier Growth with participation from K5 Global and Somersault Ventures, among others, TechCrunch has learned. Hauler Hero has raised more than $27 million in venture capital, to date. Hauler Hero has developed an all-in-one software platform for waste management companies that covers a variety of functions, including customer relationship management, billing, and routing. And now, like so many other software companies, Hauler Hero plans to offer AI agents to its customers as well. Hauler Hero was co-founded in 2020 by CEO Mark Hoadley and brother-in-law Ben Sikma, who encountered outdated software while working on mergers and acquisitions in the sector. Their aim was to bring waste management into the modern era. “All of the existing software in the space, they were clunky, old,” Hoadley told TechCrunch in 2024. “Sometimes we’d talk about how this one reminds us of the Oregon Trail, this reminds us of the cell phone Michael Douglas used in ‘Wall Street.’ They’re very clunky and antiquated.” The numbers suggest the company has had some success; Hauler Hero said it has helped facilitate 35 million trash pickups since inception in 2020. And it continues to grow, according to Hoadley, who told TechCrunch the company has doubled its head count, revenue and customer base since announcing its seed round in late 2024 . Hauler Hero has added new features to its platform since its seed round, Hoadley said. The most notable is a product that captures images from third-party cameras on garbage trucks and send them directly to a software command center. This information helps waste management companies confirm pickups, verify billing, and keep a better eye on their distributed fleet, according to the company. Techcrunch event Boston, MA | June 23, 2026 Hoadley said some sanitation workers, and their respective unions, don’t love this tech upgrade. But he added that most unions prevent the footage from being used against the drivers for disciplinary measures, for example, and that the images can actually help reduce liability for drivers in accidents or if they are accused of missing a pickup. “Think of it as like running a factory with no roof and no visibility into what’s happening in that factory,” Hoadley said. “If you don’t have visibility into what’s happening out there, it’s very hard to have quality control. And the quality control in this case is what was out there, was it picked up?” A trio of AI agents Hauler Hero is now working to develop and deploy a set of three AI agents. One, Hero Vision, is focused on automatically identifying service issues and revenue opportunities. Hero Chat is a chatbot for customer inquiries. Hero Route will use route data to automatically adjust routes to make them more efficient. “There’s an enormous amount of data in our system about pickups, work orders, billing information, and if you give these agents an understanding of your data model, you can just go tell them, hey, go build me a chart,” Hoadley said. Hauler Hero plans to use some of its recent funding to commercialize these agents in addition to bulking up its offerings for municipal entities, a growing group of the company’s customers. “It wasn’t something that I necessarily thought was going to be part of this journey,” Hoadley said about working with these public entities. “It kind of came organically as many things do. We just had inbound from some municipalities and eventually we sold one of them and then we sold another.” Hoadley said that he thinks some of that demand is driven by the fact that its two main competitors, Routeware and Wastech , merged in 2024, leaving government entities with little choice of providers to work with. Hoadley said the company is now focused on fine-tuning and scaling its product — and maybe a some new additions as well. “We’re going to continue to chart the course for the next decade of being at the cutting edge of delivering value for customers, leveraging all the latest tools,” Hoadley said. “There’s a lot more fun things and valuable things that we can do for our customers, and we’re really excited about it.” This piece has been updated to better reflect which investors participated in the Series A.",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2024/12/Hauler-Hero-_-Mark.jpg?resize=1200,800"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "Romeo is a Dead Man is bizarre, bloody, and exactly what makes Grasshopper special",
      "url": "https://www.theverge.com/games/876251/romeo-is-a-dead-man-grasshopper-manufacture-ps5-xbox-steam",
      "published": "2026-02-10T15:00:00+00:00",
      "summary": "A postmodern surrealist tale of geopolitics. An otaku's hellbent journey to become the top-ranked assassin. A high school cheerleader who kills zombies with a chainsaw and pompoms, with a story co-written by James Gunn. Every project under the Grasshopper Manufacture umbrella sounds like an unlikely anecdote. The Japanese video game studio founded in 1998 by [&#8230;]",
      "content_text": "A postmodern surrealist tale of geopolitics. An otaku’s hellbent journey to become the top-ranked assassin. A high school cheerleader who kills zombies with a chainsaw and pompoms, with a story co-written by James Gunn. Every project under the Grasshopper Manufacture umbrella sounds like an unlikely anecdote. The Japanese video game studio founded in 1998 by Goichi “Suda51” Suda proudly carries a B-movie spirit, blending absurd concepts, complex characters, and cascades of pixelated blood on screen. Over the years, the developer’s work has often received a mixed critical reception, and the founder doesn’t think there’s a game in Grasshopper’s portfolio that he would consider “financially successful.” Yet, it has also accrued a loyal following, carving its own space in the industry with games that carry an indistinguishable look and feel. And few Grasshopper releases encapsulate the studio’s ethos more than the latest, the action game Romeo is a Dead Man . Each game has an element of surprise. Whether it is the visual style, leaning on (often rock-adjacent) original songs or licensed tracks for the ambiance, or just being proudly wacky, the shared DNA is always portrayed in some way. They can be rough around the edges — combat and level design often take a backseat to aesthetics — but the games always try new things. While Suda has distanced himself from the “punk’s not dead” mantra, he still believes in the importance of doing something that others aren’t, regardless of financial and critical results. Which brings us to Romeo is a Dead Man. It’s an action game starring Romeo Stargazer, a sheriff’s deputy who, right at death’s door, is implanted with a futuristic device and brought back to life as a Dead Man — half dead, half alive. In that process, he’s recruited as an FBI Space-Time special agent. His assignment? To find his Juliet — literally. As it turns out, his girlfriend appears to be an entity that is wreaking havoc in many timelines. Who better than her lover to stop it? If this brief introduction to the sci-fi story sounds confusing, don’t fret. It only gets weirder. Romeo is saved by his grandfather, who travels from the future and then tags along in his crusade as a living jacket patch. The first timeline rendition of Juliet he brawls with is a colossal beheaded naked body called “Everyday is Like Monday” (nobody tell Morrissey). You spend most of the game fighting zombies, but you can also harvest them from a farm in your spaceship and use their abilities in combat. Romeo is a Dead Man is painstakingly self-referential The story doesn’t take itself too seriously. At its core, similar to previous Grasshopper games, there’s a galore of pop culture references: Back to the Future , Star Wars , Twin Peaks , The Clash , Mobile Suit Gundam , you name it. Romeo is a Dead Man is also painstakingly self-referential. Romeo spouts phrases like “fuckhead,” one of Travis Touchdown’s no-so-heroic lines in No More Heroes , as well as “kill the past,” the latter referencing not just past games but a collective term for games directed by Suda51 with similar themes and elements (look, it’s complicated ). Every time you meet a new character or open a menu you haven’t seen before, there’s a chance of a visual or written reference to a game from the studio’s portfolio. Romeo’s defining trait is telling this story through mixed media. It starts with a handmade diorama model, and follows with influences from comic books, Japanese animation, live-action, and pixel art. Right until the end credits, I kept being surprised by the sheer creativity on display and how it all meshed together. It’s not as groundbreaking as, say, Spider-Man: Into the Spider-Verse was for animation as a medium. It’s more of an homage. In 2026, this homage feels like a culmination of the past couple of years. Grasshopper has long been vocal about championing independent games, including crossovers of a plethora of indie studios as in-game shirts in Travis Strikes Again: No More Heroes, and regularly appearing in Devolver Digital showcases, the publisher behind the likes of Cult of the Lamb , Inscryption , and Hotline Miami . The latter is Suda’s favorite game, and one he happily gushed about with me for an hour for its 10th anniversary in 2022. In his words, Hotline Miami , which was developed by a two-person team, rekindled a spark. “When I first started up Grasshopper, I always had a preference and sort of a tendency to work with smaller teams, so even if we’re working with bigger companies on a bigger project, I’ve always felt more comfortable on smaller teams,” he told me. “So that was sort of an extra little piece of stimulation for me.” Image: Grasshopper Manufacture Despite this, Grasshopper has a history of partnering with big companies. In the early days, the studio worked on licensed games for Bandai Namco, and Killer7 was published by Capcom . Results were mixed. The most notorious was a collaboration with Electronic Arts for Shadows of the Damned . The project — which also involved developer Shinji Mikami, known for his work on Resident Evil — was tangled in a constant creative back and forth , leading to five different drafts during development and several compromises to the original vision. How did that all turn out? Let’s just say the name Damon Ricotello, a villainous corporate CEO in No More Heroes , and an antagonist you fight against, sounds very similar to former EA CEO John Riccitiello. In 2021, publisher NetEase acquired Grasshopper after a tenure under GunHo Online Entertainment. But the studio is self-publishing Romeo is a Dead Man . In an interview with VGC , Suda said that, with Grasshopper’s latest being a brand new series, the team wanted to “be able to put out this game exactly the way we wanted to put it out on our own terms.” As much financial backing as it has from its parent company, the paradigm for the studio hasn’t been ideal. The conglomerate has been cutting back and a 2024 report from Game File suggested the company plans to continue to divest from overseas studios like Grasshopper. It’s too early to predict how Romeo is a Dead Man will fare for Grasshopper, both critically and financially. Like many of the studio’s games, it looks great and has fascinating ideas, but also has some issues. The combat system is flashy but shallow, focused more on button mashing than any real strategy. Level design also gets in the way of pacing, as you’re constantly forced to travel back and forth to a labyrinthine cyberspace. But Romeo is a Dead Man wouldn’t really be a Grasshopper game if it wasn’t a little rough. If anything, it’s the ideal showcase of what the studio is all about — it builds on three decades of experience to create an experience that’s bizarre, enchanting, and unlike anything else out there. Romeo is a Dead Man is available on the PS5, Xbox, and Steam. Follow topics and authors from this story to see more like this in your personalized homepage feed and to receive email updates. Diego Nicolás Argüello Entertainment Games Review Gaming",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/ROMEO_19.png?quality=90&strip=all&crop=0%2C3.4613147178592%2C100%2C93.077370564282&w=1200"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Vercel's CEO offers to cover expenses of 'Jmail' as it has become the number 1 site for tracking the Epstein files",
      "url": "https://www.threads.com/@qa_test_hq/post/DUkC_zjiGQh",
      "published": "2026-02-10T14:58:35+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.threads.com/@qa_test_hq/post/DUkC_zjiGQh\">https://www.threads.com/@qa_test_hq/post/DUkC_zjiGQh</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46960517\">https://news.ycombinator.com/item?id=46960517</a></p> <p>Points: 98</p> <p># Comments: 82</p>",
      "content_text": "",
      "cover_image_url": "https://scontent-sjc6-1.cdninstagram.com/v/t51.2885-15/629828907_17864370486577687_3435529746473425972_n.jpg?stp=dst-jpg_e35_tt6&efg=eyJ2ZW5jb2RlX3RhZyI6InRocmVhZHMuRkVFRC5pbWFnZV91cmxnZW4uNDc1eDY4MC5zZHIuZjgyNzg3LmRlZmF1bHRfaW1hZ2UuYzIifQ&_nc_ht=scontent-sjc6-1.cdninstagram.com&_nc_cat=107&_nc_oc=Q6cZ2QFZtsOrK6HJV2EWvIH7K7Kjw2c6VQJFZjVRJwVuzkXkZTag0sOOmdRqtc6mYbFriu8&_nc_ohc=VXiNs4Y0e7UQ7kNvwH41lbl&_nc_gid=GDvgr2acHhrrlOMnXsD1Kg&edm=APs17CUBAAAA&ccb=7-5&ig_cache_key=MzgyOTE5ODc2Mzk0ODc5NDkxMw%3D%3D.3-ccb7-5&oh=00_AfvUtIxBa4u9mFq2tI5wWGhy7jHPrJIMnHpq-7dhQJQ9IA&oe=699145A3&_nc_sid=10d13b"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "India orders social media platforms to take down deepfakes faster",
      "url": "https://techcrunch.com/2026/02/10/india-orders-social-media-platforms-to-take-down-deepfakes-faster/",
      "published": "2026-02-10T14:51:13+00:00",
      "summary": "India’s new rules take effect February 20, tightening deepfake oversight and shrinking takedown windows to as little as two hours.",
      "content_text": "India has ordered social media platforms to step up policing of deepfakes and other AI-generated impersonations, while sharply shortening the time they have to comply with takedown orders. It’s a move that could reshape how global tech firms moderate content in one of the world’s largest and fastest growing market for internet services. The changes, published (PDF) on Tuesday as amendments to India’s 2021 IT Rules , bring deepfakes under a formal regulatory framework, mandating the labelling and traceability of synthetic audio and visual content, while also slashing compliance timelines for platforms, including a three-hour deadline for official takedown orders and a two-hour window for certain urgent user complaints. India’s importance as a digital market amplifies the impact of the new rules. With over a billion internet users and a predominantly young population, the South Asian nation is a critical market for platforms like Meta and YouTube, making it likely that compliance measures adopted in India will influence global product and moderation practices. Under the amended rules, social media platforms that allow users to upload or share audio-visual content must require disclosures on whether material is synthetically generated, deploy tools to verify those claims, and ensure that deepfakes are clearly labelled and embedded with traceable provenance data. Certain categories of synthetic content — including deceptive impersonations, non-consensual intimate imagery, and material linked to serious crimes — are barred outright in the rules. Non-compliance, particularly in cases flagged by authorities or users, can expose companies to greater legal liability by jeopardising their safe-harbour protections under Indian law. The rules lean heavily on automated systems to meet those obligations. Platforms are expected to deploy technical tools to verify user disclosures, identify, and label deepfakes, and prevent the creation or sharing of prohibited synthetic content in the first place. “The amended IT Rules mark a more calibrated approach to regulating AI-generated deepfakes,” said Rohit Kumar, founding partner at New Delhi-based policy consulting firm The Quantum Hub. “The significantly compressed grievance timelines — such as the two- to three-hour takedown windows — will materially raise compliance burdens and merit close scrutiny, particularly given that non-compliance is linked to the loss of safe harbour protections.” Techcrunch event Boston, MA | June 23, 2026 Aprajita Rana, a partner at AZB & Partners, a leading Indian corporate law firm, said the rules now focus on AI-generated audio-visual content rather than all online information, while carving out exceptions for routine, cosmetic or efficiency-related uses of AI. However, she cautioned that the requirement for intermediaries to remove content within three hours once they become aware of it departs from established free-speech principles. “The law, however, continues to require intermediaries to remove content upon being aware or receiving actual knowledge, that too within three hours,” Rana said, adding that the labelling requirements would apply across formats to curb the spread of child sexual abuse material and deceptive content. New Delhi-based digital advocacy group Internet Freedom Foundation said the rules risk accelerating censorship by drastically compressing takedown timelines, leaving little scope for human review and pushing platforms toward automated over-removal. In a statement posted on X, the group also raised concerns about the expansion of prohibited content categories and provisions that allow platforms to disclose the identities of users to private complainants without judicial oversight. “These impossibly short timelines eliminate any meaningful human review,” the group said, warning that the changes could undermine free-speech protections and due process. Two industry sources told TechCrunch that the amendments followed a limited consultation process, with only a narrow set of suggestions reflected in the final rules. While the Indian government appears to have taken on board proposals to narrow the scope of information covered — focusing on AI-generated audio-visual content rather than all online material — other recommendations were not adopted. The scale of changes between the draft and final rules warranted another round of consultation to give companies clearer guidance on compliance expectations, the sources said. Government takedown powers have already been a point of contention in India. Social media platforms and civil-society groups have long criticized the breadth and opacity of content removal orders , and even Elon Musk’s X challenged New Delhi in court over directives to block or remove posts , arguing that they amounted to overreach and lacked adequate safeguards. Meta, Google, Snap, X, and the Indian IT ministry did not respond to requests for comments. The latest changes come just months after the Indian government, in October 2025, reduced the number of officials authorized to order content removals from the internet in response to a legal challenge by X over the scope and transparency of takedown powers. The amended rules will come into effect on February 20, giving platforms little time to adjust compliance systems. The rollout coincides with India’s hosting of the AI Impact Summit in New Delhi from February 16 to 20, which is expected to draw senior global technology executives and policymakers to the country.",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2023/12/india-parliament-getty.jpg?w=1200"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Parse, don’t validate",
      "url": "https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/",
      "published": "2026-02-10T14:49:29+00:00",
      "summary": "<p>Article URL: <a href=\"https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/\">https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46960392\">https://news.ycombinator.com/item?id=46960392</a></p> <p>Points: 71</p> <p># Comments: 11</p>",
      "content_text": "Historically, I’ve struggled to find a concise, simple way to explain what it means to practice type-driven design. Too often, when someone asks me “How did you come up with this approach?” I find I can’t give them a satisfying answer. I know it didn’t just come to me in a vision—I have an iterative design process that doesn’t require plucking the “right” approach out of thin air—yet I haven’t been very successful in communicating that process to others. However, about a month ago, I was reflecting on Twitter about the differences I experienced parsing JSON in statically- and dynamically-typed languages, and finally, I realized what I was looking for. Now I have a single, snappy slogan that encapsulates what type-driven design means to me, and better yet, it’s only three words long: Parse, don’t validate. The essence of type-driven design Alright, I’ll confess: unless you already know what type-driven design is, my catchy slogan probably doesn’t mean all that much to you. Fortunately, that’s what the remainder of this blog post is for. I’m going to explain precisely what I mean in gory detail—but first, we need to practice a little wishful thinking. The realm of possibility One of the wonderful things about static type systems is that they can make it possible, and sometimes even easy, to answer questions like “is it possible to write this function?” For an extreme example, consider the following Haskell type signature: foo :: Integer -> Void Is it possible to implement foo ? Trivially, the answer is no , as Void is a type that contains no values, so it’s impossible for any function to produce a value of type Void . That example is pretty boring, but the question gets much more interesting if we choose a more realistic example: head :: [ a ] -> a This function returns the first element from a list. Is it possible to implement? It certainly doesn’t sound like it does anything very complicated, but if we attempt to implement it, the compiler won’t be satisfied: head :: [ a ] -> a head ( x : _ ) = x warning: [-Wincomplete-patterns] Pattern match(es) are non-exhaustive In an equation for ‘head’: Patterns not matched: [] This message is helpfully pointing out that our function is partial , which is to say it is not defined for all possible inputs. Specifically, it is not defined when the input is [] , the empty list. This makes sense, as it isn’t possible to return the first element of a list if the list is empty—there’s no element to return! So, remarkably, we learn this function isn’t possible to implement, either. Turning partial functions total To someone coming from a dynamically-typed background, this might seem perplexing. If we have a list, we might very well want to get the first element in it. And indeed, the operation of “getting the first element of a list” isn’t impossible in Haskell, it just requires a little extra ceremony. There are two different ways to fix the head function, and we’ll start with the simplest one. Managing expectations As established, head is partial because there is no element to return if the list is empty: we’ve made a promise we cannot possibly fulfill. Fortunately, there’s an easy solution to that dilemma: we can weaken our promise. Since we cannot guarantee the caller an element of the list, we’ll have to practice a little expectation management: we’ll do our best return an element if we can, but we reserve the right to return nothing at all. In Haskell, we express this possibility using the Maybe type: head :: [ a ] -> Maybe a This buys us the freedom we need to implement head —it allows us to return Nothing when we discover we can’t produce a value of type a after all: head :: [ a ] -> Maybe a head ( x : _ ) = Just x head [] = Nothing Problem solved, right? For the moment, yes… but this solution has a hidden cost. Returning Maybe is undoubtably convenient when we’re implementing head . However, it becomes significantly less convenient when we want to actually use it! Since head always has the potential to return Nothing , the burden falls upon its callers to handle that possibility, and sometimes that passing of the buck can be incredibly frustrating. To see why, consider the following code: getConfigurationDirectories :: IO [ FilePath ] getConfigurationDirectories = do configDirsString <- getEnv \"CONFIG_DIRS\" let configDirsList = split ',' configDirsString when ( null configDirsList ) $ throwIO $ userError \"CONFIG_DIRS cannot be empty\" pure configDirsList main :: IO () main = do configDirs <- getConfigurationDirectories case head configDirs of Just cacheDir -> initializeCache cacheDir Nothing -> error \"should never happen; already checked configDirs is non-empty\" When getConfigurationDirectories retrieves a list of file paths from the environment, it proactively checks that the list is non-empty. However, when we use head in main to get the first element of the list, the Maybe FilePath result still requires us to handle a Nothing case that we know will never happen! This is terribly bad for several reasons: First, it’s just annoying. We already checked that the list is non-empty, why do we have to clutter our code with another redundant check? Second, it has a potential performance cost. Although the cost of the redundant check is trivial in this particular example, one could imagine a more complex scenario where the redundant checks could add up, such as if they were happening in a tight loop. Finally, and worst of all, this code is a bug waiting to happen! What if getConfigurationDirectories were modified to stop checking that the list is empty, intentionally or unintentionally? The programmer might not remember to update main , and suddenly the “impossible” error becomes not only possible, but probable. The need for this redundant check has essentially forced us to punch a hole in our type system. If we could statically prove the Nothing case impossible, then a modification to getConfigurationDirectories that stopped checking if the list was empty would invalidate the proof and trigger a compile-time failure. However, as-written, we’re forced to rely on a test suite or manual inspection to catch the bug. Paying it forward Clearly, our modified version of head leaves some things to be desired. Somehow, we’d like it to be smarter: if we already checked that the list was non-empty, head should unconditionally return the first element without forcing us to handle the case we know is impossible. How can we do that? Let’s look at the original (partial) type signature for head again: head :: [ a ] -> a The previous section illustrated that we can turn that partial type signature into a total one by weakening the promise made in the return type. However, since we don’t want to do that, there’s only one thing left that can be changed: the argument type (in this case, [a] ). Instead of weakening the return type, we can strengthen the argument type, eliminating the possibility of head ever being called on an empty list in the first place. To do this, we need a type that represents non-empty lists. Fortunately, the existing NonEmpty type from Data.List.NonEmpty is exactly that. It has the following definition: data NonEmpty a = a :| [ a ] Note that NonEmpty a is really just a tuple of an a and an ordinary, possibly-empty [a] . This conveniently models a non-empty list by storing the first element of the list separately from the list’s tail: even if the [a] component is [] , the a component must always be present. This makes head completely trivial to implement: head :: NonEmpty a -> a head ( x :| _ ) = x Unlike before, GHC accepts this definition without complaint—this definition is total , not partial. We can update our program to use the new implementation: getConfigurationDirectories :: IO ( NonEmpty FilePath ) getConfigurationDirectories = do configDirsString <- getEnv \"CONFIG_DIRS\" let configDirsList = split ',' configDirsString case nonEmpty configDirsList of Just nonEmptyConfigDirsList -> pure nonEmptyConfigDirsList Nothing -> throwIO $ userError \"CONFIG_DIRS cannot be empty\" main :: IO () main = do configDirs <- getConfigurationDirectories initializeCache ( head configDirs ) Note that the redundant check in main is now completely gone! Instead, we perform the check exactly once, in getConfigurationDirectories . It constructs a NonEmpty a from a [a] using the nonEmpty function from Data.List.NonEmpty , which has the following type: nonEmpty :: [ a ] -> Maybe ( NonEmpty a ) The Maybe is still there, but this time, we handle the Nothing case very early in our program: right in the same place we were already doing the input validation. Once that check has passed, we now have a NonEmpty FilePath value, which preserves (in the type system!) the knowledge that the list really is non-empty. Put another way, you can think of a value of type NonEmpty a as being like a value of type [a] , plus a proof that the list is non-empty. By strengthening the type of the argument to head instead of weakening the type of its result, we’ve completely eliminated all the problems from the previous section: The code has no redundant checks, so there can’t be any performance overhead. Furthermore, if getConfigurationDirectories changes to stop checking that the list is non-empty, its return type must change, too. Consequently, main will fail to typecheck, alerting us to the problem before we even run the program! What’s more, it’s trivial to recover the old behavior of head from the new one by composing head with nonEmpty : head' :: [ a ] -> Maybe a head' = fmap head . nonEmpty Note that the inverse is not true: there is no way to obtain the new version of head from the old one. All in all, the second approach is superior on all axes. The power of parsing You may be wondering what the above example has to do with the title of this blog post. After all, we only examined two different ways to validate that a list was non-empty—no parsing in sight. That interpretation isn’t wrong, but I’d like to propose another perspective: in my mind, the difference between validation and parsing lies almost entirely in how information is preserved. Consider the following pair of functions: validateNonEmpty :: [ a ] -> IO () validateNonEmpty ( _ : _ ) = pure () validateNonEmpty [] = throwIO $ userError \"list cannot be empty\" parseNonEmpty :: [ a ] -> IO ( NonEmpty a ) parseNonEmpty ( x : xs ) = pure ( x :| xs ) parseNonEmpty [] = throwIO $ userError \"list cannot be empty\" These two functions are nearly identical: they check if the provided list is empty, and if it is, they abort the program with an error message. The difference lies entirely in the return type: validateNonEmpty always returns () , the type that contains no information, but parseNonEmpty returns NonEmpty a , a refinement of the input type that preserves the knowledge gained in the type system. Both of these functions check the same thing, but parseNonEmpty gives the caller access to the information it learned, while validateNonEmpty just throws it away. These two functions elegantly illustrate two different perspectives on the role of a static type system: validateNonEmpty obeys the typechecker well enough, but only parseNonEmpty takes full advantage of it. If you see why parseNonEmpty is preferable, you understand what I mean by the mantra “parse, don’t validate.” Still, perhaps you are skeptical of parseNonEmpty ’s name. Is it really parsing anything, or is it merely validating its input and returning a result? While the precise definition of what it means to parse or validate something is debatable, I believe parseNonEmpty is a bona-fide parser (albeit a particularly simple one). Consider: what is a parser? Really, a parser is just a function that consumes less-structured input and produces more-structured output. By its very nature, a parser is a partial function—some values in the domain do not correspond to any value in the range—so all parsers must have some notion of failure. Often, the input to a parser is text, but this is by no means a requirement, and parseNonEmpty is a perfectly cromulent parser: it parses lists into non-empty lists, signaling failure by terminating the program with an error message. Under this flexible definition, parsers are an incredibly powerful tool: they allow discharging checks on input up-front, right on the boundary between a program and the outside world, and once those checks have been performed, they never need to be checked again! Haskellers are well-aware of this power, and they use many different types of parsers on a regular basis: The aeson library provides a Parser type that can be used to parse JSON data into domain types. Likewise, optparse-applicative provides a set of parser combinators for parsing command-line arguments. Database libraries like persistent and postgresql-simple have a mechanism for parsing values held in an external data store. The servant ecosystem is built around parsing Haskell datatypes from path components, query parameters, HTTP headers, and more. The common theme between all these libraries is that they sit on the boundary between your Haskell application and the external world. That world doesn’t speak in product and sum types, but in streams of bytes, so there’s no getting around a need to do some parsing. Doing that parsing up front, before acting on the data, can go a long way toward avoiding many classes of bugs, some of which might even be security vulnerabilities. One drawback to this approach of parsing everything up front is that it sometimes requires values be parsed long before they are actually used. In a dynamically-typed language, this can make keeping the parsing and processing logic in sync a little tricky without extensive test coverage, much of which can be laborious to maintain. However, with a static type system, the problem becomes marvelously simple, as demonstrated by the NonEmpty example above: if the parsing and processing logic go out of sync, the program will fail to even compile. The danger of validation Hopefully, by this point, you are at least somewhat sold on the idea that parsing is preferable to validation, but you may have lingering doubts. Is validation really so bad if the type system is going to force you to do the necessary checks eventually anyway? Maybe the error reporting will be a little bit worse, but a bit of redundant checking can’t hurt, right? Unfortunately, it isn’t so simple. Ad-hoc validation leads to a phenomenon that the language-theoretic security field calls shotgun parsing . In the 2016 paper, The Seven Turrets of Babel: A Taxonomy of LangSec Errors and How to Expunge Them , its authors provide the following definition: Shotgun parsing is a programming antipattern whereby parsing and input-validating code is mixed with and spread across processing code—throwing a cloud of checks at the input, and hoping, without any systematic justification, that one or another would catch all the “bad” cases. They go on to explain the problems inherent to such validation techniques: Shotgun parsing necessarily deprives the program of the ability to reject invalid input instead of processing it. Late-discovered errors in an input stream will result in some portion of invalid input having been processed, with the consequence that program state is difficult to accurately predict. In other words, a program that does not parse all of its input up front runs the risk of acting upon a valid portion of the input, discovering a different portion is invalid, and suddenly needing to roll back whatever modifications it already executed in order to maintain consistency. Sometimes this is possible—such as rolling back a transaction in an RDBMS—but in general it may not be. It may not be immediately apparent what shotgun parsing has to do with validation—after all, if you do all your validation up front, you mitigate the risk of shotgun parsing. The problem is that validation-based approaches make it extremely difficult or impossible ",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "Autodesk is suing Google over the name of its Flow AI videomaker",
      "url": "https://www.theverge.com/tech/876266/autodesk-google-flow-trademark-infringement-lawsuit",
      "published": "2026-02-10T14:47:14+00:00",
      "summary": "Autodesk, a company known for its suite of 3D design software, is suing Google over claims that it infringed on its \"Flow\" trademark, as reported earlier by Reuters. The lawsuit, filed in a California court last week, alleges that the name of Google's AI video generator, Flow, will likely confuse customers with Autodesk's own AI-enabled [&#8230;]",
      "content_text": "Autodesk, a company known for its suite of 3D design software, is suing Google over claims that it infringed on its “Flow” trademark, as reported earlier by Reuters . The lawsuit, filed in a California court last week , alleges that the name of Google’s AI video generator, Flow, will likely confuse customers with Autodesk’s own AI-enabled filmmaking tools under the “Flow” brand. Autodesk first introduced Flow in 2022 as a cloud-based platform for filmmakers and other creators. It has since rolled out products under the Flow umbrella, including Flow Studio, which uses AI to transform live-action footage into 3D scenes. After Google launched its AI-powered Flow app in May 2025 , Autodesk claims it asked the tech giant to stop using the “Flow” name. Google allegedly responded by saying it would market the product as “Google Flow — rather than just ”Flow.” However, Autodesk claims Google “misrepresented” its intentions about its plans to use the “Flow” brand. The company alleges Google filed a trademark in the Kingdom of Tongo, “where applications are not generally available to the public,” before applying to register the standalone “Flow” name in the US, citing its Tonga application. Autodesk claims confusion between its Flow products and Google’s Flow app has already occurred, alleging that people on social media, magazines, and Google Flow users “have mistakenly referred to Google’s product as ‘Flow Studio.’” Autodesk is asking the court to block Google from using the Flow trademark, as well as for unspecified damages related to the alleged infringement. Google didn’t immediately respond to a request for comment.",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/google-flow-ai.png?quality=90&strip=all&crop=0%2C3.4773183923029%2C100%2C93.045363215394&w=1200"
    },
    {
      "industry": "technology",
      "source": "Ars Technica",
      "title": "Alphabet selling very rare 100-year bonds to help fund AI investment",
      "url": "https://arstechnica.com/gadgets/2026/02/alphabet-selling-very-rare-100-year-bunds-to-help-fund-ai-investment/",
      "published": "2026-02-10T14:44:52+00:00",
      "summary": "Alphabet becomes first tech company to issue 100-year bonds in nearly three decades.",
      "content_text": "Tony Trzcinka, a US-based senior portfolio manager at Impax Asset Management, which purchased Alphabet’s bonds last year, said he skipped Monday’s offering because of insufficient yields and concerns about overexposure to companies with complex financial obligations tied to AI investments. “It wasn’t worth it to swap into new ones,” Trzcinka said. “We’ve been very conscious of our exposure to these hyperscalers and their capex budgets.” Big Tech companies and their suppliers are expected to invest almost $700 billion in AI infrastructure this year and are increasingly turning to the debt markets to finance the giant data center build-out. Alphabet in November sold $17.5 billion of bonds in the US including a 50-year bond—the longest-dated dollar bond sold by a tech group last year—and raised €6.5 billion on European markets. Oracle last week raised $25 billion from a bond sale that attracted more than $125 billion of orders. Alphabet, Amazon, and Meta all increased their capital expenditure plans during their most recent earnings reports, prompting questions about whether they will be able to fund the unprecedented spending spree from their cash flows alone. Last week, Google’s parent company reported annual sales that topped $400 billion for the first time, beating investors’ expectations for revenues and profits in the most recent quarter. It said it planned to spend as much as $185 billion on capex this year, roughly double last year’s total, to capitalize on booming demand for its Gemini AI assistant. Alphabet’s long-term debt jumped to $46.5 billion in 2025, up more than four times the previous year, though it held cash and equivalents of $126.8 billion at the year-end. Investor demand was the strongest on the shortest portion of Monday’s deal, with a three-year offering pricing at only 0.27 percentage points above US Treasuries, versus 0.6 percentage points during initial price discussions, said people familiar with the deal. The longest portion of the offering, a 40-year bond, is expected to yield 0.95 percentage points over US Treasuries, down from 1.2 percentage points during initial talks, the people said. Bank of America, Goldman Sachs, and JPMorgan are the bookrunners on the bond sales across three currencies. All three declined to comment or did not immediately respond to requests for comment. Alphabet did not immediately respond to a request for comment. © 2026 The Financial Times Ltd . All rights reserved. Not to be redistributed, copied, or modified in any way.",
      "cover_image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/yields-1-1152x648.jpg"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "Former GitHub CEO raises record $60M dev tool seed round at $300M valuation",
      "url": "https://techcrunch.com/2026/02/10/former-github-ceo-raises-record-60m-dev-tool-seed-round-at-300m-valuation/",
      "published": "2026-02-10T14:30:00+00:00",
      "summary": "Thomas Dohmke's new startup offers an AI system to allow developers to better manage all the code AI agents produce.",
      "content_text": "Former GitHub CEO Thomas Dohmke has raised the largest-ever seed round for a dev tool startup, according to its lead backer, Felicis. The startup, Entire , has raised $60 million at a $300 million valuation. Entire offers an open source tool to help developers better manage code written by AI agents. Entire’s tech has three components. One is a git-compatible database to unify the AI-produced code. Git is a distributed version control system popular with enterprises and used by open source sites like GitHub and GitLab. Another component is what it calls “a universal semantic reasoning layer” intended to allow multiple AI agents to work together. The final piece is an AI-native user interface designed with agent-to-human collaboration in mind. The first product Entire is releasing is an open-source tool it calls Checkpoints that automatically pairs every bit of software the agent submits for use in a software project with the context that created it, including prompts and transcripts. The idea is to allow the human developer to review, search, and perhaps even learn from why the AI did what it did. Entire hopes to help developers better deal with the large volumes of software created by AI coding agents. Popular open-source projects are particularly overwhelmed these days with suggested code contributions that may or may not be AI slop — meaning poorly designed and possibly unusable code. Dohmke explains in the press release: “We are living through an agent boom, and now massive volumes of code are being generated faster than any human could reasonably understand. The truth is, our manual system of software production — from issues, to git repositories, to pull requests, to deployment — was never designed for the era of AI in the first place.” Techcrunch event Boston, MA | June 23, 2026 Dohmke was CEO of Microsoft’s GitHub for four years, leaving in August 2025 to found a startup, he said in a post on X at the time. During his time there, he oversaw the rise of the popular coding agent GitHub Copilot. Other investors in the seed round include Madrona, M12, Basis Set, Harry Stebbings, Jerry Yang, and Datadog founder and CEO Olivier Pomel.",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2026/02/Github-Thomas.jpeg?resize=1200,800"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Redefining Go Functions",
      "url": "https://pboyd.io/posts/redefining-go-functions/",
      "published": "2026-02-10T14:27:16+00:00",
      "summary": "<p>Article URL: <a href=\"https://pboyd.io/posts/redefining-go-functions/\">https://pboyd.io/posts/redefining-go-functions/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46960118\">https://news.ycombinator.com/item?id=46960118</a></p> <p>Points: 25</p> <p># Comments: 5</p>",
      "content_text": "Redefining Go Functions February 10, 2026 I once wrote a Perl subroutine that would memoize the subroutine that called it. That much was useful, but then it inserted a copy of itself into the caller, so that its callers would be memoized too. A well-placed call to aggressively_memoize could back-propagate to the whole codebase, spreading functional purity like a virus. The resulting program would get faster as it used more memory and became increasingly static. That was possible because Perl, like many interpreted languages, allows functions to be rewritten at runtime: no strict 'refs' ; * { $caller } = $new_sub ; Overuse of this feature earned it the derisive nickname “monkey patching”. Spend a couple hours debugging why your random numbers aren’t so random only to discover you have a mock RNG implanted by some distant dependency and you’ll hate it too. But these days I program mostly in Go where such nonsense isn’t possible. Right? Well, no, not exactly. True, Go doesn’t offer this as a language feature. But a CPU executes instructions from memory, and we can modify memory. Did Go fundamentally change all that? Not at all. In fact, Go gives us all the low-level tools we need to do the job. Let’s say I prefer Alan Jackson’s sense of time over whatever reality time.Now cares to remind me of. So I want this function to replace time.Now : func myTimeNow () time.Time { return time. Date ( 2026 , 1 , 30 , 17 , 0 , 0 , 0 , time. FixedZone ( \"Somewhere\" , - 5 )) } The first thing we need is the address of the real time.Now . The easiest way is with reflect : func main () { addr := reflect. ValueOf (time.Now). Pointer () fmt. Printf ( \"0x%x\\n\" , addr) } Run this program and you’ll get an address. On my computer: $ go build -o main && ./main 0x498b60 Now disassemble the program and note the memory address in the second column: $ go tool objdump -s time.Now main | head -3 TEXT time.Now(SB) /opt/go1.25.5/src/time/time.go time.go:1343 0x498b60 493b6610 CMPQ SP, 0x10(R14) time.go:1343 0x498b64 0f8684000000 JBE 0x498bee The actual addresses may be different for you, but the address from the program output will match the instruction address in the disassembler output. That’s because Go function pointers point to the function’s entry point. We don’t know the length of the function, but we can guess that it’s at least 8 bytes and get a slice based on that: func main () { addr := reflect. ValueOf (time.Now). Pointer () buf := unsafe. Slice (( * byte )(unsafe. Pointer (addr)), 8 ) spew. Dump (buf) } Run that and you’ll see: $ go build -o main && ./main ([]uint8) (len=8 cap=8) { 00000000 49 3b 66 10 0f 86 84 00 |I;f.....| } 49 3b 66 10 matches the first instruction from the disassembled output. Now that we can find a function and read its machine instructions, all that’s left is to modify its behavior. Copying the instructions from our replacement function to the location of the original function seems logical, but relocating machine instructions requires adjusting any relative addresses. That’s solvable, but the replacement function could still be bigger than the original, and then we’d need another solution anyway. The easiest approach is to write a JMP (or branch) instruction at the beginning of the original function to redirect the processor to our new function. Because it’s a JMP , not a CALL , the RET from our replacement function will return to the original caller and none of the remaining instructions from the original function will execute. As long as the arguments are the same for both functions, the caller will be none the wiser. On x86, the code to encode the instruction looks like: func main () { addr := reflect. ValueOf (time.Now). Pointer () buf := unsafe. Slice (( * byte )(unsafe. Pointer (addr)), 8 ) buf[ 0 ] = 0xe9 // JMP src := addr + 5 // Where to jump from dest := reflect. ValueOf (myTimeNow). Pointer () // Where to jump to binary.LittleEndian. PutUint32 (buf[ 1 :], uint32(int32(dest - src))) fmt. Println (time. Now (). Format (time.Kitchen)) } But if you run it, you’ll just get a segfault: unexpected fault address 0x499400 fatal error: fault [signal SIGSEGV: segmentation violation code=0x2 addr=0x499400 pc=0x4a3c9c] Letting a program modify its own code is dangerous, which is why protected memory has been standard for decades. But getting around it is easy—we just need to change the permissions on that memory page. On Unix systems, we do that with mprotect(2) . The start address has to be page-aligned, so we need a helper function: func mprotect (addr uintptr , length int , flags int ) error { pageSize := syscall. Getpagesize () // Round address down to page boundary. pageStart := addr &^ (uintptr(syscall. Getpagesize ()) - 1 ) // Round up to cover complete pages. regionSize := (int(addr - pageStart) + length + pageSize - 1 ) &^ (pageSize - 1 ) region := unsafe. Slice (( * byte )(unsafe. Pointer (pageStart)), regionSize) return syscall. Mprotect (region, flags) } Now we use that to allow writes to the function, and restore the protection afterwards: func main () { addr := reflect. ValueOf (time.Now). Pointer () buf := unsafe. Slice (( * byte )(unsafe. Pointer (addr)), 8 ) mprotect (addr, len(buf), syscall.PROT_READ|syscall.PROT_WRITE|syscall.PROT_EXEC) buf[ 0 ] = 0xe9 // JMP src := addr + 5 // Where to jump from dest := reflect. ValueOf (myTimeNow). Pointer () // Where to jump to binary.LittleEndian. PutUint32 (buf[ 1 :], uint32(int32(dest - src))) mprotect (addr, len(buf), syscall.PROT_READ|syscall.PROT_EXEC) fmt. Println (time. Now (). Format (time.Kitchen)) } $ go build -o main && ./main 5:00PM There you go. It’s 5PM. It’s always 5PM. Here’s the full source code. If you’re on ARM64, you’ll need this version . Aside from different instructions, ARM also requires clearing the instruction cache. (I’ve only tested the ARM64 version on a Raspberry Pi 4 running Linux. I think it will work for Darwin on Apple silicon but I don’t have hardware to test it—if you try it, let me know how it goes.) If you’re on Windows, you won’t have mprotect . Supposedly VirtualProtect is equivalent (also see the wrapper in golang.org/x/sys/windows ). If you get it working on Windows, send me a Gist and I’ll gladly link to it here. The problems Play around with overriding functions and you’ll find that some functions can’t be overridden. Inline functions are a frequent culprit. For example, the compiler will probably inline fmt.Printf because it’s a small wrapper around fmt.Fprintf . If you disassemble a call to it, you’ll see something like this: TEXT main.main(SB) /home/user/dev/gofuncs/main.go main.go:14 0x499960 493b6610 CMPQ SP, 0x10(R14) main.go:14 0x499964 7636 JBE 0x49999c main.go:14 0x499966 55 PUSHQ BP main.go:14 0x499967 4889e5 MOVQ SP, BP main.go:14 0x49996a 4883ec38 SUBQ $0x38, SP print.go:233 0x49996e 488b1d0b5b0d00 MOVQ os.Stdout(SB), BX main.go:15 0x499975 90 NOPL print.go:233 0x499976 488d05ebbc0400 LEAQ go:itab.*os.File,io.Writer(SB), AX print.go:233 0x49997d 488d0d154a0200 LEAQ 0x24a15(IP), CX print.go:233 0x499984 bf0c000000 MOVL $0xc, DI print.go:233 0x499989 31f6 XORL SI, SI print.go:233 0x49998b 4531c0 XORL R8, R8 print.go:233 0x49998e 4d89c1 MOVQ R8, R9 print.go:233 0x499991 e84a99ffff CALL fmt.Fprintf(SB) main.go:28 0x499996 4883c438 ADDQ $0x38, SP main.go:28 0x49999a 5d POPQ BP main.go:28 0x49999b c3 RET main.go:14 0x49999c 0f1f4000 NOPL 0(AX) main.go:14 0x4999a0 e8bb89fdff CALL runtime.morestack_noctxt.abi0(SB) The instructions from print.go result from inlining. The function definition of fmt.Printf exists if you get a pointer to it, but inserting a JMP there won’t matter—nothing calls that address unless you use a function pointer. Generic functions have a similar problem. For brevity, I’ll skip the details, but the gist is that the function you can get a pointer to is different from the function that’s typically called. Overriding methods introduces additional problems. A simple example: type counter struct { A int64 } //go:noinline func (c * counter) Inc () { c.A ++ } func main () { c := & counter{} c. Inc () c. Inc () fmt. Println (c.A) } Unsurprisingly, this outputs 2 . Let’s say we want to replace Inc with the version from this struct instead: type doubleCounter struct { someOtherField int32 A int32 } func (dc * doubleCounter) Inc () { dc.A += 2 } And we call it with: func main () { c := & counter{} c. Inc () c. Inc () redefineFunc (( * counter).Inc, ( * doubleCounter).Inc) c. Inc () fmt. Println (c.A) } If this worked perfectly, the output would be 4 . But it actually prints 8589934594 . doubleCounter.Inc is compiled expecting to operate on a doubleCounter struct, but we’ve forced it to use the counter struct. doubleCounter.A is at the same location as the high 32-bits of counter.A , so the output is 2<<32 + 2 , or 8589934594 . This is contrived, but you can imagine the resulting crash if these were pointers or the chaos if these weren’t simple integers but larger structs. Also consider what would happen if doubleCounter were instead: type doubleCounter struct { someOtherField int32 A int64 } Now it’s adding two to some portion of memory immediately after our instance of counter . Maybe it harmlessly updates some padding. Maybe it corrupts the heap or overwrites an unrelated variable on the stack. Who knows exactly? But I do know you can expect some awful bugs. The only potentially safe way to override a method is if the two structs are identical (or, at least, the same size and you’re very careful). So, yes, you can redefine Go functions—sometimes. Expect bugs. If you really must do this, I made a package to wrap this insidious code in a friendly interface. It only works on Linux/Unix and AMD64 (I hope to port it to ARM soon). For all the reasons above (and a few I didn’t cover), I can’t recommend using it. But it’s fun to hack on and PRs are welcome. Source / History",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Our $200M Series C",
      "url": "https://oxide.computer/blog/our-200m-series-c",
      "published": "2026-02-10T14:20:49+00:00",
      "summary": "<p>Article URL: <a href=\"https://oxide.computer/blog/our-200m-series-c\">https://oxide.computer/blog/our-200m-series-c</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46960036\">https://news.ycombinator.com/item?id=46960036</a></p> <p>Points: 228</p> <p># Comments: 129</p>",
      "content_text": "Well, yes, on both fronts, so let us explain a little. First, we have the luxury of having achieved real product-market fit: we are making a product that people want to buy. This takes on additional dimensions when making something physical: with complexities like manufacturing, inventory, cash-conversion, and shifting supply chains, product-market fit implies getting the unit economics of the business right. All of this is a long way of saying: we did not (and do not) need to raise capital to support the business. So if we didn’t need to raise, why seek the capital? Well, we weren’t seeking it, really. But our investors, seeing the business take off, were eager to support it. And we, in turn, were eager to have them: they were the ones, after all, who joined us in taking a real leap when it felt like there was a lot more risk on the table. They understood our vision for the company and shared our love for customers and our desire to build a singular team. They had been with us in some difficult moments; they know and trust us, as do we them. So being able to raise a Series C purely from our existing investors presented a real opportunity. Still, even from investors that we trust and with a quick close, if the business doesn’t need the money, does it make sense to raise? We have always believed that our biggest challenge at Oxide was time — and therefore capital. We spelled this out in our initial pitch deck from 2019: Challenges slide from Oxide original pitch deck ca. 2019 Six years later, we stand by this, which is not to minimize any of those challenges: the technical challenges were indeed hard; we feel fortunate to have attracted an extraordinary team; and we certainly caught some lucky breaks with respect to the market. With this large Series C, we have entirely de-risked capital going forward, which in turn assures our independence. This last bit is really important, because any buyer of infrastructure has had their heart broken countless times by promising startups that succumbed to acquisition by one of the established players that they were seeking to disrupt. The serial disappointments leave a refreshing bluntness in their wake, and it’s not uncommon for us to be asked directly: \"How do I know you won’t be bought?\" Our intent in starting Oxide was not to be an acquisition target but rather build a generational company; this is our life’s work, not a means to an end. With our Series C, customers don’t have to merely take our word for it: we have the capital to assure our survival into the indefinite future. If our Series B left us with confidence in achieving our mission , our Series C leaves us with certainty: we’re going to kick butt, have fun, not cheat (of course!), love our customers — and change computing forever .",
      "cover_image_url": "https://oxide-computer-huctn5h8o-oxidecomputer.vercel.app/img/blog/series-c/series-c-announcement-og-image.png"
    }
  ]
}