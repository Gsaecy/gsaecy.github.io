{
  "industry": "technology",
  "collected_at": "2026-02-12T16:29:23.107837+00:00",
  "hours": 24,
  "limit": 25,
  "count": 25,
  "items": [
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "The surprising case for AI judges",
      "url": "https://www.theverge.com/podcast/877299/ai-arbitrator-bridget-mccormack-aaa-arbitration-interview",
      "published": "2026-02-12T16:22:33+00:00",
      "summary": "Today, we’re going to talk about the role AI might play in deciding legal disputes. Not just drafting memos and doing research — actually deciding who’s right and who’s wrong, and who should pay. My guest today is Bridget McCormack, the former chief justice for the Michigan Supreme Court and now president and CEO of [&#8230;]",
      "content_text": "Today, we‚Äôre going to talk about the role AI might play in deciding legal disputes. Not just drafting memos and doing research ‚Äî actually deciding who‚Äôs right and who‚Äôs wrong, and who should pay. My guest today is Bridget McCormack, the former chief justice for the Michigan Supreme Court and now president and CEO of the American Arbitration Association. The AAA has been around for exactly 100 years and is the country‚Äôs largest nonprofit arbitrator. You‚Äôve probably heard of arbitration before. It‚Äôs is a form of dispute resolution that allows two parties to resolve conflicts outside the formal court system using a third, neutral party ‚Äî the arbitrator ‚Äî to negotiate a settlement. Verge subscribers, don‚Äôt forget you get exclusive access to ad-free Decoder wherever you get your podcasts. Head here . Not a subscriber? You can sign up here . You may have never found yourself in arbitration, but you‚Äôve almost certainly signed an arbitration clause, in one of the many contracts and terms-of-service agreements that all of us have to sign all the time. Arbitration can be much faster, cheaper, and easier than going to court, so it‚Äôs become a favored way of resolving disputes between businesses. It‚Äôs also, as it turns out, how many employers and large corporations defend against lawsuits, because they can sneak an arbitration clause into the agreements for everything from cellphone service to smart washing machine features or even your employment contract, which can protect them down the line from class action claims. Arbitration is everywhere in our legal landscape, so you can see why an organization like the AAA would want to make it faster, cheaper, and more predictable. For the past several years Bridget and her team have been developing an AI-assisted arbitration platform called the AI Arbitrator , and it‚Äôs now available for use in very specific cases ‚Äî construction disputes that can be resolved entirely on the basis of written documents. As of right now, the AI Arbitrator now has officially one case on its docket. I‚Äôm obviously fascinated at how all of that might work, but you‚Äôll hear Bridget and me really dig in here on what this kind of automation means not just for arbitration, but also the bigger, more fundamental idea of seeking justice, and whether or not our legal system feels fair. Americans‚Äô trust in the judicial system reached a record low in 2024, and you‚Äôll hear Bridget and me go back and forth on whether a system driven by AI can actually help people trust these systems more simply by making each party feel heard and showing its work, something you often don‚Äôt get from a human judge. At the same time, AI systems are AI systems. They‚Äôre new, brittle, and hallucinate facts and dates. It feels like there‚Äôs real danger in handing this kind of power to such a new, and unpredictable, technology. So you‚Äôll hear Bridget discuss where she thinks the lines should really be drawn, how she‚Äôs trying to head off some of the big concerns around AI, and where she sees this going in the future. Again, Bridget was the former chief justice of the Michigan Supreme Court; she was in charge of all the judges in her state. You‚Äôll hear her say several times that people are unreliable. By the way, if you want a broader look at all of this, Verge reporter Lauren Feiner actually published a fantastic feature on AI in the legal system last month, and I highly suggest you go read that if you‚Äôre interested in learning even more. Okay: Bridget McCormack, the president and CEO of the American Arbitration Association, on the AI Arbitrator. Here we go. This interview has been lightly edited for length and clarity. Bridget McCormack, you‚Äôre the president and CEO of the American Arbitration Association. You‚Äôre also the former Chief Justice of the Michigan Supreme Court. Welcome to Decoder . It‚Äôs great to be here. Great to see you. You and I were on a panel a while ago. You were talking about rolling out AI in arbitration. You were also talking about your history overseeing judges in Michigan, which was very funny. I‚Äôm very excited to talk about all of that with you. I just want to start at the very beginning. I suspect you and I are going to end up talking a lot about commercial and business disputes. There‚Äôs a lot there to discuss in the context of AI and arbitration. Most people‚Äôs experience of arbitration is that they just sign a contract. You were the Chief Justice in Michigan; you oversaw the literal legal system in that state. American Arbitration Association is 100 years old, so now you oversee a 100-year-old dominant provider of arbitration. Explain to people what the difference is. It‚Äôs a great starting point. The thing about being the Chief Justice of the Michigan Supreme Court is that, like every state Supreme Court, the Supreme Court of Michigan has administrative oversight of all the courts of the state. When you‚Äôre the chief justice, you‚Äôre kind of the CEO of the public dispute resolution system that most people are stuck dealing with, if they need a little justice or if somebody wants a little justice from them. Like other leadership roles, I had a leadership team and another 300 or so staff of folks who reported up to the leadership team. And it was our job to try and figure out how to improve the experience of people across the state of Michigan who had to go to their local courts because of some legal problem. It‚Äôs an enormous change management job for lots of reasons that are not true in my current job. The thing about running the public dispute resolution system, like the state court system, is that your funding isn‚Äôt based on how well you do. You can‚Äôt perform well one year and have extra revenue for R&D. You have to walk over to the legislature and convince some brand-new representative from Leelanau County that online dispute resolution is really going to increase access to justice. You literally have to pick off legislators from around the state to try and fund what you know is going to be a better way of doing business. At the same time, there are the judges across the state in Michigan, and there are approximately 1,000 judicial officers. I say that because in addition to judges, there are magistrates that report up. They‚Äôre all separately elected and they work in counties that have their own funding systems. So they‚Äôre partly funded by the state and they‚Äôre partly funded by their county. The counties across Michigan are differently resourced, right? Some counties have a larger tax base than others, and they have a bigger budget to work with. So convincing separately elected judges with different budgets that we‚Äôre going to do business a certain way going forward is super complicated. It‚Äôs a very fun change management problem. The AAA, on the other hand, is basically a court system, but a private court system, although I should say at the top, the AAA is a nonprofit. We‚Äôre a fee-for-service nonprofit, but we‚Äôre a nonprofit. We‚Äôve been administering alternative dispute resolution arbitration, but also mediation and any other alternative process parties want, for 100 years as of last Thursday. So we‚Äôve been doing it for a long time and we have administered over half a million cases a year for the last few years, and not just domestically but also in cross-border disputes. Most of them are B2B commercial disputes, but there are also B2C cases, employment consumer cases, and a growing number of self-represented parties. A lot of small and medium businesses, as I‚Äôm sure you know, can‚Äôt afford legal help. They‚Äôre legally naked. And so arbitration is an easier way for them to manage disputes. I want to talk about the difference between a private dispute resolution system and the public dispute resolution system for one more second. But first, actually, you just said something. I‚Äôm so curious about it. It sounds like, as the Chief Justice, you had a role advocating for the court system with the legislature, inside the justice system. Most people never hear about that and never think about that. What was the split in your time? How often did you have to spend time just saying, ‚ÄúHey, can you pay for the courts?‚Äù versus actually being the Chief Justice? I would say the administrative part of the job was significantly more than half compared to the decisional part of the job. It‚Äôs an enormous job. Michigan adjudicates between 3 and 4 million cases a year, and like every other state court, a majority of people who go to court to have cases resolved can‚Äôt afford lawyers. This is the primary place people interact with their government. The kind of justice we deliver, the quality of justice we deliver, it‚Äôs pretty important to, frankly, the rule of law and trust in institutions. I think it‚Äôs one of the most important jobs in government. I‚Äôm curious about that because it feels like the experience you had there really leads to your perspective on how and why AI should enter the legal system. The reason I‚Äôm starting here with your previous experience and not your current job is‚ÄîI encounter this on our show and on our site all the time‚Äîthat people think the legal system is deterministic. Particularly our audience, the tech audience, thinks the legal system is a computer. You can feed it inputs and it‚Äôll API access the law, and then you‚Äôll get some predictable outputs. I‚Äôm always trying to convince people that that‚Äôs not the case. Even hearing you talk about the politics of running the legal system underlines for me that the legal system is absolutely not deterministic. Should it be? Because you‚Äôre the first person I could just like straightforwardly ask that question to. Should the legal system be more predictable and deterministic? It absolutely should be, at least in a majority of cases. In fact, if it were more deterministic, we would have fewer disputes, right? It‚Äôs because it‚Äôs probabilistic‚Äîand I agree with you. It is, for the most part, because it‚Äôs run by humans. You‚Äôve met humans, right? They‚Äôre flawed. And therefore it isn‚Äôt always predictable. If it were more predictable, we would be a more efficient and effective system. We‚Äôd avoid a lot of disputes because people could plan their business around what, in fact, the rule was going to be, and how it was going to be enforced, and how they could count on it being enforced. In my view, that‚Äôs true for most cases for which there is a rule of law, and we know how it‚Äôs been interpreted historically, or at least how it‚Äôs been correctly interpreted historically by the majority of courts. There are always going to be new frontiers in legal. Cases where courts are having to decide how to interpret a new statute. Courts are going to have to figure that out for the first time. That‚Äôs not going to be able to be deterministic. It could get better and better, frankly. I think AI could do a very good job at the front end of statutory drafting, in making sure there was less ambiguity in statutory terms. AI could impact that. But there are even still modern questions about historic provisions in statutes and the constitutions, state and federal, that we have entrusted judges to decide. So I don‚Äôt think it can all be deterministic. I think an awful lot could be, and it would improve the way the law operates. Where do you think that the source of uncertainty in the legal system, as people experience it today, comes from? Is it just that most people can‚Äôt afford a lawyer? Is it that some percentage of judges are just weird old guys? Where does that come from? I don‚Äôt think there‚Äôs a single answer. I do think it‚Äôs relevant that 92 percent of Americans can‚Äôt afford help with their legal problems, and that‚Äôs not just individuals in the kinds of cases individuals end up going to court for. It‚Äôs also true for all small and medium businesses. For the most part, they can‚Äôt afford lawyers. So there‚Äôs an awful lot of trying to navigate legal risk and legal problems without lawyers, and that‚Äôs complicated. That‚Äôs very complicated for judges. Judges who are managing large dockets with many parties without lawyers try and do their best to work their way through those problems, but it‚Äôs not easy. We have a legal system run by humans and humans are imperfect and busy. I want to be very clear that there‚Äôs a big difference between state and federal courts, right? So 95 to 96 percent of cases are heard in state courts, not federal courts. The federal courts do a much smaller number of cases, and generally have‚Äînot better, but larger‚Äîstaff to help them. State courts are managing most disputes with fewer resources and doing the best they can. But if you look at the rate of reversals by appellate courts, by intermediate appellate courts and state Supreme Courts, they‚Äôre getting a lot wrong, right? Humans get things wrong for lots of reasons. What you mean by that rate of reversal, just to unpack that, is that someone goes to court, a state court judge decides there‚Äôs an appeal, which costs money, and that goes up to an appeals court, and the appeals court is overturning that judge. You‚Äôre saying that rate‚Äôs going up or that rate is too high? I don‚Äôt know if it‚Äôs going up. I could probably figure that out, but I don‚Äôt know that off the top of my head. It is a fact that it‚Äôs quite high. The number of cases where an appellate court reverses the work of a lower court is not a low number. It‚Äôs going to be different from state to state and different in the federal appellate courts, but you can benchmark it and it‚Äôs not an insignificant number. I like to use this example. I ran a non-DNA innocence clinic two careers ago, and we know a lot about the rate of wrongful conviction as a result of the DNA exonerations over the last, I don‚Äôt know, 30 years at this point, because there‚Äôs a database now, and we‚Äôve been able to learn both the rate at which mistakes are made‚Äîsometimes they‚Äôre made by juries, but often they‚Äôre made by judges‚Äîand the kinds and qualities of the errors that lead to those mistakes. It‚Äôs a shocking number. The wrongful convictions tell us that in 3 to 5 percent of cases, there was an error made. And you might think, ‚ÄúOh, that‚Äôs kind of a low number.‚Äù If you‚Äôre shooting free throws then probably it is a low number, but if you‚Äôre landing planes, not a great number, right? And I think the criminal justice system should be more like landing planes. The reason I‚Äôm starting there is, and I think you perceive this as well as I do, that the lack of faith and trust in our institutions is kind of pervasive across American society. The legal system is just part of it now, right? Especially if you show up and you don‚Äôt have a lawyer, you don‚Äôt have the money, and then it is a weird old guy as the judge, and then you‚Äôre looking at the statistics and they‚Äôre probably wrong, but you can‚Äôt afford to appeal. Or you‚Äôre just reading the headlines every day. It just feels like there‚Äôs more chaos in the formal legal system than ever. I wanted to start there, because I do feel like the lack of faith in our corporate institutions is equally high, and most people‚Äôs experience with arbitration is, ‚ÄúWell, I just need cell phone service. I‚Äôm not going to read this contract or these 15 contracts to get cell phone service.‚Äù And there‚Äôs a line here that says, ‚ÄúWell, I can‚Äôt even sue AT&T if they get something wrong. I‚Äôm going to end up in arbitration and that arbitration is obvious. Of course, it‚Äôs just going to be against me. There‚Äôs nothing I can do. I‚Äôm just signing away my rights.‚Äù How do you feel about that in this context? Because that feels like as big of a problem as anything. So let me unpack a couple of things you said. I completely agree about the declining trust in institutions, and that the courts are part of that problem. In fact, t",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/DCD-Bridget-McCormack_portrait.png?quality=90&strip=all&crop=0%2C10.711631919237%2C100%2C78.576736161526&w=1200"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "DHS announces the end of its ‚Äúsurge operation‚Äù in Minneapolis, but not entirely",
      "url": "https://www.theverge.com/policy/878017/dhs-ends-minneapolis-operation-metro-surge-tom-homan",
      "published": "2026-02-12T16:11:31+00:00",
      "summary": "The government's siege of the Twin Cities is reportedly coming to an end. \"I have proposed, and President Trump has concurred, that this surge operation conclude,\" border czar Tom Homan announced Thursday. This isn't the first time Department of Homeland Security officials have claimed that Operation Metro Surge, its massive campaign in Minnesota, was slowing [&#8230;]",
      "content_text": "The government‚Äôs siege of the Twin Cities is reportedly coming to an end. ‚ÄúI have proposed, and President Trump has concurred, that this surge operation conclude,‚Äù border czar Tom Homan announced Thursday . This isn‚Äôt the first time Department of Homeland Security officials have claimed that Operation Metro Surge, its massive campaign in Minnesota, was slowing down or ending altogether. Homan teased a ‚Äúdrawdown‚Äù in late January , and a few days later President Donald Trump said he had ordered the withdrawal of 700 agents from the state. But an estimated 2,000 agents remained, and residents reported that raids continued. Even by Homan‚Äôs own admission, it‚Äôs still not over. ‚ÄúA small footprint of personnel will remain for a period of time to close out and transition full command and control back to the field office,‚Äù Homan said. ‚ÄúI will also remain on the ground for a little longer to oversee the drawdown of this operation and include its success.‚Äù There are typically 150 federal immigration officers in Minnesota, according to MPR News . Their presence increased twenty-fold under Metro Surge, an operation that began after a right-wing vlogger‚Äôs alleged widespread fraud at daycares operated by Minneapolis‚Äôs Somali American community. A robust mutual aid network emerged in response to ICE‚Äôs operation in the Twin Cities. In addition to collecting groceries and crowdfunding rent for immigrant families, Minnesotans have formed ICE patrols in their neighborhoods. Two observers, Renee Good and Alex Pretti, were killed by federal immigration agents in January, sparking mass protests throughout the Twin Cities. Administration officials had previously said that the operation would end once state and local officials sufficiently collaborated with ICE. It‚Äôs not immediately clear what concessions, if any, the federal government has obtained from Gov. Tim Walz or Minneapolis Mayor Tim Frey.",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/01/268278_After_Pretti_killed_SGarcia_0056.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "Spider-Noir looks like a hard-boiled thriller in first trailer",
      "url": "https://www.theverge.com/entertainment/877922/spider-noir-trailer-release-date-amazon-mgm-plus",
      "published": "2026-02-12T16:01:26+00:00",
      "summary": "Though it's going to be a while before we see more of Sony's animated Spider-Verse or Marvel's live-action take on Peter Parker, Amazon's Spider-Noir series is right around the corner. Today, Amazon dropped a new trailer for its upcoming Spider-Noir starring Nicolas Cage as a brooding Spider-Man variant. Set in an alternate reality where it's [&#8230;]",
      "content_text": "Though it‚Äôs going to be a while before we see more of Sony‚Äôs animated Spider-Verse or Marvel‚Äôs live-action take on Peter Parker , Amazon‚Äôs Spider-Noir series is right around the corner. Today, Amazon dropped a new trailer for its upcoming Spider-Noir starring Nicolas Cage as a brooding Spider-Man variant. Set in an alternate reality where it‚Äôs still the 1930s, Spider-Noir follows private investigator Ben Reilly (Cage) as he gets back into the crimefighting game as the hero known as ‚ÄúThe Spider.‚Äù As his city‚Äôs only superhero, Reilly knows that he‚Äôs the only person with the necessary skills to deal with threats like mob boss Silvermane (Brendan Gleeson). But as a man still reeling from a personal tragedy, it‚Äôs hard for Reilly to get back into the heroic swing of things. In addition to featuring shots of club owner Cat Hardy (Li Jun Li) and reporter Robbie Robertson (Lamorne Morris), the black and white trailer (there‚Äôs also a version that‚Äôs in color , and the series will be streamed both ways) gives you a strong sense of the moody tone and dark aesthetic Spider-Noir is going to lean into. For some reason, Amazon is debuting the show on MGM Plus on May 25th before it hits Prime Video on the 27th. It‚Äôs not clear if the show will feature any connections to the rest of Sony‚Äôs universe of Spider-projects, but Spider-Noir looks like it could be pretty solid.",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/Spider-Noir.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "Sony WF-1000XM6 earbuds",
      "url": "https://www.theverge.com/tech/877503/sony-wf-1000xm6-earbuds-review",
      "published": "2026-02-12T16:00:00+00:00",
      "summary": "Apple, Bose, and Sony are in a running battle for the best noise-canceling earbuds, and the Sony WF-1000XM6 put Sony back in first place - if you can get a good fit with the included foam tips. With each iteration of the XM series, Sony has aimed for - and delivered - incredible noise-canceling performance [&#8230;]",
      "content_text": "Apple, Bose, and Sony are in a running battle for the best noise-canceling earbuds, and the Sony WF-1000XM6 put Sony back in first place ‚Äî if you can get a good fit with the included foam tips. With each iteration of the XM series, Sony has aimed for ‚Äî and delivered ‚Äî incredible noise-canceling performance with a pleasing and balanced sound profile, and features that work equally well with iOS and Android. The XM6 push even further by adding additional noise-canceling mics (for a total of 8), a new chip, newly designed drivers, and a new shape that‚Äôs supposed to better fit a wider selection of ears. That‚Äôs a lot of promises, and the Sony WF-1000XM6 earbuds fulfill them all ‚Äî almost. $330 The Sony WF-1000XM6 earbuds have top-notch noise-canceling performance with excellent, balanced sound. Read More One of the things I‚Äôve liked most about both the in-ear and over-ear Sony XM series is the sound performance. Out of the box, the XM6 sound balanced and clean. They use a new driver with a softer edge material that allows for deeper bass, combined with a high-rigidity dome for clear high frequencies. I was impressed by the bass response, even in the lowest notes of ‚Äúbury a friend‚Äù by Billie Eilish, which sound resonant and punchy. Vocals, such as Jarvis Cocker‚Äôs on ‚ÄúCommon People,‚Äù cut through thick textures, and cymbals and hi-hats have a nice sparkle to them. There‚Äôs also more drive in the midrange than with the Apple AirPods Pro 3 or Bose Ultra Earbuds gen 2 ‚Äî both of which are balanced a bit more toward the high end, especially the Bose. The top noise-canceling earbuds ‚Äî the Bose Ultra Earbuds gen 2, Apple AirPods Pro 3, and these Sony WF-1000XM6 ‚Äî are all great at blocking low-end frequencies like the engine noise in an airplane cabin. They‚Äôre all good at tamping down midrange sounds, including voices, but the XM6 are better at tuning out other people‚Äôs conversations better than any other earbuds I‚Äôve heard. Sony improved the bone conduction sensor in the XM6, and in conjunction with AI beamforming mics, the earbuds do a great job blocking out environmental sounds while on calls. I tested both sides of a call with the XM6, and found that traffic can cause some swishy noises. And when there was a lot of extra traffic and wind, my voice would sound a bit compressed, but not to an extent that was distracting or caused me to be unintelligible. Price: $329.99 Battery life: 8 hours (24 including the case) Colors: Black, light gray Connectivity: Bluetooth, Auracast Audio codecs: SBC, AAC, LDAC Dust/water resistance: IPX4 Ear tips: XS, S, M, L Weight: 5.9 grams per bud, 45.9 grams charging case Sound quality and noise-canceling performance rely on a solid fit, and that‚Äôs where the XM6 start to have some problems. I usually wear medium-sized ear tips, but I had difficulty getting a secure, long-lasting fit in my right ear with the included foam tips. I had similar fit issues when I tested the Sony WF-1000XM5 earbuds, but was eventually able to get those to sit properly for long listening periods. The Sound Connect app includes a ‚ÄúTest wearing condition‚Äù setting that plays a test tone and uses internal microphones to determine if there‚Äôs an airtight fit. According to the app, I was only able to get a good seal in my right ear by using the large tip and jamming the earbud into my ear. After a short time I could feel the ear tip start to loosen. I had a couple friends try to fit the XM6 as well, and both had varying levels of issues. There is a solution. If none of the foam ear tips work for you after testing the fit with the app, an option to order silicone ear tips will show up and Sony will send them out, free of charge. The silicone tips solved my fit problem. Both earbuds felt more secure than with the foam tips, and the fit was more comfortable, although it does sound like the silicone ear tips let in a tiny bit more mid-range noise, voices in particular. It‚Äôs a pretty minor difference and the noise cancellation is still incredibly impressive ‚Äî still on par with the Bose Ultra gen 2 ‚Äî but it‚Äôs worth noting the best-in-class noise cancellation relies on the foam tips. The included foam ear tips help the Sony earbuds achieve exceptional noise canceling, but I had trouble getting them to fit my ears. Photo by Amelia Holowaty Krales / The Verge The low-end frequency response is also slightly different with the silicone tips, with a tad less bass. It‚Äôs such a minor difference, though, and isn‚Äôt objectionable at all, just slightly different. The tom-toms at the beginning of Radiohead‚Äôs ‚ÄúThere, There‚Äù from Hail to the Thief don‚Äôt quite ring the same, and the low bass notes aren‚Äôt as full. But if I had the ability to ABX test the two different tips (which would be impossible because I‚Äôd feel the difference), I don‚Äôt think I‚Äôd be able to point to one as better. The XM6 sound fantastic regardless of tips. Our ears are highly adaptable to changes in sound balance, and within seconds using either set of tips I was enjoying everything I listened to. Still, the fact that I needed an additional set of tips to get a suitable fit ‚Äî and that the foam tips didn‚Äôt work for multiple people that tried them ‚Äî isn‚Äôt great. Sony should have included the silicone tips in the box. While three people certainly don‚Äôt make a large enough sample size for a definitive conclusion, be aware you might need to wait for those extra silicone tips to arrive or plan on buying yourself some Comply foam tips . I‚Äôm also not a huge fan of the XM6‚Äôs new shape. The earbuds are longer and narrower than the WF-1000XM5, and the body sticks out further from the ear. They are certainly not inconspicuous, though size does make touch controls easy to use. Only once did I accidentally cause music to play or pause ‚Äî something that happened on numerous occasions with the Bose as I adjusted them in my ears during testing ‚Äî and that was when I was motioning to my colleague about the size of them in my ear. The XM6 (right) are a little slimmer than the XM5, but also longer. Photo by John Higgins / The Verge The XM6 case (right) is slightly taller than the XM5 case and has sharper edges. Photo by John Higgins / The Verge For those that like to tweak settings, there are a lot of opportunities in the Sound Connect app. There‚Äôs a 10-band EQ and five EQ presets (although I think the default sound profile is excellent and doesn‚Äôt need to be altered for my taste), a 20-point slider to adjust ambient passthrough (or an auto setting), and touch control customization. There‚Äôs also an option to set scenes, which causes the earbuds to adjust to predetermined settings based on your activity or location. And if you have a compatible device (including most modern Android phones), the XM6 support LDAC high-res audio streaming and Auracast. A new listening mode ‚Äî currently available on the in-ear and over-ear XM6 series, as well as the LinkBuds Fit and LinkBuds Open ‚Äî turns whatever you‚Äôre listening to into background music. It changes the volume and EQ and adds reverb to make the music in the earbuds sound like background music in a cafe, living room, or ‚Äúmy room‚Äù (all actual options in the settings). I could see it keeping the music from being too distracting, but for me turning the volume down is enough to give a similar effect. The XM6 have up to eight hours of battery life (24 hours including the case, which has wireless charging), which is comparable to the AirPods Pro 3, and a couple more hours in the buds than the Bose Ultra Earbuds gen 2 (although those also get up to 24 hours with their charging case). The XM6‚Äôs case provides an additional 16 hours of battery life and supports wireless charging. Photo by Amelia Holowaty Krales / The Verge Sony added two more mics (one per bud) than the XM5 for improved noise cancellation. Photo by Amelia Holowaty Krales / The Verge The Bose Ultra Earbuds gen 2 are longer than the XM6, but they don‚Äôt visually sit out of the ear as far. Photo by John Higgins / The Verge The AirPods Pro 3‚Äôs body is smaller than the Sony‚Äôs, but the stem makes them more noticeable (for better or worse). Photo by Amelia Holowaty Krales / The Verge The Sony WF-1000XM6 have solid performance, following the pedigree of earlier XM series earbuds. They sound fantastic, handle ambient noise during calls well and, with the foam ear tips, deliver the best noise canceling I‚Äôve yet heard from earbuds. They sound better than the Bose buds, even though they stick out a bit more. If you‚Äôre deep in the Apple ecosystem, nothing competes with the integration of the AirPods, but the XM6 are my favorite-sounding earbuds of the three, with broader compatibility than the AirPods and a nice array of features including Auracast support. Foam or silicone tips, the XM6 are excellent earbuds. It‚Äôs nice that Sony will send along extra silicone tips if the foam ones don‚Äôt work for you; I just wish they were included from the get-go. Follow topics and authors from this story to see more like this in your personalized homepage feed and to receive email updates. John Higgins Headphone Reviews Reviews Tech",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/268261_Sony_WF-1000XM6_earbuds_AKrales_0047.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "Waymo‚Äôs next-gen robotaxi is ready for passengers ‚Äî and also ‚Äòhigh-volume production‚Äô",
      "url": "https://www.theverge.com/transportation/877902/waymo-sixth-generation-robotaxi-ojai-hyundai-sensors-cost",
      "published": "2026-02-12T16:00:00+00:00",
      "summary": "After years of testing and validation, Waymo announced today that it's sixth-generation robotaxi technology is finally ready for passenger trips. The updated autonomous driving system will first be rolled out for employees and their friends in San Francisco and Los Angeles, with public customers to follow. Waymo's current fleet of Jaguar I-Pace vehicles runs on [&#8230;]",
      "content_text": "After years of testing and validation, Waymo announced today that it‚Äôs sixth-generation robotaxi technology is finally ready for passenger trips. The updated autonomous driving system will first be rolled out for employees and their friends in San Francisco and Los Angeles, with public customers to follow. Waymo‚Äôs current fleet of Jaguar I-Pace vehicles runs on the company‚Äôs fifth generation technology , first rolled out in March 2020. But that vehicle has reached the end of its shelf life, after Jaguar discontinued the model at the end of 2024. The updated system is designed to work seamlessly across multiple vehicle types , starting with the Zeekr RT minivan ( rebranded as Ojai ) and followed by the Hyundai Ioniq 5 . Waymo is in talks with other automakers, including Toyota , about future models. Waymo says that its sixth-generation system is the smartest, most capable autonomous vehicle it‚Äôs ever designed, while also using fewer sensors to lower its overall costs. Its cameras are more powerful, its lidar are able to see things the cameras might miss, and its improved radar are able to tackle extreme weather conditions. But more importantly, its built for ‚Äúhigh-volume production,‚Äù with Waymo‚Äôs manufacturing partners able to churn out ‚Äútens of thousands of units a year.‚Äù After proving that it can build a successful robotaxi business across multiple markets, Waymo is aiming to scale more rapidly, eyeing 20 new cities in 2026. ‚ÄúDesigned for long-term growth across multiple vehicle platforms, this system‚Äôs expanded capabilities allow us to safely broaden our footprint into more diverse environments, including those with extreme winter weather, at an even greater scale,‚Äù Waymo VP of engineering Satish Jeyachandran said in a blog post. Jeyachandran listed a number of metrics ‚Äî developed over seven years, 200 million miles of testing in 10+ major cities ‚Äî to bolster the case that the sixth generation is ready for the road. And in perhaps a veiled swipe at Tesla‚Äôs camera-only autonomous system, he explained how Waymo‚Äôs multi-sensor hardware stack provides the redundancy necessary to create the most robust picture of the environment around each vehicle, while also detecting even the hardest-to-spot objects and edge cases. ‚ÄúOur experience as the only company operating a fully autonomous service at this scale has reinforced a fundamental truth: demonstrably safe AI requires equally resilient inputs,‚Äù he said. ‚ÄúThis deep understanding of real-world requirements is why the Waymo Driver utilizes a custom, multi-modal sensing suite where high-resolution cameras, advanced imaging radar, and lidar work as a unified system.‚Äù But what about those sensors? The vision system runs on high-powered 17-megapixel cameras, which Waymo calls ‚Äúimagers,‚Äù capable of capturing ‚Äúmillions of data points for incredibly sharp images.‚Äù (By contrast, Tesla‚Äôs current Hardware 4 (HW4) vehicles use 5-megapixel Sony IMX963 cameras.) The incredible resolution allows Waymo to use fewer overall cameras: 16, down from 29 in the fifth-generation system. Those cameras are bolstered by strategically placed short-range lidar, which are also coming down in cost for the company. The lidar help with identifying vulnerable road users, like pedestrians or bicyclists, while also providing ‚Äúcentimeter-scale range accuracy,‚Äù Jeyachandran says. And they‚Äôve been reengineered to help penetrate extreme weather situations that may hamper even the highest resolution camera, which will become more important as Waymo aims to launch in snowier climates. Jeyachandran also touted more affordable radar sensors, and the new system‚Äôs external audio receivers, or EARs (see what they did there), that can detect audio inputs like approaching sirens or trains. But if you‚Äôre taking anything away here, it should be that Waymo thinks it can make a lot more of these vehicles at a lower cost than its previous robotaxis. That‚Äôs crucial as the company seeks to scale up its presence in the US and overseas, cementing its position as the dominant autonomous vehicle company in the world. Waymo has said it plans on adding only 2,000 more vehicles in 2026 , for a total fleet size of 3,500. But its ultimate aim is tens of thousands of vehicles, as per today‚Äôs announcement. Lowering costs is going to be increasingly important for robotaxi companies as they look to scale up and expand into new markets. Alphabet doesn‚Äôt break out Waymo‚Äôs costs in its earnings report, but its ‚ÄúOther Bets‚Äù unit, which includes the robotaxi company, brought in $370 million in revenue in the fourth quarter of 2025, down from $400 million a year ago. But the unit‚Äôs losses widened to $3.6 billion from $1.2 billion in the year-earlier period. Waymo recently raised $16 billion in its latest funding round as it aims to take its robotaxi business ‚Äúglobal.‚Äù Follow topics and authors from this story to see more like this in your personalized homepage feed and to receive email updates. Andrew J. Hawkins Autonomous Cars News Transportation Waymo",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/unnamed-1.jpg?quality=90&strip=all&crop=0%2C10.75214828329%2C100%2C78.49570343342&w=1200"
    },
    {
      "industry": "technology",
      "source": "VentureBeat",
      "title": "AI inference costs dropped up to 10x on Nvidia's Blackwell — but hardware is only half the equation",
      "url": "https://venturebeat.com/infrastructure/ai-inference-costs-dropped-up-to-10x-on-nvidias-blackwell-but-hardware-is",
      "published": "2026-02-12T16:00:00+00:00",
      "summary": "<p>Lowering the cost of inference is typically a combination of hardware and software. A new analysis released Thursday by Nvidia details how four leading inference providers are reporting 4x to 10x reductions in cost per token.</p><p>The dramatic cost reductions were achieved using Nvidia&#x27;s Blackwell platform with open-source models. Production deployment data from Baseten, DeepInfra, Fireworks AI and Together AI shows significant cost improvements across healthcare, gaming, agentic chat, and customer service as enterprises scale AI from pilot projects to millions of users.</p><p>The 4x to 10x cost reductions reported by inference providers required combining Blackwell hardware with two other elements: optimized software stacks and switching from proprietary to open-source models that now match frontier-level intelligence. Hardware improvements alone delivered 2x gains in some deployments, according to the analysis. Reaching larger cost reductions required adopting low-precision formats like NVFP4 and moving away from closed source APIs that charge premium rates.</p><p>The economics prove counterintuitive. Reducing inference costs requires investing in higher-performance infrastructure because throughput improvements translate directly into lower per-token costs.</p><p>&quot;Performance is what drives down the cost of inference,&quot; Dion Harris, senior director of HPC and AI hyperscaler solutions at Nvidia, told VentureBeat in an exclusive interview. &quot;What we&#x27;re seeing in inference is that throughput literally translates into real dollar value and driving down the cost.&quot;</p><h2>Production deployments show 4x to 10x cost reductions</h2><p>Nvidia detailed four customer deployments in a blog post showing how the combination of Blackwell infrastructure, optimized software stacks and open-source models delivers cost reductions across different industry workloads. The case studies span high-volume applications where inference economics directly determines business viability.</p><p>Sully.ai cut healthcare AI inference costs by 90% (a 10x reduction) while improving response times 65% by switching from proprietary models to open-source models running on Baseten&#x27;s Blackwell-powered platform, according to Nvidia. The company returned over 30 million minutes to physicians by automating medical coding and note-taking tasks that previously required manual data entry.</p><p>Nvidia also reported that Latitude reduced gaming inference costs 4x for its AI Dungeon platform by running large mixture-of-experts (MoE) models on DeepInfra&#x27;s Blackwell deployment. Cost per million tokens dropped from 20 cents on Nvidia&#x27;s previous Hopper platform to 10 cents on Blackwell, then to 5 cents after adopting Blackwell&#x27;s native NVFP4 low-precision format. Hardware alone delivered 2x improvement, but reaching 4x required the precision format change.</p><p>Sentient Foundation achieved 25% to 50% better cost efficiency for its agentic chat platform using Fireworks AI&#x27;s Blackwell-optimized inference stack, according to Nvidia. The platform orchestrates complex multi-agent workflows and processed 5.6 million queries in a single week during its viral launch while maintaining low latency.</p><p>Nvidia said Decagon saw 6x cost reduction per query for AI-powered voice customer support by running its multimodel stack on Together AI&#x27;s Blackwell infrastructure. Response times stayed under 400 milliseconds, even when processing thousands of tokens per query, critical for voice interactions where delays cause users to hang up or lose trust.</p><h2>Technical factors driving 4x versus 10x improvements</h2><p>The range from 4x to 10x cost reductions across deployments reflects different combinations of technical optimizations rather than just hardware differences. Three factors emerge as primary drivers: precision format adoption, model architecture choices, and software stack integration.</p><p><b>Precision formats show the",
      "content_text": "<p>Lowering the cost of inference is typically a combination of hardware and software. A new analysis released Thursday by Nvidia details how four leading inference providers are reporting 4x to 10x reductions in cost per token.</p><p>The dramatic cost reductions were achieved using Nvidia&#x27;s Blackwell platform with open-source models. Production deployment data from Baseten, DeepInfra, Fireworks AI and Together AI shows significant cost improvements across healthcare, gaming, agentic chat, and customer service as enterprises scale AI from pilot projects to millions of users.</p><p>The 4x to 10x cost reductions reported by inference providers required combining Blackwell hardware with two other elements: optimized software stacks and switching from proprietary to open-source models that now match frontier-level intelligence. Hardware improvements alone delivered 2x gains in some deployments, according to the analysis. Reaching larger cost reductions required adopting low-precision formats like NVFP4 and moving away from closed source APIs that charge premium rates.</p><p>The economics prove counterintuitive. Reducing inference costs requires investing in higher-performance infrastructure because throughput improvements translate directly into lower per-token costs.</p><p>&quot;Performance is what drives down the cost of inference,&quot; Dion Harris, senior director of HPC and AI hyperscaler solutions at Nvidia, told VentureBeat in an exclusive interview. &quot;What we&#x27;re seeing in inference is that throughput literally translates into real dollar value and driving down the cost.&quot;</p><h2>Production deployments show 4x to 10x cost reductions</h2><p>Nvidia detailed four customer deployments in a blog post showing how the combination of Blackwell infrastructure, optimized software stacks and open-source models delivers cost reductions across different industry workloads. The case studies span high-volume applications where inference economics directly determines business viability.</p><p>Sully.ai cut healthcare AI inference costs by 90% (a 10x reduction) while improving response times 65% by switching from proprietary models to open-source models running on Baseten&#x27;s Blackwell-powered platform, according to Nvidia. The company returned over 30 million minutes to physicians by automating medical coding and note-taking tasks that previously required manual data entry.</p><p>Nvidia also reported that Latitude reduced gaming inference costs 4x for its AI Dungeon platform by running large mixture-of-experts (MoE) models on DeepInfra&#x27;s Blackwell deployment. Cost per million tokens dropped from 20 cents on Nvidia&#x27;s previous Hopper platform to 10 cents on Blackwell, then to 5 cents after adopting Blackwell&#x27;s native NVFP4 low-precision format. Hardware alone delivered 2x improvement, but reaching 4x required the precision format change.</p><p>Sentient Foundation achieved 25% to 50% better cost efficiency for its agentic chat platform using Fireworks AI&#x27;s Blackwell-optimized inference stack, according to Nvidia. The platform orchestrates complex multi-agent workflows and processed 5.6 million queries in a single week during its viral launch while maintaining low latency.</p><p>Nvidia said Decagon saw 6x cost reduction per query for AI-powered voice customer support by running its multimodel stack on Together AI&#x27;s Blackwell infrastructure. Response times stayed under 400 milliseconds, even when processing thousands of tokens per query, critical for voice interactions where delays cause users to hang up or lose trust.</p><h2>Technical factors driving 4x versus 10x improvements</h2><p>The range from 4x to 10x cost reductions across deployments reflects different combinations of technical optimizations rather than just hardware differences. Three factors emerge as primary drivers: precision format adoption, model architecture choices, and software stack integration.</p><p><b>Precision formats show the clearest impact</b>. Latitude&#x27;s case demonstrates this directly. Moving from Hopper to Blackwell delivered 2x cost reduction through hardware improvements. Adopting NVFP4, Blackwell&#x27;s native low-precision format, doubled that improvement to 4x total. NVFP4 reduces the number of bits required to represent model weights and activations, allowing more computation per GPU cycle while maintaining accuracy. The format works particularly well for MoE models where only a subset of the model activates for each inference request.</p><p><b>Model architecture matters.</b> MoE models, which activate different specialized sub-models based on input, benefit from Blackwell&#x27;s NVLink fabric that enables rapid communication between experts. &quot;Having those experts communicate across that NVLink fabric allows you to reason very quickly,&quot; Harris said. Dense models that activate all parameters for every inference don&#x27;t leverage this architecture as effectively.</p><p><b>Software stack integration creates additional performance deltas</b>. Harris said that Nvidia&#x27;s co-design approach — where Blackwell hardware, NVL72 scale-up architecture, and software like Dynamo and TensorRT-LLM are optimized together — also makes a difference. Baseten&#x27;s deployment for Sully.ai used this integrated stack, combining NVFP4, TensorRT-LLM and Dynamo to achieve the 10x cost reduction. Providers running alternative frameworks like vLLM may see lower gains.</p><p><b>Workload characteristics matter</b>. Reasoning models show particular advantages on Blackwell because they generate significantly more tokens to reach better answers. The platform&#x27;s ability to process these extended token sequences efficiently through disaggregated serving, where context prefill and token generation are handled separately, makes reasoning workloads cost-effective.</p><p>Teams evaluating potential cost reductions should examine their workload profiles against these factors. High token generation workloads using mixture-of-experts models with the integrated Blackwell software stack will approach the 10x range. Lower token volumes using dense models on alternative frameworks will land closer to 4x. </p><h2>What teams should test before migrating</h2><p>While these case studies focus on Nvidia Blackwell deployments, enterprises have multiple paths to reducing inference costs. AMD&#x27;s MI300 series, Google TPUs, and specialized inference accelerators from Groq and Cerebras offer alternative architectures. Cloud providers also continue optimizing their inference services. The question isn&#x27;t whether Blackwell is the only option but whether the specific combination of hardware, software and models fits particular workload requirements.</p><p>Enterprises considering Blackwell-based inference should start by calculating whether their workloads justify infrastructure changes. </p><p>&quot;Enterprises need to work back from their workloads and use case and cost constraints,&quot; Shruti Koparkar, AI product marketing at Nvidia, told VentureBeat.</p><p>The deployments achieving 6x to 10x improvements all involved high-volume, latency-sensitive applications processing millions of requests monthly. Teams running lower volumes or applications with latency budgets exceeding one second should explore software optimization or model switching before considering infrastructure upgrades.</p><p><b>Testing matters more than provider specifications</b>. Koparkar emphasizes that providers publish throughput and latency metrics, but these represent ideal conditions. </p><p>&quot;If it&#x27;s a highly latency-sensitive workload, they might want to test a couple of providers and see who meets the minimum they need while keeping the cost down,&quot; she said. Teams should run actual production workloads across multiple Blackwell providers to measure real performance under their specific usage patterns and traffic spikes rather than relying on published benchmarks.</p><p><b>The staged approach Latitude used provides a model for evaluation</b>. The company first moved to Blackwell hardware and measured 2x improvement, then adopted NVFP4 format to reach 4x total reduction. Teams currently on Hopper or other infrastructure can test whether precision format changes and software optimization on existing hardware capture meaningful savings before committing to full infrastructure migrations. Running open source models on current infrastructure might deliver half the potential cost reduction without new hardware investments.</p><p><b>Provider selection requires understanding software stack differences</b>. While multiple providers offer Blackwell infrastructure, their software implementations vary. Some run Nvidia&#x27;s integrated stack using Dynamo and TensorRT-LLM, while others use frameworks like vLLM. Harris acknowledges performance deltas exist between these configurations. Teams should evaluate what each provider actually runs and how it matches their workload requirements rather than assuming all Blackwell deployments perform identically.</p><p><b>The economic equation extends beyond cost per token</b>. Specialized inference providers like Baseten, DeepInfra, Fireworks and Together offer optimized deployments but require managing additional vendor relationships. Managed services from AWS, Azure or Google Cloud may have higher per-token costs but lower operational complexity. Teams should calculate total cost including operational overhead, not just inference pricing, to determine which approach delivers better economics for their specific situation.</p>",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Lines of Code Are Back (And It's Worse Than Before)",
      "url": "https://www.thepragmaticcto.com/p/lines-of-code-are-back-and-its-worse",
      "published": "2026-02-12T15:35:45+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.thepragmaticcto.com/p/lines-of-code-are-back-and-its-worse\">https://www.thepragmaticcto.com/p/lines-of-code-are-back-and-its-worse</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46990103\">https://news.ycombinator.com/item?id=46990103</a></p> <p>Points: 9</p> <p># Comments: 0</p>",
      "content_text": "The software industry doesn't agree on much. Tabs versus spaces, monoliths versus microservices, whether stand-ups are useful or performance artâ€”pick a topic and you'll find engineers ready to die on both hills. But for about forty years, we had one consensus: lines of code is a terrible metric. Dijkstra called it \"a very costly measuring unit because it encourages the writing of insipid code.\" Lines are spent, not produced. Bill Gates compared measuring programming progress by lines of code to measuring aircraft building progress by weight. Ken Thompson said one of his most productive days was throwing away a thousand lines. In 2009, Tom DeMarco â€”the man who wrote \"you can't control what you can't measure\"â€”formally retracted the statement. Software projects, he concluded, are fundamentally experimental; the important goal is transformation, not control. By 2023, Kent Beck was calling LOC \"an input metric\" â€”the worst category. \"Only use it if you have nothing else to measure success with.\" That was the consensus. Settled. Done. Then AI showed up, and we brought it back. Every major tech CEO is now competing on what percentage of their code is written by AI. Watch the progression. Sundar Pichai told investors in October 2024 that 25% of Google's new code was AI-generated. By mid-2025, that number climbed past 30%. Satya Nadella said \"maybe 20%, 30%\" of Microsoft's code is now written by software. Mark Zuckerberg predicted AI would handle half of Meta's development within a year. And Dario Amodei predicted 90% of code would be AI-written within six months; when the deadline passed, he revised the claim to \"70, 80, 90% of the code written at Anthropic is written by Claude.\" Twenty-five percent. Thirty percent. Fifty percent. Ninety percent. The numbers only go up, and they're presented as achievementsâ€”on earnings calls, in press releases, at conferences. Nobody is reporting \"percentage of bugs introduced by AI-generated code\" or \"percentage of AI code that survived review unchanged.\" Nobody is mentioning how much of that generated code was thrown away, reworked, or never deployed. The headline metric is volume. LOC by another name. The tooling reinforces it. GitHub Copilot's dashboard shows \"Total Lines Suggested\" and \"Total Lines Accepted\" as primary metrics. Cursor tracks lines added per user, reporting a 28.6% increase following adoption. The industry generated 256 billion lines of AI-written code in 2024 alone. That number is treated as progress. And it's not just executives. The LOC obsession has filtered into social media culture. A viral tweet last weekâ€”1.6 million viewsâ€”celebrated Anthropic's AI agents building a C compiler: \"100k lines, compiles the Linux kernel, $20k, 2 weeks.\" A Community Note corrected the framing; GCC took about two years to build from conception, not thirty-seven. But the correction didn't go viral. The line count did. One developer compared his AI agents' outputâ€”3.2 million lines of code in three monthsâ€”to his lifetime achievement of 700,000 lines across sixty years. Then he used Grok to generate an argument for why LOC is a valid metric. Using AI to justify the metric that AI makes meaningless. You can't make this up. I asked a simple question on X last week: \"Why are people in the AI space so obsessed with lines of code?\" The question got 10,000 views. This article is my answer. You know Goodhart's Law : when a measure becomes a target, it ceases to be a good measure. LOC was a textbook case of this before AI entered the picture. Developers rewarded for adding lines wrote verbose code; teams measured by output shipped bloat instead of solutions. The industry recognized the problem, and for the most part, we stopped using LOC as a productivity metric. AI didn't just repeat the mistake. It broke the mistake open. Think about it in three layers. Layer one: LOC failed as a human metric because it was gameable. Developers rewarded for adding lines could write verbose code to hit targets. Managers knew this. The industry spent decades documenting the problem. We moved on. Layer two: AI makes the metric infinitely gameable. When a human developer games LOC, there's friction. Writing unnecessary code takes effort; the gaming has a natural ceiling because a person can only type so fast and only tolerate so much tedium. Remove those limits. An AI can produce ten thousand lines in the time a developer writes fifty. The cost of generating a line of code is now functionally zero. If LOC was misleading when it cost effort to produce, it is meaningless when it costs nothing. Layer three: we are applying Goodhart's Law to Goodhart's Law. The metric that was already broken is now the target for a system with infinite capacity to game it. The constraint that kept a bad metric merely bad has been eliminated; what's left is a metric that measures nothing at all. We're not repeating a forty-year-old mistake. We're running it with the guardrails removed. Andrej Karpathy coined the term \"vibe coding\" in February 2025â€”\"forget that the code even exists.\" When code generation requires zero comprehension, measuring code volume measures zero comprehension. Greptile's data shows lines per developer grew 76%, from 4,450 to 7,839. More output. Not more understanding. The question every CTO should be asking: if the cost of generating code is zero, what does the volume of generated code tell you? The answer is nothing. It tells you nothing. The data on what happens when you optimize for volume is already in. The numbers are not encouraging. GitClear analyzed 211 million lines of code across private repos and 25 major open-source projects from 2020 to 2024. Copy-pasted code rose from 8.3% to 12.3%. Code blocks with five or more duplicated lines increased eightfold during 2024. Refactoring collapsedâ€”the percentage of moved, restructured lines dropped from 24.1% in 2020 to 9.5% in 2024. A 60% decline. And code churn doubled : new code revised within two weeks of commit grew from 3.1% to 5.7%. Read that again: 2024 was the first year in GitClear's dataset where copy-pasted lines exceeded moved lines. The industry crossed a threshold. We are now generating more duplicate code than we are refactoring existing code. That is the cost of optimizing for volume. The productivity numbers are worse than the quality numbers. METR ran a randomized controlled trial â€”sixteen experienced open-source developers, 246 tasks on well-known repositories. Developers using AI tools took 19% longer to complete their work. But they believed they were 20% faster. A 40-point perception gap between what developers think AI does for them and what it measurably does. The Stack Overflow 2025 Developer Survey reinforces this. Trust in AI accuracy fell from 40% to 29% year over year. More developers actively distrust AI tools (46%) than trust them (33%). And 66% say they spend more time fixing \"almost-right\" AI-generated code than they save in the initial writing phase. On the security side, 45% of AI-generated code contains security flaws according to Veracode's 2025 report. Vibe-coded applications are already failing in production; one high-profile exercise saw AI ignore a code freeze, fabricate data, and delete a production database . A Swedish vibe-coding platform shipped 170 apps with exploitable vulnerabilities out of 1,645 tested. More code. Worse code. Less understood code. And we're measuring the \"more\" as if it were a feature. The industry recognized that raw LOC was indefensible, so it found a replacement: acceptance rate. The percentage of AI-suggested code that developers accept. This is the metric on most engineering leaders' dashboards today. It suffers from every flaw LOC had, plus new ones. Accepting code doesn't mean it's good code . A developer might accept a suggestion because it's close enough, because they're tired of rejecting and rewriting, because the context-switching cost of evaluating each suggestion exceeds the cost of just taking it. Acceptance rate conflates \"not rejected\" with \"valuable\"â€”and those are not the same thing. As CodeRabbit put it : \"Most tooling gives you vanity metrics like lines of code generated and number of AI completions accepted, which tell you nothing about what happens after the AI writes code.\" The metric ends at the moment of acceptance. It says nothing about whether the code worked, whether it introduced bugs, whether someone understood it, whether it survived the next refactor. The pattern keeps repeating. Lines of code, function points, story points, velocity, acceptance rateâ€”each generation of metric gets critiqued by its own advocates, discarded, and replaced with something that measures the same wrong thing in a new wrapper. We keep looking for a number that captures developer productivity in a single figure, and we keep finding that no such number exists. Sixty percent of engineering leaders cite a lack of clear metrics as their biggest AI challenge. They know the current metrics are broken. They just don't know what to replace them with. LOC is not always meaningless. Stating otherwise would be dishonest. As a rough sizing metricâ€”not a productivity metricâ€”lines of code can help estimate project scope. Tracking codebase growth over time can signal maintainability concerns before they become crises. At the aggregate level, LOC trends reveal how work is changing across the industry; Greptile's reports use LOC data to show real patterns in how developers interact with AI tools. And as an adoption metricâ€”how much AI-generated code is entering your codebaseâ€”LOC indicates tool usage levels, even if it says nothing about value delivered. AI coding tools are also not the problem. The problem is how we measure them. Salvatore Sanfilippoâ€” antirez , the creator of Redisâ€”makes a compelling case that AI genuinely enables building things faster when you know what to build. He created a pure C library for BERT-like embedding models in five minutes: 700 lines of code with output comparable to PyTorch. The value was in his decades of knowing what to build; the AI handled the typing. That's a legitimate productivity gain. MIT Technology Review named generative coding one of ten Breakthrough Technologies for 2026. The recognition is deserved. These tools are useful for boilerplate, for exploring unfamiliar APIs, for rubber-ducking problems, for rapid prototyping. I use them. Most CTOs I know use them. The argument is not that AI coding tools are bad. The argument is that measuring their value by counting the code they produce is like measuring a surgeon's skill by how many incisions they make. More incisions is not better surgery. More code is not better software. The metric rewards the wrong thing. If LOC and acceptance rate are broken, the obvious question is: what should replace them? The answer requires a fundamental shift in what you're looking at. Stop measuring inputsâ€”lines generated, suggestions accepted, percentage of code from AI. Start measuring outcomesâ€”what happened to the software and the team after the code was written. This is harder. It requires more instrumentation, more judgment, more patience. It also measures something worth knowing. Four metrics survive Goodhart's Law because they're hard to game and they measure what matters. Time-to-value. Not \"how fast did we write code\" but \"how long from identified need to working feature in production?\" AI should compress this timeline. If it doesn't, the code volume is noise. This is the metric your board cares about even if they don't know the name for it; it maps directly to customer impact and revenue. When a CEO asks \"what is AI doing for us,\" the answer should be a time-to-value number, not a line count. Code half-life. How long does new code survive before it needs revision? GitClear's churn data shows AI code gets revised fasterâ€”new code rewritten within two weeks nearly doubled from 2020 to 2024. Healthy code has a long half-life. Code that gets rewritten in fourteen days was never finished. Track this by origin; if AI-generated code has a shorter half-life than human-written code, that tells you something LOC never will. Defect origin rate. What percentage of production defects trace back to AI-generated code versus human-written code? Not as a blame metricâ€”as a calibration metric. If AI-generated code introduces defects at a higher rate, you need more review, not less AI. Track the ratio; adjust your process accordingly. Comprehension coverage. Can someone on the team explain how every critical path in the system works? This is the metric nobody tracks and everybody should. If the answer is \"the AI wrote that and nobody reviewed the logic,\" you have a time bomb. Vibe coding makes this worse by design; Karpathy's own framing was to \"forget that the code even exists.\" Code that nobody understands is code that nobody can debug, extend, or secure. The meta-principle: good metrics measure what happened after the code was written. Bad metrics measure what happened during writing. LOC, acceptance rate, lines suggestedâ€”all measure the act of creation. Time-to-value, code half-life, defect origin, comprehension coverageâ€”all measure the result. The act of writing code has never been the bottleneck; understanding, design, and judgment are the bottleneck. Measure accordingly. Currently, I'm doing what I have always done at Demac, at Humi and now at LiORA; we are tracking time to value, customer impact and customer trust. We are not measuring the volume of code we are generating, it is not a meaningful signal. Building the right things, at the right pace, with the right quality, is the key to success; of any startup, or any business. This metrics are harder to measure than counting lines; that's the point. If a metric is easy to collect, it probably measures inputs. The useful metrics require you to follow the code past the point of creation and into production. We use AI tools throughout the engineering org, mostly to assist reviewing code rather than writing it, helping increase our time to value. I might be wrong about some of this. Maybe the industry will figure out how to make volume a meaningful signal. But I'd rather measure the hard things poorly than measure the easy things precisely; at least the hard things point in the right direction. And I'd rather explain to my board why our metrics are nuanced than explain why we shipped code nobody understands. When your board asks what percentage of code is AI-generated, what are they asking? And is the answer you're giving them what they need to hear? If your AI tools disappeared tomorrow, would your team ship slowerâ€”or just write less code? What percentage of your codebase can someone on your team explain from memory? Is that number going up or down? The bottleneck in software was never typing speed. It was understanding, design, and judgment. LOC measured the wrong thing when humans wrote code. It measures even less now that machines do. The question for every CTO is not \"how much code are we generating?\" It is \"how much of that code should exist at all?\"",
      "cover_image_url": "https://substackcdn.com/image/fetch/$s_!wSs2!,w_1200,h_675,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf5ea19e-5c41-414c-8d25-be0736089290_1536x1024.jpeg"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "Apple acquires all rights to 'Severance,' will produce future seasons in-house",
      "url": "https://techcrunch.com/2026/02/12/apple-acquires-all-rights-to-severance-will-produce-future-seasons-in-house/",
      "published": "2026-02-12T15:34:11+00:00",
      "summary": "The show is expected to run for four seasons, with the possibility of spin-offs, a prequel, and foreign versions.",
      "content_text": "Apple has acquired the IP and all rights to its hit show “Severance” from its original studio, Fifth Season, as first reported by Deadline . Under the deal, which was worth just under $70 million, Apple’s in-house studio will be producing the show’s future seasons. Fifth Season will remain as an executive producer. The deal is similar to one Apple struck with AMC Studios for sci-fi show “Silo” after season one. “Severance” will now be one of Apple’s marquee titles, joining popular series such as “Your Friends and Neighbors” and “Stick.” According to Deadline, the production costs for “Severance” had exceeded what Fifth Season could afford. The studio had already requested advances from Apple and was considering relocating production from New York to Canada for bigger and quicker tax rebates. Since Apple definitely has the funds to back the show, the company decided to take full control. “Severance” is important to the streamer, as the show’s second season became Apple’s most-watched series at the time. It also had the highest number of nominations at the 2025 Emmy Awards. Deadline reports that the show is expected to run for four seasons, with the possibility of spin-offs, a prequel, and foreign versions. Techcrunch event Boston, MA | June 23, 2026",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2026/02/Severance_Photo_020401.jpg.photo_modal_show_home_large.jpg?resize=1200,675"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "ByteDance‚Äôs next-gen AI model can generate clips based on text, images, audio, and video",
      "url": "https://www.theverge.com/ai-artificial-intelligence/877931/bytedance-seedance-2-video-generator-ai-launch",
      "published": "2026-02-12T15:26:00+00:00",
      "summary": "Big Tech's race to leapfrog the latest AI models continues with the launch of ByteDance's next-gen video generator. In a blog post, ByteDance - the China-based company behind TikTok - says Seedance 2.0 supports prompts that combine text, images, video, and audio. The company claims it \"delivers a substantial leap in generation quality,\" offering improvements [&#8230;]",
      "content_text": "Big Tech‚Äôs race to leapfrog the latest AI models continues with the launch of ByteDance‚Äôs next-gen video generator. In a blog post , ByteDance ‚Äì the China-based company behind TikTok ‚Äì says Seedance 2.0 supports prompts that combine text, images, video, and audio. The company claims it ‚Äúdelivers a substantial leap in generation quality,‚Äù offering improvements in generating complex scenes with multiple subjects and its ability to follow instructions. Users can refine their text prompts by feeding Seedance 2.0 up to nine images, three video clips, and three audio clips. The model can generate up to 15-second clips with audio, while taking camera movement, visual effects, and motion into account. It can also reference text-based storyboards, according to ByteDance. AI-powered video generation models have only gotten more advanced within the past year, with Google Veo 3 adding the ability to generate audio-supported clips , and OpenAI launching Sora 2 along with a new app that allows users to create videos with ‚Äúhyperreal motion and sound.‚Äù The AI startup, Runway, has also released a new version of its AI video model that it claims has ‚Äúunprecedented‚Äù accuracy. In one example shared by ByteDance, which shows two figure skaters performing a routine together, the company says Seedance 2.0 can ‚Äúreliably perform a sequence of high-difficulty movements ‚Äî including synchronized takeoffs, mid-air spins and precise ice landings ‚Äî while strictly following real-world physical laws.‚Äù Users on social media have already started showing off what the new tool can do, with one person posting an AI-generated video with the likenesses of Brad Pitt and Tom Cruise in a cinematic fight sequence. Deadpool writer Rhett Reese reposted the video with the comment, ‚ÄúI hate to say it. It‚Äôs likely over for us.‚Äù Other posts demonstrate Seedance 2.0‚Äôs ability to generate anime-style clips, cartoons , cinematic sci-fi scenes , and videos that look like a content creator made them. It‚Äôs not clear what (if any) copyright protections Seedance 2.0 offers, as a quick search on X reveals a bunch of clips featuring characters from Dragon Ball Z , Family Guy , Pok√©mon , and more. For now, Seedance 2.0 is only available through ByteDance‚Äôs Dreamina AI platform and through its AI assistant, Doubao. It‚Äôs unclear whether it will make its way to TikTok ‚Äî especially now that the app is under new ownership in the US.",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/seedance-clip.png?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "a brief history of barbed wire fence telephone networks",
      "url": "https://loriemerson.net/2024/08/31/a-brief-history-of-barbed-wire-fence-telephone-networks/",
      "published": "2026-02-12T14:56:18+00:00",
      "summary": "<p>Article URL: <a href=\"https://loriemerson.net/2024/08/31/a-brief-history-of-barbed-wire-fence-telephone-networks/\">https://loriemerson.net/2024/08/31/a-brief-history-of-barbed-wire-fence-telephone-networks/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46989605\">https://news.ycombinator.com/item?id=46989605</a></p> <p>Points: 24</p> <p># Comments: 10</p>",
      "content_text": "If you look at the table of contents for my book, Other Networks: A Radical Technology Sourcebook , you’ll see that entries on networks before/outside the internet are arranged first by underlying infrastructure and then chronologically. You’ll also notice that within the section on wired networks, there are two sub-sections: one for electrical wire and another for barbed wire. Even though the barbed wire section is quite short, it was one of the most fascinating to research and write about – mostly because the history of using barbed wire to communicate is surprisingly long and almost entirely undocumented, even though barbed wire fence phones in particular were an essential part of early- to mid-twentieth century rural life in many parts of the U.S. and Canada! While I was researching barbed wire fence phones and wondering whether any artists had been intrepid enough to experiment with this other network, I came across Phil Peters and David Rueter ‘s work “Barbed Wire Fence Telephone” which they installed in a Chicago gallery in 2015. libi striegl (Managing Director of the Media Archaeology Lab through which we run many of our Other Networks projects) and I decided we should see if we can get Peters and Rueter to re-install their barbed wire fence telephone on the CU Boulder campus…to our delight and surprise, they said yes. But even more delightful and surprising was the fact that the college I’m now based in, the College of Media, Communication, and Information (CMCI), was enthusiastically supportive of our ask to install this fence phone network in a university classroom! In fact, not only was CMCI supportive in principle, they helped fund the project and staff members even helped us drill holes, put up fence posts, and string barbed wire. Phil and libi (with modest assistance from me) wrapped up the installation of “ Barbed Wire Fence Telephone II ” on Thursday August 29th and on Friday August 30th Phil gave a group of about 20 people a hands-on demo of this ad hoc network. Since so little documentation exists online about the history of this important communication network, below I include the introduction I wrote for the section on barbed wire along with the entry on barbed wire fence phones. I admit I hope someone adds this information to Wikipedia and cites either this post or Other Networks: A Radical Technology Sourcebook (forthcoming in 2025 by Anthology Editions). *** Barbed Wire Networks Barbed wire was originally proposed as an inexpensive and potentially painful material that could be used to create a fence and thus act as a deterrent to keep livestock within a confined area and/or to keep out intruders. Alan Krell documents numerous designs for wire that featured barbs throughout the 19th century, including one proposed by French inventor Léonce Eugène Grassin-Baledans in 1860 for a “Grating of wire-work for fences and other purposes.” The first patent in the U.S. for a wire fence featuring barbs was given to Lucien B. Smith from Kent, Ohio (U.S.) in 1867. Illinois farmer Joseph Glidden submitted a patent for an improved version of barbed wire in 1874 which has since become the dominant design. As Reviel Netz puts it, after this point the physical control of wide open spaces was largely complete. Many farmers objected to the cruelty built into barbed wire, the way in which the fencing meant cattle drives were no longer possible, and the way it marked the end of seemingly free and open public land; notably they formed anti-barbed-wire associations and pleaded with legislators and government officials to enact laws limiting or regulating the use of the wire. Nonetheless, as the price of wire fell from twenty cents per pound in 1874 to two cents a pound by 1893, few ranchers could afford any other type of fencing material. By the 1890s, the barbed wire industry had become wealthy enough and powerful enough that they effectively quelled all opposition to the wire. The availability of inexpensive barbed wire, especially across the western U.S. in the late 19th century, largely made it possible to keep larger herds of livestock than had been possible up to that point. It also played a significant role in “settling” the American west by violently asserting individual ownership over land that was already occupied by Native Americans. Appropriately nicknamed ‘the devil’s rope,’ barbed wire is made from steel (later coated in zinc, a zinc-aluminum alloy, or a kind of polymer coating such as polyvinyl chloride) and single or double barbs placed roughly four to six inches apart. To erect a fence, one only needs barbed wire, posts, and materials to afix the wire to the posts. Finally, although this section focuses on its use as a cooperative, non-commercial form of telecommunications network, it is also worth noting the frequent use barbed wire for trench warfare or as a security measure atop walls or buildings. Sources: Alan Krell, The Devil’s Rope: A Cultural History of Barbed Wire (Reaktion Books, 2002); Léonce Eugène Grassin-Baledans, “Grating of wire-work for fences and other purposes,” France Patent 45827; Lucien B. Smith, “Wire Fence,” US Patent 66182A (25 June 1867); Joseph Glidden, “Improvement in Wire Fences,” US Patent 157124A (27 OCtober 1873); Reviel Netz, Barbed Wire: An Ecology of Modernity (Wesleyan University Press, 2009) 53. Fence Phones Country of Origin: U.S.A. Creator(s): unknown Earliest Known Use: roughly 1893 Basic Materials: copper wire, barbed wire, posts, fasteners (such as nails or staples), insulators (such as porcelain knobs, glass bottles, leather, corn cobs, cow horns), battery-powered telephone handsets Description: A fence phone, also referred to as a barbed wire fence phone or squirrel lines, is the use of “smooth” (presumably copper) wire running from a house to nearby barbed wire fencing to create an informal, ad hoc, cooperative, non-commercial, local telephone network. Two key developments in the 1890s led to its adoption primarily by farmers, ranchers, and those living in rural or isolated areas especially in the U.S. and Canada: the widespread availability and inexpensiveness of barbed wire in the 1890s; and the erosion of Alexander Graham Bell’s patent monopoly in 1893 and 1894 which, according to Robert MacDougall, led to the sudden explosion of 80 to 90 independent telephone companies manufacturing telephone sets that could be used outside of the burgeoning Bell telephone system. According to Ronald Kline, the sudden explosion of independent telphone companies in turn set into motion the independent telephone movement. Not only had Bell largely neglected to provide those in rural areas with telephone service in favor of focusing on those in urban areas, but early Bell telephone owners were also intent on controlling telephone usage. Writes MacDougall, “Bell’s early managers sought to limit frivolous telephoning, especially undignified activities like courting or gossiping over the telephone, and to control certain groups of users, like women, children, and servants, who were thought to be particular offenders.” By contrast, according to Kline, the independent telephone companies recognized it would be too expensive to build lines in rural areas and they instead openly “advised farm people to buy their own telephone equipment, build their own lines, and create cooperatives to bring phones to the countryside.” In need of a practical way to overcome social isolation; communicate emergencies, weather, and crop prices; and chafing under attempts to curtail free speech, ranchers and farmers began to take advantage of the growing ubiquity of both telephone sets and barbed wire fencing. They would hook up telephones to wire strung from their homes to a nearby fence; at the time, telephones had their own battery which produced a DC current that could carry a voice signal; turning a crank on the phone would generate an AC current to produce a ring at the end of the line. Bob Holmes elaborates on the process: “the barbed wire networks had no central exchange, no operators–and no monthly bill. Instead of ringing through the exchange to a single address, every call made every phone on the system ring. Soon each household had its own personal ringtone…but anyone could pick up…Talk was free, and so people soon began to ‘hang out’ on the phone.” The fence phone lines could also be used to broadcast urgent information to everyone on the line. Reportedly, the quality of the signal traveling over the heavy wire was excellent, but weather would frequently cause short circuits which locals attempted to fix with anything that could serve as an insulator (such as leather straps, corn cobs, cow horns, or glass bottles). from “A CHEAP TELEPHONE SYSTEM FOR FARMERS”, Scientific American 82:13 (MARCH 31, 1900), p. 196 There are newspaper reports of ranchers and farmers using fence phones in U.S. states such as California, Texas, New Mexico, Colorado, Kansas, Iowa, Nebraska, Indiana, Minnesota, Ohio, Pennsylvania, New York, Montana, South Dakota and also parts of Canada. For example, a 1902 issue of the Chicago-based magazine Telephony reported on a barbed wire fence telephone network that operated between Broomfield and Golden, Colorado (U.S.A.) over a distance of 25 miles and which cost roughly $10 to build. The line was used for a “woman operator” to notify a worker at the end of the line “when to send down a head of water and how much.” The author notes one “peculiar feature of this system is that only the operator can begin the talk. When it is decided to send down water the operator calls up the man at the headgate and gives him specific instructions, which he must follow. If he has anything to say he must say it then or hold his peace till he is called up again, for it is not a circuit system and only the Broomfield office can call up. This gives the lady the advantage of being able to shut off the other fellow at will and of getting in the last word.” The fence phone systems also seemed to thrive in areas known for having cooperatives, especially related to farming. The model of a cooperative network particularly thrived throughout the 1920s as farmers experienced economic depression some years before the Great Depression. For example, according to David Sicilia, farmers in Montana created the Montana East Line Telephone Association to which they each contributed $25 plus several dollars a year for maintenance along with telephone sets, batteries, wire, and insulators. Anecdotally, fence phones were still being used throughout the 1970s and perhaps even later. C.F. Eckhardt describes calling his parents who lived in rural Texas and still used a fence phone; their number was simply 37, designated on the small local network by three long rings and one short ring. Sources: Alan Krell, The Devil’s Rope: A Cultural History of Barbed Wire (Reaktion Books, 2002); David B. Sicilia, “How the West Was Wired,” Inc.com (15 June 1997); Early W. Hayter, Free Range and Fencing , Vol. 3 (Kansas State Teachers College of Emporia Department of English, 1960); Robert MacDougall, The People’s Network: The Political Economy of the Telephone in the Gilded Age (University of Pennsylvania Press, 2014); Ronald Kline, Consumers in the Country: Technology and Social Change in Rural America (Johns Hopkins University Press, 2002); “A CHEAP TELEPHONE SYSTEM FOR FARMERS,” Scientific American , 82:13 (31 March 1900); “Bloomfield’s Barbed Wire System,” Telephony: An Illustrated Monthly Telephone Journal 4:6 (December 1902); Bob Holmes, “Wired Wild West: Cowpokes chatted on fence-wire phones,” New Scientist (17 December 2013); C. F. Eckhardt, “Before Maw Bell: Rural Telephone Systems in the West,” Texasescapes.com (2008); Phil Peters, “Barbed Wire Fence Telephone,” https://philipbpeters.com/ (2014)",
      "cover_image_url": "https://i0.wp.com/loriemerson.net/wp-content/uploads/2024/08/img_2201.jpg?fit=1200%2C900&ssl=1"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "US FTC airs concerns over allegations that Apple News suppresses right-wing content",
      "url": "https://techcrunch.com/2026/02/12/us-ftc-airs-concerns-over-allegations-that-apple-suppresses-right-wing-content-on-apple-news/",
      "published": "2026-02-12T14:40:03+00:00",
      "summary": "In a letter to Apple CEO Tim Cook, FTC chair Andrew Ferguson cited reports from Media Research Center, a right-leaning think tank, which accused Apple of excluding right-leaning outlets from the top 20 articles in the Apple News feed.",
      "content_text": "The U.S. Federal Trade Commission (FTC) has raised concerns over allegations that Apple is censoring conservative content on the Apple News app. In a letter to Apple CEO Tim Cook, FTC chair Andrew Ferguson cited reports from Media Research Center, a right-leaning think tank, which accused Apple of excluding right-leaning outlets from the top 20 articles in the Apple News feed. “These reports raise serious questions about whether Apple News is acting in accordance with its terms of service and its representations to consumers […] I abhor and condemn any attempt to censor content for ideological reasons,” Ferguson’s letter reads. Ferguson, a Big Tech critic who Trump appointed to lead the competition regulator, noted the FTC doesn’t have any powers to require Apple to take ideological or political positions when curating news, but he said that if the company’s practices are “inconsistent” its terms of service or “reasonable expectations of consumers,” they may be in violation of the FTC Act. Brendan Carr, the chairman of the Federal Communications Commission (another Trump appointee critical of Big Tech), supported Ferguson’s stance, writing, “Apple has no right to suppress conservative viewpoints in violation of the FTC Act.” Ferguson has urged Apple to conduct a “comprehensive review” of its terms of service and ensure that the content curated on Apple News is consistent with its policies, and “take corrective action swiftly” if the curation isn’t in line. The letter comes a day after President Donald Trump shared the report by Media Research Center on his social media platform, Truth Social. Trump has repeatedly accused Big Tech companies of censoring right-leaning content, though many major platforms have rolled back several measures to curb fake news and disinformation they had imposed in the years prior to his second stint at the White House. Techcrunch event Boston, MA | June 23, 2026 Apple’s relationship with the Trump administration has oscillated between warm and cold over the past year. Trump has criticized Big Tech, especially Apple, for manufacturing its devices in China, but after Cook promised to spend more than $600 billion over the next four years stateside and moved to mend fences, relations between the administration and the company have improved . Apple also dodged planned tariffs on smartphones made overseas and imported into the U.S. The FTC last year also launched an investigation into “censorship by tech platforms,” seeking input from the public who felt they were silenced due to their political ideologies or affiliations. “Tech firms should not be bullying their users,” Ferguson said at the time. “This inquiry will help the FTC better understand how these firms may have violated the law by silencing and intimidating Americans for speaking their minds.” Apple did not immediately return a request for comment.",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2193735641.jpg?w=1024"
    },
    {
      "industry": "technology",
      "source": "Wired",
      "title": "Elon Musk’s X Appears to Be Violating US Sanctions by Selling Premium Accounts to Iranian Leaders",
      "url": "https://www.wired.com/story/elon-musk-x-premium-accounts-iran/",
      "published": "2026-02-12T14:27:09+00:00",
      "summary": "While publicly supporting protesters in Iran, Elon Musk’s X appears to have been selling premium accounts to regime officials. Check marks were removed from certain accounts after a WIRED inquiry.",
      "content_text": "In recent weeks, Elon Musk has followed president Donald Trump’s lead, slamming Iranian government officials and supporting the thousands of protesters railing against the regime. He even provided free access to his Starlink satellites in the midst of a nationwide internet blackout . But while publicly proclaiming his support of the protesters, Musk’s company X appears to be profiting from the very same government officials he railed against, potentially violating US sanctions in the process, according to a new report from the Tech Transparency Project (TTP) shared exclusively with WIRED. TTP identified more than two dozen X accounts allegedly run by Iranian government officials, state agencies, and state-run news outlets, which display a blue check mark, indicating they have access to X’s premium service. These accounts were sharing state-sponsored propaganda at a time when ordinary Iranians had no access to the internet, and their messages appeared to be artificially boosted to increase reach and engagement, which is a key aspect of X’s premium service . An X Premium subscription, which is the only way to receive a blue check mark, costs $8 a month, while a Premium+ subscription, which removes ads and boosts reach even further, costs $40 a month. At a time when the Trump administration is threatening Iran with possible military action if it does not meet demands related to nuclear enrichment and ballistic missiles, X appears to be undermining those efforts by providing a social media bullhorn for the Iranian government to spread its message. “The fact that Elon Musk is not just platforming these individuals, but taking their money to boost their content through these premium subscriptions and give them extra features also means he's undermining the sanctions that the US and the Trump administration are actually applying,” Katie Paul, the director of the TTP, tells WIRED. X did not respond to a request for comment, but within hours of WIRED flagging several X accounts belonging to Iranian officials, their blue check marks were removed. The rest of the accounts identified by TTP but not shared with X continue to display a blue check mark. The White House directed WIRED to the Treasury when asked for comment. A Treasury spokesperson said they do not comment on specific allegations but that it “take[s] allegations of sanctionable conduct extremely seriously.” Protests broke out in the Iranian capital of Tehran on December 28 over the continuing devaluation of the Iranian rial against the dollar and a widespread economic crisis in the country. Over the following days, tens of thousands of protesters poured onto the streets in cities across the country, calling for regime change and the end of Supreme Leader Ayatollah Ali Khamenei’s 37-year reign. In response, the regime brutally cracked down on protesters, arresting tens of thousands of people and killing thousands more. The true death toll is still unknown but could be much higher than currently reported. Trump signaled his support for the protesters in a post on Truth Social on January 2, promising to come to their rescue. “We are locked and loaded and ready to go,\" he wrote. Musk quickly followed Trump, calling Khamenei “delusional.” On January 5, Gholamhossein Mohseni-Ejei, the head of Iran’s judiciary, who had a blue check mark at the time, wrote in a post on X, “This time, we will show no mercy to the rioters.” Ejei was among the accounts whose blue check marks were removed on Wednesday after WIRED contacted the company. A few days later, X changed the Iranian flag emoji on the platform to one used before the 1979 revolution, featuring a lion and sun. On January 14, Musk announced that anyone with a Starlink device would be free to access the internet in Iran without a subscription. At the time, Starlink devices were the only viable way of getting online after the government imposed a near-total internet blackout.",
      "cover_image_url": "https://media.wired.com/photos/698b604825bc2ca5f5fb4d53/191:100/w_1280,c_limit/Elon-Double-Dipping-Iran-Support-Politics-2246892016.jpg"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Email is tough: Major European Payment Processor's Emails rejected by GWorkspace",
      "url": "https://atha.io/blog/2026-02-12-viva",
      "published": "2026-02-12T14:24:15+00:00",
      "summary": "<p>Article URL: <a href=\"https://atha.io/blog/2026-02-12-viva\">https://atha.io/blog/2026-02-12-viva</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46989217\">https://news.ycombinator.com/item?id=46989217</a></p> <p>Points: 171</p> <p># Comments: 98</p>",
      "content_text": "<p>Article URL: <a href=\"https://atha.io/blog/2026-02-12-viva\">https://atha.io/blog/2026-02-12-viva</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46989217\">https://news.ycombinator.com/item?id=46989217</a></p> <p>Points: 171</p> <p># Comments: 98</p>",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "TikTok is tracking you, even if you don't use the app. Here's how to stop it",
      "url": "https://www.bbc.com/future/article/20260210-tiktok-is-tracking-you-even-if-you-dont-use-the-app-heres-how-to-stop-it",
      "published": "2026-02-12T14:19:32+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.bbc.com/future/article/20260210-tiktok-is-tracking-you-even-if-you-dont-use-the-app-heres-how-to-stop-it\">https://www.bbc.com/future/article/20260210-tiktok-is-tracking-you-even-if-you-dont-use-the-app-heres-how-to-stop-it</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46989151\">https://news.ycombinator.com/item?id=46989151</a></p> <p>Points: 51</p> <p># Comments: 24</p>",
      "content_text": "But most people might not realise that TikTok holds data about them even if they have never used the social media platform. An invisible tracker Tracking pixels are nothing new. For years, companies that run advertising networks – including Google, Meta and hundreds of others – have used them to eavesdrop on what people do across the web. They're an invisible image the size of one pixel of your screen that loads in the background of a website, full of data-harvesting tech. They're everywhere, and they're constantly watching you. Here's how it works. TikTok, for example, encourages companies to put pixels on their websites to help the social media giant harvest more data. Let's say I have an online shoe store. If I use a pixel, it lets TikTok collect lots of data about my customers in order to show them targeted ads. Plus, it helps TikTok figure out whether people who see those shoe ads end up making a purchase. That way, I know the ads I paid for are working, and maybe I'll pay for more. (Like most news organisations, the BBC uses analytics tools and shares data with advertising partners in accordance with our privacy policy . The BBC does not use TikTok tracking pixels on its website or place advertising pixels on third-party sites.) When it's shoe store data, the information might be innocuous. But I've reported on TikTok's data collection for years and pixels can collect extremely personal information. For example, last week I visited the website for a cancer support group. According to Disconnect, when I clicked a button on a form that said I was a cancer patient or a survivor, the website sent TikTok my email address along with those details. A women's health company sent TikTok data when I looked at fertility tests. A mental health organisation pinged TikTok when I indicated I'm looking for a crisis counsellor. Websites that use pixels send data about every single visitor, so it doesn't matter if you don't have a TikTok account.",
      "cover_image_url": "https://ychef.files.bbci.co.uk/624x351/p0n08phs.jpg"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Culture Is the Mass-Synchronization of Framings",
      "url": "https://aethermug.com/posts/culture-is-the-mass-synchronization-of-framings",
      "published": "2026-02-12T14:17:37+00:00",
      "summary": "<p>Article URL: <a href=\"https://aethermug.com/posts/culture-is-the-mass-synchronization-of-framings\">https://aethermug.com/posts/culture-is-the-mass-synchronization-of-framings</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46989124\">https://news.ycombinator.com/item?id=46989124</a></p> <p>Points: 33</p> <p># Comments: 7</p>",
      "content_text": "Marco Giancotti, February 12, 2026 Cover image: The Four Seasons; Spring, Christopher R. W. Nevinson 1 If you descend onto the Marunouchi Line platform in Ikebukuro Station on any weekday morning, you will witness an unusual train-boarding ritual. Like in any other Japanese station, people wait in line at the two sides of where each train's door will open. This is called seiretsu jousha (整列乗車, orderly boarding), and is a universal standard in Japan. Unlike most other stations in Tokyo, though, on Ikebukuro's Marunouchi platform people will form not one but two queues on each side. One of these queues, the one closest to the doors, is the senpatsu (先発, earlier departure) line, and will board the next train that comes; the other, shorter, queue, is called kouhatsu (後発, later departure) and is waiting to take the place of the senpatsu line: they'll skip the next train, and board the one after that instead. When the new train arrives, first everyone waits for the passengers to get off (the \"orderly boarding\" common sense), then the people in the senpatsu queue all get in, and finally the people in the kouhatsu queue shift laterally on the platform to become the new senpatsu . A new kouhatsu queue immediately starts forming in its now-vacated place. ① People get off. ② Senpatsu line gets on. ③ Kouhatsu line becomes the new senpatsu line. This is a rather strange way to do things. Why not simply form one kind of line, and use the age-old first-come-first-served approach? Why would anyone ever choose not to try boarding the next train? And why is this procedure used in the Marunouchi Line in Ikebukuro and not in the many other lines in the same station, or (for that matter) on most other lines and stations in Tokyo? The key to it all is the observation that Ikebukuro Station is a terminal of the Marunouchi Line, so all trains always start empty on that platform. This double-queueing ritual gives passengers a tradeoff that would not be available in most other cases: speed vs comfort. If you're in a hurry, you can directly join the senpatsu crowd and be (almost) guaranteed a spot on the very next train, but forget about sitting down—be ready to stand squeezed like a sardine. If, on the other hand, you have plenty of time, you may decide to get in the shorter kouhatsu queue—which will become the front of the senpatsu once the next train leaves—and you'll be (almost) guaranteed a comfy seat in your long commute. For an Italian like me, this whole process is nothing short of a miracle. I grew up in a city where metro train boarding during rush hour feels like a prelude to the apocalypse. ① Go for it!!! Many Italians can come up with the idea of waiting for passengers to get off before boarding themselves, but most crowds there lack the restraint to apply it with any kind of regularity. When it comes to the strategy of directly aiming for the next next train , though, I wonder if it has ever even occurred to anyone south of the Alps. The miraculous thing about the Japanese method is that there is no authoritative \"director\" standing next to each door and yelling at people where to stand. There are \" senpatsu \" and \" kouhatsu \" signs on the ground, but no detailed instructions or explanations. I doubt it is taught at school or anywhere else, either. People just seem to know, and to naturally implement the whole process without exchanging so much as a word with each other. Are these people human? Live in Japan as a foreigner for a while, and you'll see miracles of this kind everywhere. No one steals, even when people leave their purses and smartphones and wallets unattended in plain sight for half an hour at a time; no one litters; no one disturbs fellow train passengers by talking loudly or making phone calls; and people are extremely polite and go out of their way to help you if you ask. In Japan, you will only witness restraint and patience, even in the face of rudeness and selfishness from strangers. What kind of DNA compels them to behave in such a coordinated and collectively useful manner? Of course, I know that there is nothing innate in the miraculous \"Japanese Way\" because expats living here quickly adapt to the same behaviors. It's not just the ethnic Japanese that correctly follow the senpatsu / kouhatsu queueing system, for instance. All the long-time expats I know in Japan are—at least in public—just as polite, restrained, and rule-following as the average Japanese, regardless of their nationality. I wrote that no one steals unattended wallets in Japan, not that no native Japanese steals. In fact, the sure-fire way to spot a tourist in Tokyo is not by their appearance or the language they speak, but by how loudly they talk in public, or how they stand in spots that inconvenience other people. They're not trying to disrupt, they simply haven't had enough time to assimilate the local behavioral patterns. Those miraculous scenes have nothing to do with the Japanese DNA: it's their culture. And culture is, by and large, random, arbitrary, and self-reinforcing. 2 You can go and look at the history of Japan, their institutions past and present, their religious philosophies and military values, and you can point to many things that seem to \"explain\" why today's Japanese are polite, orderly, and ultra-civilized. This is a mistake, though, because all it does is kick the can a little farther down the road. Why were those institutions and philosophies like that? Why did the first samurai become so honorable? Simply going farther back in history only repeats the mistake. You won't find a final answer, because the answer is not at the beginning, it's in the ongoing process itself: chance and contingency. People behave the way they do because , period. If that seems implausible to you, think about simpler cases you might witness anywhere in the world. When a corridor is being traversed by crowds of people moving in both directions, two or more lanes will form spontaneously: the first two people trying to avoid each other's path will randomly dodge either left or right; the people behind them will find it more convenient to follow the path of those walking ahead, and very quickly everyone is walking in a line on \"their side\". Whether those going northward walk on the left and those going southward on the right, or the other way around, doesn't matter, and no one really cares. It's just arbitrarily become the easiest thing to do, and it stays that way as long as there are enough people in both directions. Sometimes there is a good initial reason behind a cultural standard, but that reason becomes irrelevant later on. The QWERTY layout of English keyboards started as a clever design for typewriters—it helped minimize jamming of the mechanisms—but is now completely meaningless and even sub-optimal for modern digital keyboards. If these things are simply cultural and arbitrary, why can't people change them, then? Well, have you tried changing the rhythm of a mass applause by clapping in a specific way? Or typing with a DVORAK keyboard? E... F... F... I... C... I... E... N... C... Y... ! Once a self-sustaining feedback loop has started, how it started ceases to mean anything. Mindless forces emerge that suck you in a specific direction. 3 So far, it sounds like what gets \"synchronized\" between people living in the same culture is their behavior and habits. This is true, but I don't believe it's the whole, or even the main, story. What I'm talking about is not a unification of actions but of the thinking patterns from which those actions arise. Culture is the mass-synchronization of framings. A mental model is a simulation of \" how things might unfold \", and we all build and rebuild hundreds of mental models every day. A framing, on the other hand, is \" what things exist in the first place \", and it is much more stable and subtle. Every mental model is based on some framing, but we tend to be oblivious to which framing we're using most of the time (I've explained all this better in A Framing and Model About Framings and Models ). Framings are the basis of how we think and what we are even able to perceive, and they're the most consequential thing that spreads through a population in what we call \"culture\". You're forced to learn this (at least in the abstract) when you begin noticing some apparent contradictions in the collective behavior of Japanese crowds. Non-residents tend to think that the core tenet of Japanese culture is to \"obey the rules\" or \"do things properly\", but that is absolutely not the case. How do you explain the fact that some rules are ignored by literally everyone here? People in Japan never follow the written rule to switch off your phone in the \"priority seats\" area of each train carriage (it's a precaution for those with pacemakers). People always actively climb escalators, despite incessant written and vocal requests that people stand still for safety reasons. Flows of people in corridors very often form lanes that go opposite those indicated by the signs on the floor. I only had to walk 30 seconds from the cafe I wrote this post in to take this picture. People don't mind parking all around the very explicit \"no bicycle parking\" signs. The list goes on. The more you pay attention, the more collective infractions you'll notice. Sure, these are mostly small transgressions of little consequence, and they are not enforced in any strong way. But if following the rules were a core value of Japanese culture, why would that matter? The real core value of Japanese culture (or one of them) is something like \"never stand out or make a fuss\". Nowhere in that principle is a strict requirement to follow the rules. In fact, it's perfectly fine, in Japan, to break the rules as long as that's what everyone does and expects you to do . In terms of framings, the Japanese culture has acquired—by arbitrary and unimportant means—a definition of the concept of (or a \" black box \" for) \"standing out\" that differs from its equivalent in many other cultures: instead of being generally neutral, it is seen as intrinsically unpleasant and embarrassing. The behavior that stems from employing this ontological \"thing\" (this particular flavor of \"standing out\") in your mental models is what you see manifested on the train platforms, on the escalators, etc. The Italian culture has the concept of simpatia that translates awkwardly to English as \"being a mix of likeable and/or charming and fun to be around\" and doesn't even exist in Japan. I do believe that having this compact and convenient idea of simpatia makes Italians more conscious of the importance of being simpatico and seek that property in others. It drives their behavior in more or less explicit ways. Similarly, English (as most Western languages) has a cultural black box for what we call \"sarcasm\", but this black box is largely absent from the Japanese cultural framing: sarcasm is simply not a thing in Japan, and people aren't (I'm tempted to say can't be ) sarcastic. It doesn't occur to them to be it. Each culture is made of shared framings—ontologies of things that are taken to exist and play a role in mental models—that arose in those same arbitrary but self-reinforcing ways. Anthropologist Joseph Henrich, in The Secret of Our Success , brings up several studies demonstrating the cultural differences in framings. He mentions studies that estimated the average IQ of Americans in the early 1800's to have been around 70—not because they were dumber, but because their culture at the time was much poorer in sophisticated concepts. Their framings had fewer and less-defined moving parts, which translated into poorer mental models. Other studies found that children in Western countries are brought up with very general and abstract categories for animals, like \"fish\" and \"bird\", while children in small-scale societies tend to think in terms of more specific categories, such as \"robin\" and \"jaguar\", leading to different ways to understand and interface with the world. But framings affect more than understanding. They influence how we take in the information from the world around us. Explaining this paper , Henrich writes: People from different societies vary in their ability to accurately perceive objects and individuals both in and out of context. Unlike most other populations, educated Westerners have an inclination for, and are good at, focusing on and isolating objects or individuals and abstracting properties for these while ignoring background activity or context. Alternatively, expressing this in reverse: Westerners tend not to see objects or individuals in context, attend to relationships and their effects, or automatically consider context. Most other peoples are good at this. How many connections and interrelations you consider when thinking is in the realm of framings. If your mental ontology treats most things as largely independent and self-sufficient, your mental models will tend to be, for better or worse, more reductionist and less holistic. 4 The definition of \"framing\" that I'm adopting on Aether Mug is more precise than what people use in general, and for this reason I don't know of any study that specifically tested how framings evolve in social interactions. But I don't think I'm making a bold leap by saying that we experience, at a deeper level, the same form of synchronization between framings that we can trivially witness between surface behaviors. It might take longer, but if everyone around you talks and acts based on the assumption that concepts A and B exist with certain properties, and no one ever mentions concept D or acknowledges it with their behavior, you will gradually shift to think in terms of A and B and not so much in terms of D. Given enough time, the ontological status of D in your mind might atrophy and vanish in the background, while A and B's status solidifies. Somehow, the commuters on Ikebukuro's Marunouchi platform have acquired clear and distinct concepts for \" senpatsu queue\" and \" kouhatsu queue\", both of which are absent from the framings of Italian commuters. The \"miraculous\" part is not that any of them can conceive the idea—any Italian would have no trouble understanding it and following it if those around them did the same—but that feedback loops emerged to reinforce them in the whole commuter culture. Like in the emergent walking lanes in a corridor, once these recursive forces have gained traction, it's almost trivial for newcomers to learn them as \"rules\" and comply. As is often the case, here the shared framing led to the rules, not the other way around. In this case, the emergent cultural rules have clear advantages for everyone: more choices, less stress, everyone wins. But it is not true, in general, that all framing synchronizations lead to better behaviors. Imitation is the sincerest flattery, William Henry Walker The basic force behind all culture formation is imitation. This ability is innate in all humans, regardless of culture: we are extraordinarily good imitators. Indeed, we are overimitators , sometimes with unfortunate consequences. Overimitation ... may be distinctively human. For example, although chimpanzees imitate the way conspecifics instrumentally manipulate their environment to achieve a goal, they will copy the behavior only selectively, skipping steps which they recognize as unnecessary [unlike humans, who tend to keep even the unnecessary steps]. ... Once chimpanzees and orangutans have figured out how to solve a problem, they are conservative, sticking to whatever solution they learn first. Humans, in contrast, will often switch to a new solution that is demonstrated by peers, sometimes even switching to less effective strategies under peer influence. — The Psychology of Normative Cognition , Stanford Encyclopedia of Philosophy, emphasis theirs. We have a built-in need to do what the people around us do, even when we know of better or less wasteful ways. This means that we can't even ex",
      "cover_image_url": "https://aethermug.com/assets/posts/culture-is-the-mass-synchronization-of-framings/536968ldsdl.jpg"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "The future for Tyr",
      "url": "https://lwn.net/Articles/1055590/",
      "published": "2026-02-12T14:17:06+00:00",
      "summary": "<p>Article URL: <a href=\"https://lwn.net/Articles/1055590/\">https://lwn.net/Articles/1055590/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46989117\">https://news.ycombinator.com/item?id=46989117</a></p> <p>Points: 44</p> <p># Comments: 12</p>",
      "content_text": "February 3, 2026 This article was contributed by Daniel Almeida The team behind Tyr started 2025 with little to show in our quest to produce a Rust GPU driver for Arm Mali hardware, and by the end of the year, we were able to play SuperTuxKart (a 3D open-source racing game) at the Linux Plumbers Conference (LPC). Our prototype was a joint effort between Arm, Collabora, and Google; it ran well for the duration of the event, and the performance was more than adequate for players. Thankfully, we picked up steam at precisely the right moment: Dave Airlie just announced in the Maintainers Summit that the DRM subsystem is only \" about a year away \" from disallowing new drivers written in C and requiring the use of Rust. Now it is time to lay out a possible roadmap for 2026 in order to upstream all of this work. What are we trying to accomplish with Tyr? Miguel Ojeda's talk at LPC this year summarized where Rust is being used in the Linux kernel, with drivers like the anonymous shared memory subsystem for Android (ashmem) quickly being rolled out to millions of users. Given Mali's extensive market share in the phone market, supporting this segment is a natural aspiration for Tyr, followed by other embedded platforms where Mali is also present. In parallel, we must not lose track of upstream, as the objective is to evolve together with the Nova Rust GPU driver and ensure that the ecosystem will be useful for any new drivers that might come in the future. The prototype was meant to prove that a Rust driver for Arm Mali could come to fruition with acceptable performance, but now we should iterate on the code and refactor it as needed. This will allow us to learn from our mistakes and settle on a design that is appropriate for an upstream driver. What is there, and what is not A version of the Tyr driver was merged for the 6.18 kernel release , but it is not capable of much, as a few key Rust abstractions are missing. The downstream branch (the parts of Tyr not yet in the mainline kernel) is where we house our latest prototype; it is working well enough to run desktop environments and games, even if there are still power-consumption and GPU-recovery problems that need to be fixed. The prototype will serve the purpose of guiding our upstream efforts and let us experiment with different designs. A kernel-mode GPU driver such as Tyr is a small component backing a much larger user-mode driver that implements a graphics API like Vulkan or OpenGL. The user-mode driver translates hardware-independent API calls into GPU-specific commands that can be used by the rasterization process. The kernel's responsibility centers around sharing hardware resources between applications, enforcing isolation and fairness, and keeping the hardware operational. This includes providing the user-mode driver with GPU memory, letting it know when submitted work finishes, and giving user space a way to describe dependency chains between jobs. Our talk ( YouTube video ) at LPC2025 goes over this in detail. Having a working prototype does not mean it's ready for real world usage, however, and a walkthrough of what is missing reveals why. Mali GPUs are usually found on mobile devices where power is at a premium. Conserving energy and managing the thermal characteristics of the device is paramount to user experience, and Tyr does not have any power-management or frequency-scaling code at the moment. In fact, Rust abstractions to support these features are not available at all. Something else worth considering is what happens if the GPU hangs. It is imperative that the system remains working to the extent possible, or users might lose all of their work. Owing to our \"prototype\" state, there is no GPU-recovery code right now. These two things are a hard requirement for deployability. One simply cannot deploy a driver that gobbles all of the battery in the system — making it hot and unpleasant in the process — or crashes and takes the user's work with it. On top of that, Vulkan must be correctly implementable on top of Tyr, or we may fail to achieve drop-in compatibility with our Vulkan driver (PanVK). This requires passing the Vulkan Conformance Testing Suite when using Tyr instead of the C driver. At that point, we would be confident enough to add support for more GPU models beyond the currently supported Mali-G610. Finally, we will turn our attention to benchmarking to ensure that Tyr can match the C driver's performance while benefiting from Rust's safety guarantees. We have demonstrated running a complex game with acceptable performance, so results are good so far. Which Rust abstractions are missing Some required Rust infrastructure is still work-in-progress. This includes Lyude Paul's work on the graphics execution manager (GEM) shmem objects, needed to allocate memory for systems without discrete video RAM. This is notably the case for Tyr, as the GPU is packaged in a larger system-on-chip and must share system memory. Additionally, there are still open questions, like how to share non-overlapping regions of a GPU buffer without locks, preferably encoded in the type system and checked at compile time. On top of allocating GPU memory, modern kernel drivers must let the user-mode driver manage its own view of the GPU address space. In the DRM ecosystem, this is delegated to GPUVM , which contains the common code to manage those address spaces on hardware that offers memory-isolation capabilities similar to modern CPUs. The GPU firmware also expects control over the placement of some sections in memory, so it will not work until this capability is available. Alice Ryhl is working on the Rust abstractions for GPUVM as well as the io-pgtable abstractions that are needed to manipulate the IOMMU page tables used to enforce memory isolation. These are both based on the previous work of Asahi Lina, who pioneered the first Rust abstractions for the DRM subsystem. Another unsolved issue is DRM device initialization. The current code requires an initializer for the driver's private data in order to return a drm::Device instance, but some drivers need the drm::Device to build the private data in the first place, which leads to an impossible-to-satisfy cycle of dependencies. This is also the case for Tyr: allocating GPU memory through the GEM shmem API requires a drm::Device , but some fields in Tyr's private data need to store GEM objects — for example, to parse and boot the firmware. Lyude Paul is working on this by introducing a drm::DeviceCtx that encodes the device state in the type system. The situation remains the same as when the first Tyr patches were submitted: most of the roadmap is blocked on GEM shmem , GPUVM, io-pgtable and the device initialization issue. There is room to integrate some work by the Nova team, as well: the register! macro and bounded integers. Once we can handle those items, we expect to quickly become able to boot the GPU firmware and then progress unhindered until it is time to discuss job submission. Another area needing consideration is the paths where the driver makes forward progress on completing fences , which are synchronization primitives that GPU drivers signal once jobs finish executing. These paths must be carefully annotated or the system may deadlock, and the driver must ensure that only safe locks are taken in the signaling path. Additionally, DMA fences must always signal in finite time, or someone elsewhere in the system may block forever. Allocating memory using anything other than GFP_ATOMIC must be disallowed, or the shrinker may kick in under memory pressure and wait on the very job that triggered it. All of this is covered in the documentation . We conveniently ignore this in the prototype, meaning it can randomly deadlock under memory pressure. Addressing this is straightforward: it is just a matter of carefully vetting key parts of the driver. Doing so elegantly, however, and perhaps in a way that takes advantage of Rust's type system is something that remains to be discussed. Looking into the future We have not touched upon what is next for Linux GPU drivers as a whole: reworking the job-submission logic in Rust. The current design assumes that drm_gpu_scheduler is used, but this has become a hindrance for some drivers in an age where GPU firmware can schedule jobs itself, and it's been plagued by hard-to-solve lifetime problems. Quite some time was spent at the X.Org Developer's Conference in 2025 discussing how to fix it. The current consensus for Rust is to write a new component that merely ensures that the dependencies for a given job are satisfied before the job is eligible to be assigned in the GPU's ring buffer, at which point the firmware scheduler takes over. This seems to be where GPU hardware is going, as most vendors have switched to firmware-assisted scheduling in recent years. As this component will not schedule jobs, it will probably be called JobQueue instead. This correctly conveys the meaning of a queue where new work is deposited in and removed once the dependencies are met and a job is ready to run. Philip Stanner has been spearheading this work. The plan is to also expose an API for C drivers using a technique I have described here in the past . This will possibly be the first Rust kernel component usable from C drivers, another milestone for Rust in the kernel, and a hallmark of seamless interoperability between C and Rust. One way that Tyr can fit into this overall vision is by serving as a testbed for the new design. If the old drm_gpu_scheduler can be replaced with the JobQueue successfully in the prototype, it will help attest its suitability for other, more complex drivers like Nova. Expect this discussion to continue for a while. In all, Tyr has made a lot of progress this past year. Hopefully, it will continue to do so through 2026 and beyond.",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Apple patches decade-old iOS zero-day exploited in the wild • The Register",
      "url": "https://www.theregister.com/2026/02/12/apple_ios_263/",
      "published": "2026-02-12T14:16:10+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.theregister.com/2026/02/12/apple_ios_263/\">https://www.theregister.com/2026/02/12/apple_ios_263/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46989107\">https://news.ycombinator.com/item?id=46989107</a></p> <p>Points: 99</p> <p># Comments: 47</p>",
      "content_text": "Apple patched a zero-day vulnerability affecting every iOS version since 1.0, used in what the company calls an \"extremely sophisticated attack\" against targeted individuals. CVE-2026-20700, discovered by Google's Threat Analysis Group, affects dyld - Apple's dynamic linker - and allows attackers with memory write capability to execute arbitrary code. Apple said the flaw was exploited in the wild and may have been part of an exploit chain. Its advisory stated: \"An attacker with memory write capability may be able to execute arbitrary code. Apple is aware of a report that this issue may have been exploited in an extremely sophisticated attack against specific targeted individuals on versions of iOS before iOS 26.\" Google's researchers also referenced two December vulnerabilities in their report that both carry 8.8 CVSS scores. CVE-2025-14174 is an out-of-bounds memory access flaw in Google Chrome's ANGLE graphics engine on Mac that could be exploited through a malicious webpage. The other, CVE-2025-43529, is a use-after-free leading to code execution. Brian Milbier, deputy CISO at Huntress, said: \"Think of dyld as the doorman for your phone. Every single app that wants to run must first pass through this doorman to be assembled and given permission to start. \"Usually, the doorman checks credentials and places apps in a high-security 'sandbox' where they can't touch your private data. This vulnerability allows an attacker to trick the doorman into handing over a master key before security checks even begin.\" By chaining this with WebKit flaws Apple also addressed in the iOS 26.3 update, \"attackers have created a 'zero-click' or 'one-click' path to total control. They use a fake ID to bypass the front gate – your browser – and then exploit the doorman's flaw to take over the entire building,\" Milbier added. \"This level of sophistication resembles other exploits developed by the commercial surveillance industry. These are private companies that also developed prominent spyware tools like Pegasus and Predator . They sell these types of exploits or tools to government clients. While some updates in this patch address minor issues, such as data leakage from physical access, the dyld/WebKit chain is in a different league. iOS 26.3 closes a door that has been unlocked for over a decade.\" Apple's updates for iOS and iPadOS also feature a host of other fixes for various bugs, including flaws that grant root access and disclose sensitive user information, but CVE-2026-20700 is the only one it said was exploited in the wild. ®",
      "cover_image_url": "https://regmedia.co.uk/2023/12/01/apple_shutterstock.jpg"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "Amazon‚Äôs Send to Alexa Plus makes the Kindle Scribe feel more like a productivity device",
      "url": "https://www.theverge.com/gadgets/877625/amazon-send-to-alexa-plus-kindle-scribe-hands-on",
      "published": "2026-02-12T14:04:48+00:00",
      "summary": "Amazon's rolling out a new \"Send to Alexa Plus\" feature to the latest Kindle Scribe and Kindle Scribe Colorsoft owners starting February 12. The feature lets you send your notes or documents to Amazon's AI-powered Alexa Plus assistant, which can then summarize them, turn them into to-do lists, calendar events, or reminders, as well as [&#8230;]",
      "content_text": "Amazon‚Äôs rolling out a new ‚ÄúSend to Alexa Plus‚Äù feature to the latest Kindle Scribe and Kindle Scribe Colorsoft owners starting February 12. The feature lets you send your notes or documents to Amazon‚Äôs AI-powered Alexa Plus assistant, which can then summarize them, turn them into to-do lists, calendar events, or reminders, as well as help brainstorm, and offer project guidance. I spent about a day or so testing it primarily to help with caregiving tasks, and it was mostly helpful despite some limitations. It works best when asked to digest information into something actionable. It accurately summarized my handwritten notes and PDF documents, even across different templates or hard-to-read text colors, and it worked well for logistics, like turning my notes about my mom‚Äôs next appointment into calendar events and reminders with helpful context. I also tested how well it answered questions about my notes and documents, provided guidance, and handled brainstorming. In one test, while I was on hold with Medi-Cal for three hours, my Echo Show 8 read back key information from a dispute letter I had written earlier. I also sent a PDF of an email and asked it to add up a list of charges, which it calculated correctly. It was also good at pulling specific details. In one case, I wrote ‚ÄúBlue Shield‚Äù in messy handwriting without labeling it as an insurance company, and it still identified it from context. In another test, I wrote notes for someone who was taking my mom to an appointment and intentionally left out details like the address. After I clicked ‚ÄúShare‚Äù and ‚ÄúSend to Alexa,‚Äù I asked what might be missing from the note. Alexa received the note and suggested adding the address, doctor‚Äôs name, medication list, and questions to ask. It also generated a decent draft when I asked for help brainstorming a phone script for arguing with the insurance company, though unfortunately Alexa couldn‚Äôt actually apply those changes to the original Scribe note. I wasn‚Äôt able to send the full version to my email address either, sadly; I could, however, view the draft in the ‚Äúchat history‚Äù section of the Alexa app. Where it struggled was depth and nuance. I wanted to test how useful it could be for, say, quizzing yourself on a piece, so I sent myself a copy of my old Kindle Scribe review and another article. It took four or five tries to generate a detailed outline, and it sometimes missed small but meaningful distinctions, like interpreting ‚ÄúAI-powered summarization feature‚Äù as just ‚ÄúAI-powered feature.‚Äù That was a problem as it marked answers as correct when they were only partially right. Send to Alexa Plus isn‚Äôt perfect, but overall it‚Äôs genuinely useful and gives Amazon an edge over rivals like the Kobo Elipsa 2E, which lack voice assistant integration. The Elipsa 2E is still my favorite because it covers the basics better ‚Äî notably, it‚Äôs much easier to annotate ebooks‚Äî but the Scribe is getting easier to recommend if you‚Äôre in Amazon‚Äôs ecosystem.",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2025/12/258193_Kindle_Scribe_Colorsoft_AKrales_0120.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "Eclipse backs all-EV marketplace Ever in $31M funding round",
      "url": "https://techcrunch.com/2026/02/12/eclipse-backs-all-ev-marketplace-ever-in-31m-funding-round/",
      "published": "2026-02-12T14:00:09+00:00",
      "summary": "The San Francisco-based startup says its AI-first approach has allowed it to scale faster.",
      "content_text": "If you want to buy or sell a used EV right now, what’s the first step you’d take? A startup called Ever wants to be the answer to that question. The company, which bills itself as the first “AI-native, full-stack auto retail business” for electric vehicles, already has thousands of customers buying and selling their EVs on the platform. Now it’s looking to scale with help from a $31 million Series A funding round led by Eclipse, with Ibex Investors, Lifeline Ventures, and JIMCO — the investment arm of the Saudi Arabian Jameel family (an early investor in Rivian) — as co-investors. Over the last decade, companies like Carvana and Carmax helped usher in the digital car-buying experience. More recently, myriad startups have tried to improve the car-buying experience with AI, pitching ideas like voice agents or smarter scheduling software. Eclipse’s Jiten Behl thinks this is the wrong approach if you want to really modernize the automotive retail experience, though. “These bolt-on AI tools are band-aids,” he said in an interview with TechCrunch. He likened it to how many major automakers’ first EVs were essentially combustion vehicles that were repackaged to fit electric drivetrains. That approach came with major tradeoffs compared to designing a new EV from the ground up, which was the approach companies like Tesla and Rivian took. “Auto retail is a perfect candidate for disrupting with AI, you know? It’s a lot of process, lot of labor, [very] rules-based,” he said. Lasse-Mathias Nyberg, Ever co-founder and CEO, said in an interview that buying or selling a car typically triggers “hundreds or thousands of different actions” that a retailer needs to perform in order to complete the transaction. “There’s massive complexities or frictions on both sides.” Techcrunch event Boston, MA | June 23, 2026 In 2022, he and his team set out to reduce or remove those complexities. What they settled on after a year of research was a digital-first auto retailer. The core tech is an orchestration layer or “operating system” that can handle all the different workflows behind a transaction, whether it’s processing information submitted by a prospective buyer or seller, or managing the vehicle inventory. “When you do appraisals, or pricing, or titling, it’s very deterministic in terms of what steps need to be taken. And today, there are lots of single point solution tools that are used,” he said. Most companies “use these tools together in a very inefficient manner, and you think that you are on a digital journey — but if you actually could clean-sheet it, and if you actually could use the power of agentic AI, and you can create one unified customer experience and remove all these micro-frictions.” Nyberg claimed that building the company this way has allowed Ever’s sales team to be two to three times more productive than they would be otherwise, and he expects that to scale as the company grows. He said this extra efficiency and productivity beefs up their margins, which can be booked as profit or passed along to the customer by offering lower prices. Ever applies this fresh approach to both its online marketplace and physical locations. Nyberg said the hybrid model is important because seeing and trying a car in person remains crucial to the shopping experience for a lot of buyers — especially those who might be assessing EVs for the first time. Early reviews of Ever’s product have been mixed. Users on one particular Reddit thread from last year were split, with some drawn to how Ever is making EVs easier to buy, while others detailed struggles getting in touch with the startup’s team. Ever was just getting off the ground and was more or less operating in stealth, and so Nyberg chalks that up to a learning experience. He said his team is working hard to make sure its system can be flexible enough to accomplish everything the company has set out to do. The bigger challenge may be overall interest in EVs, which has cooled a bit in the United States. Nyberg said he hasn’t ruled out Ever buying or selling used combustion cars in the future, but wants to stick to EVs in the near-term since there isn’t a retailer that is laser-focused on these vehicles. Behl, who spent eight years on Rivian’s leadership team, admitted he’s a “hopeless romantic when it comes to EVs,” and said he still believes the industry is moving towards electric propulsion because of the inherent benefits. And he said his “first thought” when started doing diligence on Ever was: “I wish Rivian was doing this.” More broadly, Behl said, companies like Carvana are still in the single digits of market share when it comes to automotive retail. That’s why he sees so much upside in Ever. “Customers are going to continue to gravitate towards better experience when it comes to buying cars, which means it is going to be a digitally-led customer experience which takes away all the friction of buying and selling a car,” he said.",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2026/02/ever-car-founders.jpg?resize=1156,1200"
    },
    {
      "industry": "technology",
      "source": "Wired",
      "title": "How to Choose the Right TV in 2026",
      "url": "https://www.wired.com/story/how-to-buy-the-right-tv/",
      "published": "2026-02-12T14:00:00+00:00",
      "summary": "How many ports do you need? What does 4K UHD even mean? What's RGB LED? Here’s everything you need to know.",
      "content_text": "TVs can be tricky to buy. Whether you read our exquisite guide to the Best TVs or not, you probably won't find every top model at your local Walmart, Best Buy, or Costco. And when you're browsing retail websites, it's easy to feel overwhelmed by marketing speak like HDR, QLED, or “local dimming.” That's why we've collected a few tips, tricks, and terms to help you shop smarter when buying a new TV. Think of this as a checklist to consider before you buy your next big screen. Be sure to check out our guide to the Best Bookshelf Speakers , Best Soundbars , and our How to Buy a Soundbar Guide for other helpful hints on how to upgrade your home theater. Updated February 2026: We've added new information with the latest in TV tech, including RGB LED, SQD, ATSC 3.0, and other acronyms explained in plain English. Table of Contents How Much Should You Expect to Pay? You'll see quite a range of prices when shopping for a new TV. It all comes down to the features, screen type, and performance. Here's a basic price guide on what you can expect for the money. Be sure to check out our list of the Best TVs for specific model information. Under $500 If you just want a TV with a regular-sized screen that can stream your favorite shows, this is a good starting place, especially if you're on a budget. We highly suggest you avoid TVs that aren’t made by the likes of Samsung, LG, Sony, TCL, Hisense, Vizio, Panasonic, or Roku. Also, avoid spending less than $200 on anything , unless it’s smaller than 55 inches. On the other side, we suggest spending a bit more if you want a 65-inch screen or bigger for quality control. Here are some TVs we like in this price range: Between $500 and $1,000 If you watch TV often and want something between 50 and 65 inches, this is a great price category to target. Stick to the major brands above—especially our favorite value-oriented brands like TCL, Hisense, and Roku—and you can hardly go wrong. If you want the best from more premium brands like Samsung, LG, and Sony, you'll likely need to spend up. Here are some TVs we like in this general price range: More Than $1,000 This price range is for TVs 65 inches or larger, and/or those with advanced panel technologies, like OLED, and QLED/mini-LED. You’ll also typically get higher-end processing for improved clarity and upscaling, and better hardware like a 120-Hz panel for video games and sports. Get into the $2,000 range and you'll find top-rated 4K TVs from Samsung, LG, Panasonic, and Sony. Spend more than a few thousand, and you can get into 8K resolution if you care to—not that there is currently much 8K content to watch. Here are some TVs we love at this price range: What Size TV Should You Get? TCL QM6K Photograph: Ryan Waniata When it comes to screens, bigger is better, right? Sure, but that doesn't mean you should always spring for the largest size. Pricing on bigger models continues to drop, but it can still get out of hand quickly, and you should always make sure you have room for all that screen footage. If you're watching your budget, common cheap TV performance issues like screen aberrations, inaccurate colors, and mediocre image processing may be harder to overlook on a bigger canvas, so you'll want to balance your size and performance needs equally. Under 55 Inches Some high-end OLED TVs have smaller sizes so they can double as computer monitors, but most TVs below 55 inches are largely nonstandard these days. Many manufacturers still make models below this size, but you likely won't get the latest and greatest performance. 55-Inch TVs This is the smallest standard size for modern TVs. It’s generally considered ideal for the small-to-average room in a home or apartment, but if you have a larger space, you may want to size up. Be aware that TVs this size and larger sometimes come with feet on both ends of the screen; if you’re not wall-mounting, make sure you measure the width of your TV stand to ensure it can hold your new TV. That said, we're seeing even midrange models switching to adaptable feet and/or pedestal stands more often. 65-Inch TVs This has long been the default option for larger living rooms or for spaces where you’re going to have many eyes watching at once. As TV tech has improved and prices have dropped, 65-inch models have started to become the standard for many, though it should be noted that they may dominate your decor to some extent in smaller rooms. If you have the extra cash and space, it is worth the upgrade for those who love seeing details even closer in films and TV shows. 75 Inches and Above I only recommend TVs this size and larger for people who have huge rooms and are looking for a truly cinematic experience. Buyer beware: Pricing on TVs this large can get quite pricey (well above $2,000 for higher-end models). Cheaper ones (under $1,000) might not look great due to limitations in processing and panel lighting. Be prepared to have a professional or a group of friends help you move and mount a screen this large. What Screen Resolution Do You Need? Resolution means the number of pixels on your screen. Modern TVs come in 1080p “Full HD” resolution (1,920 x 1,080 pixels), 4K Ultra HD or “UHD” (3,840 x 2,160), and 8K “8K UHD” (7,680 x 4,320) variants. TVs below or above 4K UHD resolution are relatively rare but for opposite reasons: Full HD screens are now old tech and are reserved for only the smallest and cheapest models; 8K resolution is available mostly on expensive, big-screen TVs and 8K content is still virtually nonexistent. What's the difference? More pixels! 4K TVs have about four times the number of pixels as 1080p screens, and 8K TVs have about four times that of 4K models, or 16 times (!) that of 1080p. In theory, this means a much clearer picture than 4K TVs, but that's not necessarily the reality. Because the pixels in a 4K TV at standard sizes (55-65 inches) are already packed so densely into the display, you may not be able to see the difference between 4K and 8K TVs unless you're watching from a close range or on a very large screen (75 inches or above). The lack of 8K content also means most of the video you're watching will need to be upscaled, or raised to the TV's resolution by its internal processing system. For this reason, if you're buying an 8K TV, you'll usually want to buy a more premium model with good upscaling from a brand that makes them regularly, like Samsung's QN900 series .",
      "cover_image_url": "https://media.wired.com/photos/6849a33ef3e19a5bab3da558/191:100/w_1280,c_limit/Sony%20Bravia%208%20II%20OLED%20TV%20Review_.png"
    },
    {
      "industry": "technology",
      "source": "Wired",
      "title": "Here’s What It’s Like to Use TurboTax’s Mobile App to File Taxes on Your Own (2026)",
      "url": "https://www.wired.com/story/whats-new-this-year-at-turbotax/",
      "published": "2026-02-12T13:38:00+00:00",
      "summary": "Sigh. Another tax season is upon us. I filed my taxes with TurboTax, exploring its newest features and free or low-cost filing options.",
      "content_text": "I've used TurboTax to file my taxes for several years. It's the most popular DIY tax service , and also often the cheapest and arguably most straightforward. TurboTax has the filer in mind by utilizing an easy-to-use interface, offering available expert help, with different options for document auto-upload; helpful tips and information regarding tax requirements; and transparent, low-cost options for every type of filer. The service makes it super easy for returning users by storing previous years’ information, allowing easy auto-upload, and remembering choices and previously used forms from years past. Doing my taxes as a returning user with TurboTax takes a fraction of the time of other tax services I have tested. (Need a jumping off point? I've got a guide on how to file your taxes online for extra help.) Yes, You Can Actually File for Free If you haven't tried TurboTax, this is the best time to see if it's the right fit for you (and be able to file for free). You can file both state and federal taxes for $0 right now. There are only a few requirements for this awesome free filing deal. You must not have filed with TurboTax before (and are switching from another provider), and you must file in the TurboTax mobile app by February 28. You'll need to both start and file within the mobile app; this is only eligible on DIY (self-guided) tax services and excludes expert assist products. This means that it applies to Simple Form 1040 returns only (meaning no schedules, except for EITC, CTC, student loan interest, and Schedule 1-A forms are eligible). One of the downsides to TurboTax is that while it's (in my opinion) the easiest-to-use interface with seamless auto-upload features, it can be a bit more expensive than similar competitors. I've used FreeTaxUSA in the past, when my income was lower and my taxes were simpler. The service is very similar in design to TurboTax, and while it is still a low-cost option, it's not completely 100 percent free, as it charge $16 for filing a state return. Plus, when I tested the service last year, FreeTaxUSA gave me the highest amount of taxes owed from all services I tested. TurboTax filed more than twice the number of free returns as FreeTaxUSA last year (based on the total number of federal and state returns filed in Tax Year 2024). And this tax season, more than 100 million people in the US are eligible for free filing with TurboTax. If you file your own federal and state returns using DIY TurboTax products, filing will be free if you use the mobile app until February 28. Filing in Your Hands Filing taxes can be confusing and potentially expensive. While I urge anyone who hasn't filed with TurboTax to take advantage of the free federal and state filing deal through the mobile app , there are several options if you have filed with the service before or have more complicated returns that may require additional assistance. There are three options for filers: DIY , where you file yourself with step-by-step instructions (the previously mentioned service eligible for the free filing deal); Expert Assist , where you get help from tax experts throughout the process and have the expert review it before submitting; or you can also get your taxes done completely by a local tax expert with Expert Full Service . Prices vary based on the chosen tier and when you file (the earlier, the cheaper, especially if you're able to file before March). The filing process starts out with a helpful questionnaire so that the program knows which sections are applicable to you, like dependents, assets, and education, so you’re not slogging through things that aren’t relevant. At the beginning, TurboTax also estimated the time it’d take to finish and asked how I filed last year—no other service I previously tested did either, which was helpful in estimating how long the process would take.",
      "cover_image_url": "https://media.wired.com/photos/698d086e7e85bd6db370b312/191:100/w_1280,c_limit/Screenshot%202026-02-11%20at%202.53.17%E2%80%AFPM.png"
    },
    {
      "industry": "technology",
      "source": "Wired",
      "title": "The Best Kindle Accessories",
      "url": "https://www.wired.com/story/best-kindle-accessories/",
      "published": "2026-02-12T13:34:00+00:00",
      "summary": "Looking to better protect your Kindle or add a little personality to your favorite e-reader? From cases and covers to page turners and even charms, this is the guide for you.",
      "content_text": "Moko Blue Twill Kindle Case This soft case has nice details on the cover, plus it has both a kickstand and a strap so that you can easily hold it or prop it up. There's a magnet to close it that will sometimes wake the Kindle up when you open it, though not always in my experience. PopSockets PopCase Kindle and PopGrip PopSockets has a new Kindle case collection with built-in MagSafe attachments, so you can easily add on a PopGrip or other attachment if you wish. The Bookish designs are fun, and there are two collections of designs to enjoy. This one's the Curled Up With a Good Book case and grip. Kindle Straps and Grips Looking for an easier way to hold your Kindle? Add these straps and grips right onto the Kindle or the case. Strapsicle E-Reader Hand Strap These silicone straps are super simple to use: Just pull them over opposite corners and you're done. No sticky adhesive, no magnets, just simple straps. The set comes with two, but using just one felt plenty secure. PopSockets Heart of Silver MagSafe PopGrip This is my favorite PopGrip from PopSockets, since it's a little larger and feels more secure. It uses a magnetic sticker that you can add onto any existing Kindle or case, or a case that already has a built-in MagSafe strip like the PopSockets cases. You can also get it as a PopTop that you can switch out with other sticky-backed PopSockets grips. PopSockets Bookish PopGrips This PopGrip works similar to the Heart of Silver above but is a little smaller in the regular round design. PopSockets has two cute Bookish collections with pretty grips that lean into fantasy themes, like this one of a dragon hoarding its books. CoBak Secure Hand Strap for Kindle and Tablets This strap is similar to the Strapsicle, but it has a softer feel and can stretch farther. It's both affordable and super comfortable, and it can be used with a slim case. Kindle Holders Hate holding up your Kindle? Or struggle with chronic pain that makes holding it feel terrible ? These holders will literally take the weight out of your hands. Lamicall Gooseneck iPad Holder This holder works for Kindles and tablets alike, and even my Nintendo Switch. The clamp base lets you attach it to tables and furniture, and it's easy to position in front of or even above you if you wanted to lie down and read. Lamicall Tablet Pillow Stand Holder If you want something that's freestanding, this pillow tablet stand holder works great for a Kindle. I use it on the couch, and I can sit up or lounge back and adjust the stand arm to suit my position. There are also two built-in cup and snack holders. Lamicall says they're food safe, but I just use it to hold my tea mug and phone. A Kindle Page Turner The hottest new item to get as a Kindle lover is a page turner. They're especially handy for holders like the ones above, where your hands aren't already on the device, and can make for a great accessibility accessory for readers with different needs. My biggest irritation with these devices so far is that you have to charge them both individually, and if one runs out of battery, the whole thing is useless. I also don't love that the turner does tend to block at least one letter while I read, and you can't place it on the lower or upper margins since it'll activate the menus instead of turning the page. Still, it makes reading ultra comfortable, especially for my strained wrists . Here's my favorite one so far that's been solid at holding a charge, and next I'm testing this remote ($15) with a wearable ring clicker instead of a remote.",
      "cover_image_url": "https://media.wired.com/photos/698d5a45e7f60ad4a7dd4475/191:100/w_1280,c_limit/The%20Best%20Kindle%20Accessories.png"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "I Improved 15 LLMs at Coding in One Afternoon. Only the Harness Changed.",
      "url": "http://blog.can.ac/2026/02/12/the-harness-problem/",
      "published": "2026-02-12T13:30:20+00:00",
      "summary": "<p>Article URL: <a href=\"http://blog.can.ac/2026/02/12/the-harness-problem/\">http://blog.can.ac/2026/02/12/the-harness-problem/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46988596\">https://news.ycombinator.com/item?id=46988596</a></p> <p>Points: 208</p> <p># Comments: 91</p>",
      "content_text": "In fact only the edit tool changed. That’s it. 0x0: The Wrong Question The conversation right now is almost entirely about which model is best at coding, GPT-5.3 or Opus. Gemini vs whatever dropped this week. This framing is increasingly misleading because it treats the model as the only variable that matters, when in reality one of the bottlenecks is something much more mundane: the harness. Not only is it where you capture the first impression of the user (is it uncontrollably scrolling, or smooth as butter?), it is also the source of every input token, and the interface between their output and every change made to your workspace. I maintain a little “hobby harness”, oh-my-pi , a fork of Pi , a wonderful open-source coding agent by Mario Zechner. I’ve so far authored ~1,300 commits, mostly playing around and making incremental improvements here and there when I see a pain point, ( or autism strikes and I see an opportunity to embed more Rust via N-API because “spawning rg feels wrong” ). Why bother, you ask? Opus may be a great model, but Claude Code to this day leaks raw JSONL from sub-agent outputs, wasting hundreds of thousands of tokens. I get to say, “fuck it, subagents output structured data now”. Tool schemas, error messages, state management, everything between “the model knows what to change” and “the issue is resolved.” This is where most failures happen in practice. Being model agnostic, it is a great testing ground, as the model is but a parameter. The real variable is the harness, where you have unimaginable control over. Anyhow, let me tell you about this one variable I changed yesterday. Before I explain what I built, it’s worth understanding the state of the art. Codex uses apply_patch : It takes a string as input, which is essentially an OpenAI-flavored diff, and instead of relying on a structured schema, the harness just expects this blob to follow a strict set of rules. Since OpenAI folks are without a doubt smart, I’m sure the token selection process is biased to fit this structure at the LLM gateway for the Codex variants of GPT, similar to how other constraints like JSON schemas or required tool calls work. But give this to any other model, completely unaware of it? Patch failures go through the roof. Grok 4’s patch failure rate in my benchmark was 50.7% , GLM-4.7’s was 46.2% . These aren’t bad models — they just don’t speak the language. Claude Code (and most others) use str_replace : find the exact old text, swap in the new text. Very simple to think about. But the model must reproduce every character perfectly, including whitespace and indentation. Multiple matches? Rejected. The “String to replace not found in file” error is so common it has its own GitHub issues megathread (+27 other issues). Not exactly optimal. Gemini does essentially the same thing plus some fuzzy whitespace matching. Cursor trained a separate neural network : a fine-tuned 70B model whose entire job is to take a draft edit and merge it into the file correctly. The harness problem is so hard that one of the most well-funded AI companies decided to throw another model at it, and even then they mention in their own blog post that “fully rewriting the full file outperforms aider-like diffs for files under 400 lines.” Aider’s own benchmarks show that format choice alone swung GPT-4 Turbo from 26% to 59%, but GPT-3.5 scored only 19% with the same format because it couldn’t reliably produce valid diffs. The format matters as much as the model. The Diff-XYZ benchmark from JetBrains confirmed it systematically: no single edit format dominates across models and use cases. EDIT-Bench found that only one model achieves over 60% pass@1 on realistic editing tasks. As you can see, there is no real consensus on the “best solution” to the simple “how do you change things” problem. My 5c: none of these tools give the model a stable, verifiable identifier for the lines it wants to change without wasting tremendous amounts of context and depending on perfect recall. They all rely on the model reproducing content it already saw. When it can’t — and it often can’t — the user blames the model. 0x2: Hashline! Now bear with me here. What if, when the model reads a file, or greps for something, every line comes back tagged with a 2-3 character content hash: 1 1:a3|function hello() { 2 2:f1| return \"world\"; 3 3:0e|} When the model edits, it references those tags — “replace line 2:f1 , replace range 1:a3 through 3:0e , insert after 3:0e .” If the file changed since the last read, the hashes (optimistically) won’t match and the edit is rejected before anything gets corrupted. If they can recall a pseudo-random tag, chances are, they know what they’re editing. The model then wouldn’t need to reproduce old content, or god forbid whitespace, to demonstrate a trusted “anchor” to express its changes off of. 0x3: The Benchmark Since my primary concern was about real-world performance, the fixtures are generated as follows: Take a random file from the React codebase. Introduce mutations, framed as bugs, via an edit whose inverse we can expect (e.g. operator swaps, boolean flips, off-by-one errors, optional chains removed, identifiers renamed). Generate a description of the issue in plain English. An average task description looks something like this: 1 # Fix the bug in `useCommitFilteringAndNavigation.js` 2 A guard clause (early return) was removed. 3 The issue is in the `useCommitFilteringAndNavigation` function. 4 Restore the missing guard clause (if statement with early return). Naturally, we don’t expect 100% success rate here, since the model can come up with a unique solution that isn’t necessarily the exact same file, but the bugs are mechanical enough that most of the time, the fix is our mutation being reverted. 3 runs per task, 180 tasks per run. Fresh agent session each time, four tools (read, edit, write). We simply give it a temporary workspace, pass the prompt, and once the agent stops, we compare against the original file before and after formatting. Sixteen models, three edit tools, and the outcome is unambiguous: patch is the worst format for nearly every model, hashline matches or beats replace for most, and the weakest models gain the most. Grok Code Fast 1 went from 6.7% to 68.3%, a tenfold improvement, because patch was failing so catastrophically that the model’s actual coding ability was almost completely hidden behind mechanical edit failures. MiniMax more than doubled. Grok 4 Fast’s output tokens dropped 61% because it stopped burning tokens on retry loops. 0x4: So What? +8% improvement in the success rate of Gemini is bigger than most model upgrades deliver, and it cost zero training compute. Just a little experimenting (and ~$300 spent benchmarking). Often the model isn’t flaky at understanding the task. It’s flaky at expressing itself. You’re blaming the pilot for the landing gear. 0x5: Little Bit About the Vendors Anthropic recently blocked OpenCode , a massively popular open-source coding agent, from accessing Claude through Claude Code subscriptions. Anthropic’s position “OpenCode reverse-engineered a private API” is fair on its face. Their infrastructure, their rules. But look at what the action signals: Don’t build harnesses. Use ours. It’s not just Anthropic either. While writing this article, Google banned my account from Gemini entirely: Not rate-limited. Not warned. Disabled . For running a benchmark — the same one that showed Gemini 3 Flash hitting 78.3% with a novel technique that beats their best attempt at it by 5.0 pp. I don’t even know what for. Here is why that is backwards. I just showed that a different edit format improves their own models by 5 to 14 points while cutting output tokens by ~20%. That’s not a threat. It’s free R&D. No vendor will do harness optimization for competitors’ models. Anthropic won’t tune for Grok. xAI won’t tune for Gemini. OpenAI won’t tune for Claude. But an open-source harness tunes for all of them, because contributors use different models and fix the failures they personally encounter. The model is the moat. The harness is the bridge. Burning bridges just means fewer people bother to cross. Treating harnesses as solved, or even inconsequential, is very short-sighted. I come from a background of game security. Cheaters are hugely destructive to the ecosystem. Sure, they get banned, chased, sued, but a well-known secret is that eventually the security team asks, “Cool! Want to show us how you got around that?”, and they join the defense. The correct response when someone messes with your API, and manages to gather a significant following using their tools is “tell us more”, not “let’s blanket-ban them in thousands; plz beg in DMs if you want it reversed tho.” The harness problem is real, measurable, and it’s the highest-leverage place to innovate right now. The gap between “cool demo” and “reliable tool” isn’t model magic. It’s careful, rather boring, empirical engineering at the tool boundary. The harness problem will be solved. The question is whether it gets solved by one company, in private, for one model, or by a community, in the open, for all of them. The benchmark results speak for themselves. All code, benchmarks, and per-run reports: oh-my-pi",
      "cover_image_url": "https://blog.can.ac/2026/02/12/the-harness-problem/og.png"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Pebble Emulator",
      "url": "https://ericmigi.github.io/pebble-qemu-wasm/",
      "published": "2026-02-12T13:16:11+00:00",
      "summary": "<p>Article URL: <a href=\"https://ericmigi.github.io/pebble-qemu-wasm/\">https://ericmigi.github.io/pebble-qemu-wasm/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46988462\">https://news.ycombinator.com/item?id=46988462</a></p> <p>Points: 25</p> <p># Comments: 3</p>",
      "content_text": "Pebble Emulator Runs entirely in your browser — no server, no install. QEMU is compiled to WebAssembly, emulating the original Pebble ARM hardware and booting real PebbleOS firmware. Tested on desktop, doesn't work well on mobile yet. Firmware: Full PebbleOS SDK PebbleOS Boot May take 2-4 mins to boot Select firmware and click Boot FPS: -- Back Up Select Down Keys: Left=Back, Up/Down=Navigate, Right=Select",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Apache Arrow is 10 years old ðŸŽ‰",
      "url": "https://arrow.apache.org/blog/2026/02/12/arrow-anniversary/",
      "published": "2026-02-12T13:13:30+00:00",
      "summary": "<p>Article URL: <a href=\"https://arrow.apache.org/blog/2026/02/12/arrow-anniversary/\">https://arrow.apache.org/blog/2026/02/12/arrow-anniversary/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46988438\">https://news.ycombinator.com/item?id=46988438</a></p> <p>Points: 57</p> <p># Comments: 11</p>",
      "content_text": "Apache Arrow is 10 years old ðŸŽ‰ Published 12 Feb 2026 By The Apache Arrow PMC (pmc) The Apache Arrow project was officially established and had its first git commit on February 5th 2016, and we are therefore enthusiastic to announce its 10-year anniversary! Looking back over these 10 years, the project has developed in many unforeseen ways and we believe to have delivered on our objective of providing agnostic, efficient, durable standards for the exchange of columnar data. How it started From the start, Arrow has been a joint effort between practitioners of various horizons looking to build common grounds to efficiently exchange columnar data between different libraries and systems. In this blog post , Julien Le Dem recalls how some of the founders of the Apache Parquet project participated in the early days of the Arrow design phase. The idea of Arrow as an in-memory format was meant to address the other half of the interoperability problem, the natural complement to Parquet as a persistent storage format. Apache Arrow 0.1.0 The first Arrow release, numbered 0.1.0, was tagged on October 7th 2016. It already featured the main data types that are still the bread-and-butter of most Arrow datasets, as evidenced in this Flatbuffers declaration : /// ---------------------------------------------------------------------- /// Top-level Type value, enabling extensible type-specific metadata. We can /// add new logical types to Type without breaking backwards compatibility union Type { Null, Int, FloatingPoint, Binary, Utf8, Bool, Decimal, Date, Time, Timestamp, Interval, List, Struct_, Union } The release announcement made the bold claim that \"the metadata and physical data representation should be fairly stable as we have spent time finalizing the details\" . Does that promise hold? The short answer is: yes, almost! But let us analyse that in a bit more detail: the Columnar format , for the most part, has only seen additions of new datatypes since 2016. One single breaking change occurred: Union types cannot have a top-level validity bitmap anymore. the IPC format has seen several minor evolutions of its framing and metadata format; these evolutions are encoded in the MetadataVersion field which ensures that new readers can read data produced by old writers. The single breaking change is related to the same Union validity change mentioned above. First cross-language integration tests Arrow 0.1.0 had two implementations: C++ and Java, with bindings of the former to Python. There were also no integration tests to speak of, that is, no automated assessment that the two implementations were in sync (what could go wrong?). Integration tests had to wait for November 2016 to be designed, and the first automated CI run probably occurred in December of the same year. Its results cannot be fetched anymore, so we can only assume the tests passed successfully. ðŸ™‚ From that moment, integration tests have grown to follow additions to the Arrow format, while ensuring that older data can still be read successfully. For example, the integration tests that are routinely checked against multiple implementations of Arrow have data files generated in 2019 by Arrow 0.14.1 . No breaking changes... almost As mentioned above, at some point the Union type lost its top-level validity bitmap, breaking compatibility for the workloads that made use of this feature. This change was proposed back in June 2020 and enacted shortly thereafter. It elicited no controversy and doesn't seem to have caused any significant discontent among users, signaling that the feature was probably not widely used (if at all). Since then, there has been precisely zero breaking change in the Arrow Columnar and IPC formats. Apache Arrow 1.0.0 We have been extremely cautious with version numbering and waited until July 2020 before finally switching away from 0.x version numbers. This was signalling to the world that Arrow had reached its \"adult phase\" of making formal compatibility promises, and that the Arrow formats were ready for wide consumption amongst the data ecosystem. Apache Arrow, today Describing the breadth of the Arrow ecosystem today would take a full-fledged article of its own, or perhaps even multiple Wikipedia pages. Our \"powered by\" page can give a small taste. As for the Arrow project, we will merely refer you to our official documentation: The various specifications that cater to multiple aspects of sharing Arrow data, such as in-process zero-copy sharing between producers and consumers that know nothing about each other, or executing database queries that efficiently return their results in the Arrow format. The implementation status page that lists the implementations developed officially under the Apache Arrow umbrella (native software libraries for C, C++, C#, Go, Java, JavaScript, Julia, MATLAB, Python, R, Ruby, and Rust). But keep in mind that multiple third-party implementations exist in non-Apache projects, either open source or proprietary. However, that is only a small part of the landscape. The Arrow project hosts several official subprojects, such as ADBC and nanoarrow . A notable success story is Apache DataFusion , which began as an Arrow subproject and later graduated to become an independent top-level project in the Apache Software Foundation, reflecting the maturity and impact of the technology. Beyond these subprojects, many third-party efforts have adopted the Arrow formats for efficient interoperability. GeoArrow is an impressive example of how building on top of existing Arrow formats and implementations can enable groundbreaking efficiency improvements in a very non-trivial problem space. It should also be noted that Arrow, as an in-memory columnar format, is often used hand in hand with Parquet for persistent storage; as a matter of fact, most official Parquet implementations are nowadays being developed within Arrow repositories (C++, Rust, Go). Tomorrow The Apache Arrow community is primarily driven by consensus, and the project does not have a formal roadmap. We will continue to welcome everyone who wishes to participate constructively. While the specifications are stable, they still welcome additions to cater for new use cases, as they have done in the past. The Arrow implementations are actively maintained, gaining new features, bug fixes, and performance improvements. We encourage people to contribute to their implementation of choice, and to engage with us and the community . Now and going forward, a large amount of Arrow-related progress is happening in the broader ecosystem of third-party tools and libraries. It is no longer possible for us to keep track of all the work being done in those areas, but we are proud to see that they are building on the same stable foundations that have been laid 10 years ago.",
      "cover_image_url": "https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png"
    }
  ]
}