{
  "industry": "technology",
  "collected_at": "2026-02-22T10:54:00.419131+00:00",
  "hours": 24,
  "limit": 25,
  "count": 25,
  "items": [
    {
      "industry": "technology",
      "source": "Wired",
      "title": "Creatine Supplements Are Everywhere. Do I Need Them? (2026)",
      "url": "https://www.wired.com/story/should-you-be-taking-creatine-supplements/",
      "published": "2026-02-22T10:30:00+00:00",
      "summary": "It’s the most studied supplement in sports medicine, but it’s not just for athletes anymore.",
      "content_text": "Updated February 2026: I’ve added Onnit Creatine to our Honorable Mentions. We also updated links and prices. Creatine, Explained Creatine is a compound your body produces in the liver, kidneys, and pancreas, according to Federica Amati, a medical scientist and registered public health nutritionist. Most of it ends up in your skeletal muscles, where it’s stored as phosphocreatine and used to regenerate adenosine triphosphate (ATP), the molecule that powers muscle contractions, nerve signals, and protein synthesis for tissue repair. The rest— less than 5 percent —is found in brain tissue and the testes. You make about a gram of creatine a day from three amino acids: arginine, glycine, and methionine. You also get more from animal-based foods like red meat, fish, and poultry. For most healthy people, that’s enough. So creatine supplements aren’t essential for general health, but they can boost performance and recovery under the right conditions. How Creatine Works in the Body ATP is your cells’ main energy source. High-intensity exercise drains it fast, breaking ATP down into adenosine diphosphate (ADP) . Creatine phosphate donates a phosphate group to ADP, recycling it back into ATP almost immediately. The more phosphocreatine stored in your muscles, the faster you can regenerate ATP and the more power you can produce in short bursts. The Best Form of Creatine Photo by Steve Mitchell/EMPICS via Getty Images Nowadays, step into any GNC, and you’ll find several forms of creatine: creatine hydrochloride, magnesium creatine chelate, creatine citrate, creatine nitrate, creatine ethyl ester, and buffered creatine. But creatine monohydrate is the most studied, the most effective, and usually the cheapest, according to the International Society of Sports Nutrition . No other form has shown any additional benefits, confirms Amati. Creatine monohydrate is typically sold as a flavorless, white powder that you can mix into water or a shake. According to Amati, the standard dose is 3 to 5 grams per day. Some athletes may “load” with higher doses, but studies show that this offers no long-term benefits and can put unnecessary stress on the kidneys. Muscle Growth and Exercise Performance Creatine is most useful for activities that demand short, intense effort: sprinting, weightlifting, and high-intensity interval training, to name a few. It’s far less relevant for endurance sports like marathons. Some evidence suggests creatine also increases muscle glycogen storage, which could help with recovery and energy replenishment between sessions. Creatine doesn’t directly build muscle, but paired with resistance training and adequate nutrition, it can help preserve muscle strength and lean body mass. It’s particularly important if you’re recovering from an injury, or as you age , when sarcopenia (age-related muscle loss) becomes a risk, as it activates specialized stem cells known as satellite cells. Zimmermann points out that women, especially in perimenopause and menopause, may see health benefits. “As women get older—starting in our forties—we lose body mass 1 to 2 percent a year, and that can affect bone health later on in life,” says Zimmermann. “Creatine [supplements] support keeping and building lean muscle mass.” Zimmermann adds, “Women tend to have lower muscle mass than men, just naturally, so I think women may actually respond better to supplementation, because they’re at baseline.” Creatine and Brain Health While the effects of creatine supplementation on athletic performance are well-documented, its effects on mental performance are still emerging. Early findings are promising. Studies suggest it may reduce mental fatigue, especially during high-stress situations, such as sleep deprivation or exhaustive exercise. It may also improve certain aspects of memory, particularly in groups with lower baseline creatine levels, such as vegetarians and older adults. Some preliminary research even suggests it could help with symptoms of depression by supporting brain energy and boosting the production of feel-good neurotransmitters like dopamine and serotonin. That’s particularly relevant for women experiencing perimenopause and menopause, says Zimmermann. “Estrogen shifts affect brain health, our mood, brain fog, and our ability to think clearly.” Is Creatine Safe? For most healthy adults, creatine is a safe and well-tolerated supplement for months or even years of use. Short-term and long-term clinical trials have found no significant health risks. The main side effects are minor: weight gain from temporary water retention in the first week of supplementation, bloating, and mild gastric discomfort, usually from oversized doses. However, there are a few considerations to keep in mind:",
      "cover_image_url": "https://media.wired.com/photos/689bd2171585083b1b64223e/191:100/w_1280,c_limit/What%20Is%20Creatine,%20and%20Should%20You%20Be%20Taking%20It_.png"
    },
    {
      "industry": "technology",
      "source": "Wired",
      "title": "How to View the ‘Blood Moon’ Total Lunar Eclipse on March 3",
      "url": "https://www.wired.com/story/how-to-view-the-blood-moon-total-lunar-eclipse-on-march-3/",
      "published": "2026-02-22T10:00:00+00:00",
      "summary": "Next month, the Earth will come between the sun and the moon, causing the moon to take on an eerie reddish hue.",
      "content_text": "The first major astronomical event visible in 2026 is a total lunar eclipse , or “blood moon .” This phenomenon is highly prized by stargazers because the entire lunar disk takes on a reddish color for a few moments. The total lunar eclipse will occur on March 3. It will be clearly visible in North and Central America, while in Central and South Asia it will only be partially visible. It will not be visible in Europe or Africa. Although the eclipse will begin in the early morning, totality will occur almost at dawn on March 3. A few hours before sunrise, the full moon will take on its characteristic reddish color for just 12 minutes. Times of the Total Lunar Eclipse or “Blood Moon” Los Angeles: 3:04 am Denver: 4:04 am Chicago: 5:04 am St. Louis: 5:04 am New York: 6:04 am Washington, DC: 6:04 am It's safe to view a total lunar eclipse, unlike a solar eclipse. You don't need any special equipment; just go to a high vantage point and dress warmly. Keep in mind that at the time of totality, the moon will be almost touching the horizon, about to disappear. That's why it's necessary to view it from a high place, with no buildings or trees blocking your view. The little light from the sun filters through the Earth's atmosphere and impacts the moon, giving it its orange color. NASA Why Does the Moon Turn Red? During a total lunar eclipse, the moon does not lose its brightness completely, but takes on a dull red hue. This happens because the Earth is positioned between the sun and the moon and casts its shadow on the lunar surface. Unlike the moon, the Earth is surrounded by an atmosphere, which filters sunlight. Thanks to it, we see the blue sky, the orange sunset, or distant objects in opaque tones. The light coming from the sun and passing through the Earth's atmosphere manages to reach the moon, although in a smaller proportion. Those already filtered rays impact the satellite, painting it red. “It's as if all the world's sunrises and sunsets are projected onto the moon,” NASA explains. Total lunar eclipses are slightly rarer than total solar eclipses . A blood moon occurs every 2.5 years on average, while a total solar eclipse happens about every 18 months, according to NASA's astronomical catalogs. Total lunar eclipses seem more frequent because they can be observed from anywhere it is nighttime. In contrast, to see a total solar eclipse it is necessary to be exactly in the narrow band of totality. For example, an observer can see a total lunar eclipse every two to three years, but might have to wait about 375 years to see a total solar eclipse from his or her city. This story originally appeared on WIRED en Español and has been translated from Spanish.",
      "cover_image_url": "https://media.wired.com/photos/699872684fbc7cc192b292e0/191:100/w_1280,c_limit/GettyImages-1345490899.jpg"
    },
    {
      "industry": "technology",
      "source": "Wired",
      "title": "Best Electric Toothbrush, Backed by Real-Life Testing (2026)",
      "url": "https://www.wired.com/story/the-best-electric-toothbrush-for-healthier-teeth-and-gums/",
      "published": "2026-02-22T10:00:00+00:00",
      "summary": "How to choose a toothbrush that protects your smile and gum health.",
      "content_text": "Battery life is excellentâ€”lasting about two weeks per charge in our testsâ€”and unlike Oral-B, the 4100 shuts off after the two-minute timer. It's also designed with BrushSync technology, which tracks how long you've been using your brush head and how much pressure you're applying. A light on the handle and a beep remind you when it's time for a replacementâ€”a thoughtful touch. Unlike the Oral-B brush's standard black and white, the Sonicare 4100 also comes in pink, azure blue, and dark forest green, which might not seem like much, but even the smallest pop of color can make a mundane task feel a little less â€¦ mundane. Best Oscillating AccordionItemContainerButton WIRED High-power motor Built-in 2-minute timer with quadrant pulses Small round brush head to reach molars Durable build TIRED Shorter battery life than its competitors Can feel intense The Oral-B Pro 1000 has been around since 2012, outlasting and outperforming flashier models because it's powerful and priced right. The high-power motor stays focused on your teeth, so you won't feel that hand-numbing vibration along the handle. If you're switching from a manual toothbrush, the intensity might be startling initially, but you'll adjust. With just one button and three brushing modes (Daily Clean, Whiten, and Sensitive), the Pro 1000 keeps it simple. A built-in timer pulses every 30 seconds to remind you to move to another quadrant of your mouth, with a triple pulse at the two-minute mark. Unlike some competitors, it won't shut off automatically, which is handy if you like to keep brushing. Oral-B says the battery lasts about a week, but we squeezed out 10 days in testing. That's decent, though not as long as some other models. Like all of Oral-B's electric brushes, the Pro 1000 uses an oscillating brush head , which is a small, circular design that gets between teeth more effectively than larger oval-shaped bristles. Round brush heads tend to be easier to maneuver for those with smaller jaws, and they reach the back of the farthest molars. Though we haven't tried all of them, most Pro models are a good bet.",
      "cover_image_url": "https://media.wired.com/photos/699a7450b50ec92b680fbb35/191:100/w_1280,c_limit/The%20Best%20Electric%20Toothbrush%20for%20Healthier%20Teeth%20and%20Gums.png"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "Move over, Apple: Meet the alternative app stores available in the EU and elsewhere",
      "url": "https://techcrunch.com/2026/02/22/move-over-apple-meet-the-alternative-app-stores-available-in-the-eu-and-elsewhere/",
      "published": "2026-02-22T09:00:21+00:00",
      "summary": "A list of some of the alternative app stores iPhone users in the EU can try today.",
      "content_text": "People in the European Union are now allowed to access alternative app stores thanks to the Digital Markets Act (DMA), a regulation designed to foster increased competition in the app ecosystem. Like Apple’s App Store, alternative app marketplaces on allow for easy access to a wider world of apps on Apple devices, but instead of the apps going through Apple’s App Review process, the apps on these third-party marketplaces have to go through a notarization process to ensure they meet some “baseline platform integrity standards,” Apple says — like being malware-free. However, each store can review and approve apps according to its own policies. The stores are also responsible for any matters relating to support and refunds, not Apple. To run an alternative app marketplace, developers must accept Apple’s alternative business terms for DMA-compliant apps in the EU. This includes paying a new Core Technology Fee of €0.50 for each first annual install of their marketplace app, even before the threshold of 1 million installs is met, which is the bar for other EU apps distributed under Apple’s DMA business terms. Despite the complicated new rules, a handful of developers have taken advantage of the opportunity to distribute their apps outside of Apple’s walls. Beyond the EU, other markets are experimenting with alternative app stores, as well, like Japan. In December 2025, Apple announced its compliance with the Mobile Software Competition Act (MSCA), which gives developers new options to distribute apps and process payments outside of Apple’s App Store. This option also requires developers to accept new business terms , like a reduced 10% to 21% App Store commission, a payment processing fee for Apple in-app purchases of 5%, a core technology fee of 5%, and a 15% store services commission on web sales made through a link in the app. Below is a list of the alternative app stores iPhone users in these markets can try today. AltStore PAL (EU) Image Credits: AltStore Co-created by developer Riley Testut, maker of the Nintendo game emulator app Delta , the AltStore PAL is an officially approved alternative app marketplace in the EU. The open source app store will allow independent developers to distribute their apps alongside the apps from AltStore’s makers, Delta, and a clipboard manager, called Clip . Techcrunch event Boston, MA | June 9, 2026 Unlike Apple’s App Store, AltStore apps are self-hosted by the developer. To work, developers download an alternative distribution packet (ADP) and upload it to their server, then create a “source” that users will add to the AltStore to access their apps. That means the only apps you’ll see in the AltStore are those you’ve added yourselves. Some popular apps that users are adding include the virtual machine app UTM , which lets you run Windows and other software on iOS or iPad; OldOS , a re-creation of iOS 4 that’s built in SwiftUI; Kotoba , the iOS dictionary available as a stand-alone app; torrenting app iTorrent ; qBittorrent remote client for iOS devices called qBitControl ; and social discovery platform PeopleDrop . Setapp Mobile (EU – closed Feb. 2026) Image Credits: Setapp MacPaw’s Setapp became one of the first companies to agree to Apple’s new DMA business terms to set up an alternative app store for EU users. Unfortunately, this app store didn’t last long — the company announced it would sunset the Setapp Mobile service on February 16, 2026. (Applications on Setapp Desktop weren’t affected.) The company cited Apple’s “still-evolving” and complex business terms as the reason for its decision. The company had long offered a subscription-based service featuring a selection of curated apps for customers on iOS and Mac. Following the implementation of the DMA, it released the alternative app store for Setapp Mobile for iOS users only in the EU. Similar to its other subscription offerings, the now-shuttered app store had included dozens of apps under a single recurring subscription price, and the number of apps grew over time. The apps were free from in-app purchases or ads and are generally considered high quality. However, it didn’t include big-name apps like Facebook, Uber, Netflix, and others. Epic Games Store (EU) VIDEO Fortnite maker Epic Games launched its alternative iOS app store in the EU in August 2024, allowing users to download games, including its own Fortnite and others like Rocket League Sideswipe and Fall Guys, with more to come. The company said it’s also bringing its games to other alternative app stores, including AltStore PAL, which it’s now supporting via a grant, as well as Aptoide’s iOS store in the EU and ONE Store on Android. The move to launch Fortnite in alternative iOS marketplaces comes more than four years after Apple removed the game from its App Store over policy violations, ahead of Epic’s legal challenge to the alleged App Store monopoly. While U.S. courts decided that Apple was not engaged in antitrust behavior, the lawsuit did pave the way for developers to link to their own websites for a reduced commission. Aptoide (EU) Image Credits: Aptoide An alternative game store for iPhone, Lisbon-based Aptoide is an open source solution for app distribution. The company, already known for its Google Play alternative, says it scans the apps to ensure they are safe to download and install. The iOS version of the Aptoide store launched as an invite-only beta in June 2024 before becoming available to all across the EU. As a free-to-use store, Aptoide doesn’t charge its users to cover its Core Technology Fee paid to Apple, but takes a 10% to 20% commission on in-app purchases on iOS, depending on whether they were generated by the marketplace or not. Across all platforms, including Android, web, car, and TV, Aptoide offers 1 million apps to its more than 430 million users. Mobivention marketplace (EU) Image Credits: Mobivention A B2B-focused app store, the Mobivention marketplace allows EU companies to distribute their internal apps that are used by employees, but can’t — or shouldn’t — be published in Apple’s App Store. The company also offers the development of a customized app marketplace for companies that want to offer employees their own app store just for their corporate apps. Larger companies can even license Mobivention’s technology to more deeply customize the app marketplace to their own needs. Skich (EU) Image Credits: Skich Last March, Skich announced the launch of an alternative app store for EU users, which differentiates itself by offering a Tinder-like interface for app discovery. That is, users swipe right to “match” with apps they might enjoy. They can also create playlists and see what apps their friends are playing. The new store will replace Skich’s existing app and will see the company taking a 15% commission on all purchases. Instead of filling its app store with apps right away, the store marketed to developers at the Game Developers Conference ( GDC ). Onside (EU and Japan) Onside Onside is an alternative iOS app store available in both the EU and, now, Japan, as of February 17, 2026, thanks to the new regulations. The company promises it will charge developers lower rates while still offering security, including keeping payment information private. The store currently supports bank card payments and Apple Pay and will later roll out support for other payment methods like iDeal, Klarna, and more. For consumers, Onside touts a range of top apps and exclusives that can’t be found on other marketplaces within a familiar interface that includes traditional app store features, like editorial collections, ratings and reviews, and automatic updates.",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2024/04/altstore.png?resize=1200,490"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Back to FreeBSD: Part 1",
      "url": "https://hypha.pub/back-to-freebsd-part-1",
      "published": "2026-02-22T07:16:27+00:00",
      "summary": "<p>Article URL: <a href=\"https://hypha.pub/back-to-freebsd-part-1\">https://hypha.pub/back-to-freebsd-part-1</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47108989\">https://news.ycombinator.com/item?id=47108989</a></p> <p>Points: 22</p> <p># Comments: 3</p>",
      "content_text": "<p>Article URL: <a href=\"https://hypha.pub/back-to-freebsd-part-1\">https://hypha.pub/back-to-freebsd-part-1</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47108989\">https://news.ycombinator.com/item?id=47108989</a></p> <p>Points: 22</p> <p># Comments: 3</p>",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "What's the best way to learn a new language?",
      "url": "https://www.bbc.com/future/article/20260220-whats-the-best-way-to-learn-a-new-language",
      "published": "2026-02-22T07:14:22+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.bbc.com/future/article/20260220-whats-the-best-way-to-learn-a-new-language\">https://www.bbc.com/future/article/20260220-whats-the-best-way-to-learn-a-new-language</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47108977\">https://news.ycombinator.com/item?id=47108977</a></p> <p>Points: 5</p> <p># Comments: 0</p>",
      "content_text": "The benefits of language-learning for our long-term brain health and happiness are well noted, so no regrets there. But had my four years of studying a language to degree level conjugating verbs and memorising vocabulary become an outdated way of learning? (Read more about the benefits of bilingualism here ). Krupa Padhy Krupa Padhy during her year abroad in Paris where she worked in a high school (Credit: Krupa Padhy) Along with the promise of becoming fluent at lightning speed, a range of new methods and technologies have transformed how we pick up languages in an increasingly time-poor age. One is \"microlearning\", an approach that breaks down new information into small chunks that are meant to be absorbed quickly, sometimes within minutes or even seconds. It's rooted in a concept known as the forgetting curve , which states that when people take in large amounts of information, they remember less of it over time. In addition, there's a wealth of new technologies, from chatbots offering instant feedback , to virtual reality and augmented reality technologies which drop you into conversations with virtual native speakers. However, some argue that the promise of fast fluency misses crucial elements of actually learning to speak to people in another language, such as developing cultural understanding and nuance. So, with all this choice, what's actually the best, science-backed way to learn a language? To find out, I teamed up with two researchers at Lancaster University's Language Learning Lab: Patrick Rebuschat, a professor of linguistics and cognitive science, and Padraic Monaghan, a professor of cognition in the department of psychology. They let me try out an experiment they designed to mirror language-learning in the real world, and reveal how our brain picks up and makes sense of new words and sounds. The tasks basically simulate how we would cope if we were dropped into a foreign country with an unknown language, and just had to use our innate skills to figure out the new, mysterious sounds around us, and start to make sense of them.",
      "cover_image_url": "https://ychef.files.bbci.co.uk/624x351/p0n28j0x.jpg"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Japanese Print Search and Database",
      "url": "https://ukiyo-e.org/",
      "published": "2026-02-22T03:18:36+00:00",
      "summary": "<p>Article URL: <a href=\"https://ukiyo-e.org/\">https://ukiyo-e.org/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47107781\">https://news.ycombinator.com/item?id=47107781</a></p> <p>Points: 104</p> <p># Comments: 18</p>",
      "content_text": "Japanese Woodblock Print Search Ukiyo-e Search provides an incredible resource: The ability to both search for Japanese woodblock prints by simply taking a picture of an existing print AND the ability to see similar prints across multiple collections of prints. Below is an example print, click to see it in action.",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "A Botnet Accidentally Destroyed I2P (The Full Story)",
      "url": "https://www.sambent.com/a-botnet-accidentally-destroyed-i2p-the-full-story/",
      "published": "2026-02-22T01:08:17+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.sambent.com/a-botnet-accidentally-destroyed-i2p-the-full-story/\">https://www.sambent.com/a-botnet-accidentally-destroyed-i2p-the-full-story/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47106985\">https://news.ycombinator.com/item?id=47106985</a></p> <p>Points: 111</p> <p># Comments: 68</p>",
      "content_text": "VIDEO On February 3, 2026, the I2P anonymity network was flooded with 700,000 hostile nodes in what became one of the most devastating Sybil attacks an anonymity network has ever experienced. The network normally operates with 15,000 to 20,000 active devices. The attackers overwhelmed it by a factor of 39 to 1. For three consecutive years, I2P has been hit with Sybil attacks every February. The 2023 and 2024 attacks used malicious floodfill routers and remain unattributed. When the 2026 attack began, most assumed it was the same state-sponsored operation continuing its annual disruption campaign. The assumption was wrong. The attacker was identified as the Kimwolf botnet, an IoT botnet that infected millions of devices including streaming boxes and consumer routers throughout late 2025. Kimwolf is the same operation behind the record-setting 31.4 terabit per second DDoS attack in December 2025. The operators admitted on Discord they accidentally disrupted I2P while attempting to use the network as backup command-and-control infrastructure after security researchers destroyed over 550 of their primary C2 servers. The I2P development team responded by shipping version 2.11.0 just six days after the attack began. The release includes hybrid ML-KEM plus X25519 post-quantum encryption enabled by default, making I2P one of the first production anonymity networks to ship post-quantum cryptography to all users. Additional Sybil mitigations, SAMv3 API upgrades, and infrastructure improvements were included.",
      "cover_image_url": "https://www.sambent.com/content/images/size/w1200/2026/02/A-Botnet-Accidentally-Destroyed-I2P--The-Full-Story-.jpg"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "How I Use Claude Code",
      "url": "https://boristane.com/blog/how-i-use-claude-code/",
      "published": "2026-02-22T00:29:05+00:00",
      "summary": "<p>Article URL: <a href=\"https://boristane.com/blog/how-i-use-claude-code/\">https://boristane.com/blog/how-i-use-claude-code/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47106686\">https://news.ycombinator.com/item?id=47106686</a></p> <p>Points: 552</p> <p># Comments: 333</p>",
      "content_text": "I’ve been using Claude Code as my primary development tool for approx 9 months, and the workflow I’ve settled into is radically different from what most people do with AI coding tools. Most developers type a prompt, sometimes use plan mode, fix the errors, repeat. The more terminally online are stitching together ralph loops, mcps, gas towns (remember those?), etc. The results in both cases are a mess that completely falls apart for anything non-trivial. The workflow I’m going to describe has one core principle: never let Claude write code until you’ve reviewed and approved a written plan . This separation of planning and execution is the single most important thing I do. It prevents wasted effort, keeps me in control of architecture decisions, and produces significantly better results with minimal token usage than jumping straight to code. flowchart LR R[Research] --> P[Plan] P --> A[Annotate] A -->|repeat 1-6x| A A --> T[Todo List] T --> I[Implement] I --> F[Feedback & Iterate] Phase 1: Research Every meaningful task starts with a deep-read directive. I ask Claude to thoroughly understand the relevant part of the codebase before doing anything else. And I always require the findings to be written into a persistent markdown file, never just a verbal summary in the chat. read this folder in depth, understand how it works deeply, what it does and all its specificities. when that’s done, write a detailed report of your learnings and findings in research.md study the notification system in great details, understand the intricacies of it and write a detailed research.md document with everything there is to know about how notifications work go through the task scheduling flow, understand it deeply and look for potential bugs. there definitely are bugs in the system as it sometimes runs tasks that should have been cancelled. keep researching the flow until you find all the bugs, don’t stop until all the bugs are found. when you’re done, write a detailed report of your findings in research.md Notice the language: “deeply” , “in great details” , “intricacies” , “go through everything” . This isn’t fluff. Without these words, Claude will skim. It’ll read a file, see what a function does at the signature level, and move on. You need to signal that surface-level reading is not acceptable. The written artifact ( research.md ) is critical. It’s not about making Claude do homework. It’s my review surface. I can read it, verify Claude actually understood the system, and correct misunderstandings before any planning happens. If the research is wrong, the plan will be wrong, and the implementation will be wrong. Garbage in, garbage out. This is the most expensive failure mode with AI-assisted coding, and it’s not wrong syntax or bad logic. It’s implementations that work in isolation but break the surrounding system. A function that ignores an existing caching layer. A migration that doesn’t account for the ORM’s conventions. An API endpoint that duplicates logic that already exists elsewhere. The research phase prevents all of this. Phase 2: Planning Once I’ve reviewed the research, I ask for a detailed implementation plan in a separate markdown file. I want to build a new feature <name and description> that extends the system to perform <business outcome>. write a detailed plan.md document outlining how to implement this. include code snippets the list endpoint should support cursor-based pagination instead of offset. write a detailed plan.md for how to achieve this. read source files before suggesting changes, base the plan on the actual codebase The generated plan always includes a detailed explanation of the approach, code snippets showing the actual changes, file paths that will be modified, and considerations and trade-offs. I use my own .md plan files rather than Claude Code’s built-in plan mode. The built-in plan mode sucks. My markdown file gives me full control. I can edit it in my editor, add inline notes, and it persists as a real artifact in the project. One trick I use constantly: for well-contained features where I’ve seen a good implementation in an open source repo, I’ll share that code as a reference alongside the plan request. If I want to add sortable IDs, I paste the ID generation code from a project that does it well and say “this is how they do sortable IDs, write a plan.md explaining how we can adopt a similar approach.” Claude works dramatically better when it has a concrete reference implementation to work from rather than designing from scratch. But the plan document itself isn’t the interesting part. The interesting part is what happens next. The Annotation Cycle This is the most distinctive part of my workflow, and the part where I add the most value. flowchart TD W[Claude writes plan.md] --> R[I review in my editor] R --> N[I add inline notes] N --> S[Send Claude back to the document] S --> U[Claude updates plan] U --> D{Satisfied?} D -->|No| R D -->|Yes| T[Request todo list] After Claude writes the plan, I open it in my editor and add inline notes directly into the document . These notes correct assumptions, reject approaches, add constraints, or provide domain knowledge that Claude doesn’t have. The notes vary wildly in length. Sometimes a note is two words: “not optional” next to a parameter Claude marked as optional. Other times it’s a paragraph explaining a business constraint or pasting a code snippet showing the data shape I expect. Some real examples of notes I’d add: “use drizzle:generate for migrations, not raw SQL” — domain knowledge Claude doesn’t have “no — this should be a PATCH, not a PUT” — correcting a wrong assumption “remove this section entirely, we don’t need caching here” — rejecting a proposed approach “the queue consumer already handles retries, so this retry logic is redundant. remove it and just let it fail” — explaining why something should change “this is wrong, the visibility field needs to be on the list itself, not on individual items. when a list is public, all items are public. restructure the schema section accordingly” — redirecting an entire section of the plan Then I send Claude back to the document: I added a few notes to the document, address all the notes and update the document accordingly. don’t implement yet This cycle repeats 1 to 6 times. The explicit “don’t implement yet” guard is essential. Without it, Claude will jump to code the moment it thinks the plan is good enough. It’s not good enough until I say it is. Why This Works So Well The markdown file acts as shared mutable state between me and Claude. I can think at my own pace, annotate precisely where something is wrong, and re-engage without losing context. I’m not trying to explain everything in a chat message. I’m pointing at the exact spot in the document where the issue is and writing my correction right there. This is fundamentally different from trying to steer implementation through chat messages. The plan is a structured, complete specification I can review holistically. A chat conversation is something I’d have to scroll through to reconstruct decisions. The plan wins every time. Three rounds of “I added notes, update the plan” can transform a generic implementation plan into one that fits perfectly into the existing system. Claude is excellent at understanding code, proposing solutions, and writing implementations. But it doesn’t know my product priorities, my users’ pain points, or the engineering trade-offs I’m willing to make. The annotation cycle is how I inject that judgement. The Todo List Before implementation starts, I always request a granular task breakdown: add a detailed todo list to the plan, with all the phases and individual tasks necessary to complete the plan - don’t implement yet This creates a checklist that serves as a progress tracker during implementation. Claude marks items as completed as it goes, so I can glance at the plan at any point and see exactly where things stand. Especially valuable in sessions that run for hours. Phase 3: Implementation When the plan is ready, I issue the implementation command. I’ve refined this into a standard prompt I reuse across sessions: implement it all. when you’re done with a task or phase, mark it as completed in the plan document. do not stop until all tasks and phases are completed. do not add unnecessary comments or jsdocs, do not use any or unknown types. continuously run typecheck to make sure you’re not introducing new issues. This single prompt encodes everything that matters: “implement it all” : do everything in the plan, don’t cherry-pick “mark it as completed in the plan document” : the plan is the source of truth for progress “do not stop until all tasks and phases are completed” : don’t pause for confirmation mid-flow “do not add unnecessary comments or jsdocs” : keep the code clean “do not use any or unknown types” : maintain strict typing “continuously run typecheck” : catch problems early, not at the end I use this exact phrasing (with minor variations) in virtually every implementation session. By the time I say “implement it all,” every decision has been made and validated. The implementation becomes mechanical, not creative. This is deliberate. I want implementation to be boring . The creative work happened in the annotation cycles. Once the plan is right, execution should be straightforward. Without the planning phase, what typically happens is Claude makes a reasonable-but-wrong assumption early on, builds on top of it for 15 minutes, and then I have to unwind a chain of changes. The “don’t implement yet” guard eliminates this entirely. Feedback During Implementation Once Claude is executing the plan, my role shifts from architect to supervisor. My prompts become dramatically shorter. flowchart LR I[Claude implements] --> R[I review / test] R --> C{Correct?} C -->|No| F[Terse correction] F --> I C -->|Yes| N{More tasks?} N -->|Yes| I N -->|No| D[Done] Where a planning note might be a paragraph, an implementation correction is often a single sentence: “You didn’t implement the deduplicateByTitle function.” “You built the settings page in the main app when it should be in the admin app, move it.” Claude has the full context of the plan and the ongoing session, so terse corrections are enough. Frontend work is the most iterative part. I test in the browser and fire off rapid corrections: “wider” “still cropped” “there’s a 2px gap” For visual issues, I sometimes attach screenshots. A screenshot of a misaligned table communicates the problem faster than describing it. I also reference existing code constantly: “this table should look exactly like the users table, same header, same pagination, same row density.” This is far more precise than describing a design from scratch. Most features in a mature codebase are variations on existing patterns. A new settings page should look like the existing settings pages. Pointing to the reference communicates all the implicit requirements without spelling them out. Claude would typically read the reference file(s) before making the correction. When something goes in a wrong direction, I don’t try to patch it. I revert and re-scope by discarding the git changes: “I reverted everything. Now all I want is to make the list view more minimal — nothing else.” Narrowing scope after a revert almost always produces better results than trying to incrementally fix a bad approach. Staying in the Driver’s Seat Even though I delegate execution to Claude, I never give it total autonomy over what gets built . I do the vast majority of the active steering in the plan.md documents. This matters because Claude will sometimes propose solutions that are technically correct but wrong for the project. Maybe the approach is over-engineered, or it changes a public API signature that other parts of the system depend on, or it picks a more complex option when a simpler one would do. I have context about the broader system, the product direction, and the engineering culture that Claude doesn’t. flowchart TD P[Claude proposes changes] --> E[I evaluate each item] E --> A[Accept as-is] E --> M[Modify approach] E --> S[Skip / remove] E --> O[Override technical choice] A & M & S & O --> R[Refined implementation scope] Cherry-picking from proposals: When Claude identifies multiple issues, I go through them one by one: “for the first one, just use Promise.all, don’t make it overly complicated; for the third one, extract it into a separate function for readability; ignore the fourth and fifth ones, they’re not worth the complexity.” I’m making item-level decisions based on my knowledge of what matters right now. Trimming scope: When the plan includes nice-to-haves, I actively cut them. “remove the download feature from the plan, I don’t want to implement this now.” This prevents scope creep. Protecting existing interfaces: I set hard constraints when I know something shouldn’t change: “the signatures of these three functions should not change, the caller should adapt, not the library.” Overriding technical choices: Sometimes I have a specific preference Claude wouldn’t know about: “use this model instead of that one” or “use this library’s built-in method instead of writing a custom one.” Fast, direct overrides. Claude handles the mechanical execution, while I make the judgement calls. The plan captures the big decisions upfront, and selective guidance handles the smaller ones that emerge during implementation. Single Long Sessions I run research, planning, and implementation in a single long session rather than splitting them across separate sessions. A single session might start with deep-reading a folder, go through three rounds of plan annotation, then run the full implementation, all in one continuous conversation. I am not seeing the performance degradation everyone talks about after 50% context window. Actually, by the time I say “implement it all,” Claude has spent the entire session building understanding: reading files during research, refining its mental model during annotation cycles, absorbing my domain knowledge corrections. When the context window fills up, Claude’s auto-compaction maintains enough context to keep going. And the plan document, the persistent artifact, survives compaction in full fidelity. I can point Claude to it at any point in time. The Workflow in One Sentence Read deeply, write a plan, annotate the plan until it’s right, then let Claude execute the whole thing without stopping, checking types along the way. That’s it. No magic prompts, no elaborate system instructions, no clever hacks. Just a disciplined pipeline that separates thinking from typing. The research prevents Claude from making ignorant changes. The plan prevents it from making wrong changes. The annotation cycle injects my judgement. And the implementation command lets it run without interruption once every decision has been made. Try my workflow, you’ll wonder how you ever shipped anything with coding agents without an annotated plan document sitting between you and the code.",
      "cover_image_url": "https://boristane.com/assets/blog/how-i-use-claude-code/og.png"
    },
    {
      "industry": "technology",
      "source": "Ars Technica",
      "title": "NASA says it needs to haul the Artemis II rocket back to the hangar for repairs",
      "url": "https://arstechnica.com/space/2026/02/nasa-says-it-needs-to-haul-the-artemis-ii-rocket-back-to-the-hangar-for-repairs/",
      "published": "2026-02-21T23:54:38+00:00",
      "summary": "\"Accessing and remediating any of these issues can only be performed in the VAB.\"",
      "content_text": "The helium system on the SLS upper stage —officially known as the Interim Cryogenic Propulsion Stage (ICPS) —performed well during both of the Artemis II countdown rehearsals. “ Last evening, the team was unable to get helium flow through the vehicle. This occurred during a routine operation to repressurize the system,” Isaacman wrote. The Space Launch System rocket emerges from the Vehicle Assembly Building to begin the rollout to Launch Pad 39B last month. Credit: Stephen Clark/Ars Technica The Space Launch System rocket emerges from the Vehicle Assembly Building to begin the rollout to Launch Pad 39B last month. Credit: Stephen Clark/Ars Technica Another molecule, another problem Helium is used to purge the upper stage engine and pressurize its propellant tanks. The rocket is in a “safe configuration,” with a backup system providing purge air to the upper stage, NASA said in a statement. NASA encountered a similar failure signature during preparations for launch of the first SLS rocket on the Artemis I mission in 2022. On Artemis I, engineers traced the problem to a failed check valve on the upper stage that needed replacement. NASA officials are not sure yet whether the helium issue Friday was caused by a similar valve failure, a problem with an umbilical interface between the rocket and the launch tower, or a fault with a filter, according to Isaacman. In any case, technicians are unable to reach the problem area with the rocket at the launch pad. Inside the VAB, ground teams will extend work platforms around the rocket to provide physical access to the upper stage and its associated umbilical connections. NASA said moving into preparations for rollback now will allow managers to potentially preserve the April launch window, “pending the outcome of data findings, repair efforts, and how the schedule comes to fruition in the coming days and weeks.” It’s not clear if NASA will perform another fueling test on the SLS rocket after it returns to Launch Pad 39B, or whether technicians will do any more work on the delicate hydrogen umbilical near the bottom of the rocket responsible for recurring leaks during the Artemis I and Artemis II launch campaigns. Managers were pleased with the performance of newly-installed seals during Thursday’s countdown demonstration, but NASA officials have previously said vibrations from transporting the rocket to and from the pad could damage the seals.",
      "cover_image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/IMG_0016-1-1152x648-1771717658.jpg"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Evidence of the bouba-kiki effect in naïve baby chicks",
      "url": "https://www.science.org/doi/10.1126/science.adq7188",
      "published": "2026-02-21T21:51:58+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.science.org/doi/10.1126/science.adq7188\">https://www.science.org/doi/10.1126/science.adq7188</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47105198\">https://news.ycombinator.com/item?id=47105198</a></p> <p>Points: 134</p> <p># Comments: 39</p>",
      "content_text": "<p>Article URL: <a href=\"https://www.science.org/doi/10.1126/science.adq7188\">https://www.science.org/doi/10.1126/science.adq7188</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47105198\">https://news.ycombinator.com/item?id=47105198</a></p> <p>Points: 134</p> <p># Comments: 39</p>",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "Sam Altman would like remind you that humans use a lot of energy, too",
      "url": "https://techcrunch.com/2026/02/21/sam-altman-would-like-remind-you-that-humans-use-a-lot-of-energy-too/",
      "published": "2026-02-21T21:38:01+00:00",
      "summary": "\"It also takes a lot of energy to train a human.\"",
      "content_text": "OpenAI CEO Sam Altman addressed concerns about AI’s environmental impact this week while speaking at an event hosted by The Indian Express . For one thing, Altman — who was in India for a major AI summit — said concerns about AI’s water usage are “totally fake,” though he acknowledged it was a real issue when “we used to do evaporative cooling in data centers.” “Now that we don’t do that, you see these things on the internet where, ‘Don’t use ChatGPT, it’s 17 gallons of water for each query’ or whatever,” Altman said. “This is completely untrue, totally insane, no connection to reality.” He added that it’s “fair” to be concerned about “the energy consumption — not per query, but in total, because the world is now using so much AI.” In his view, this means the world needs to “move towards nuclear or wind and solar very quickly.” There’s no legal requirement for tech companies to disclose how much energy and water they use, so scientists have been trying to study it independently . Data centers have also been connected to rising electricity prices . Citing a previous conversation with Bill Gates, the interviewer asked whether it’s accurate to say a single ChatGPT query currently uses the equivalent of 1.5 iPhone battery charges, to which Altman replied, “There’s no way it’s anything close to that much.” Altman also complained that many discussions about ChatGPT’s energy usage are “unfair,” especially when they focus on “how much energy it takes to train an AI model, relative to how much it costs a human to do one inference query.” Techcrunch event Boston, MA | June 9, 2026 “But it also takes a lot of energy to train a human,” Altman said. “It takes like 20 years of life and all of the food you eat during that time before you get smart. And not only that, it took the very widespread evolution of the 100 billion people that have ever lived and learned not to get eaten by predators and learned how to figure out science and whatever, to produce you.” So in his view, the fair comparison is, “If you ask ChatGPT a question, how much energy does it take once its model is trained to answer that question versus a human? And probably, AI has already caught up on an energy efficiency basis, measured that way.” You can watch the full interview below. The conversation about water and energy usage begins at around 26:35. VIDEO",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2198353376.jpg?resize=1200,800"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "Arturia’s FX Collection 6 adds two new effects and a $99 intro version",
      "url": "https://www.theverge.com/tech/882852/arturia-fx-collection-6",
      "published": "2026-02-21T21:10:01+00:00",
      "summary": "Arturia launched a new version of its flagship effects suite, FX Collection, which includes two new plugins, EFX Ambient and Pitch Shifter-910. FX Collection 6 also marks the introduction of an Intro version with a selection of six effects covering the basics for $99. That pales in comparison to the 39 effects in the full [&#8230;]",
      "content_text": "Arturia launched a new version of its flagship effects suite, FX Collection, which includes two new plugins, EFX Ambient and Pitch Shifter-910. FX Collection 6 also marks the introduction of an Intro version with a selection of six effects covering the basics for $99. That pales in comparison to the 39 effects in the full FX Collection Pro, but that also costs $499. Pitch Shifter-910 is based on the iconic Eventide H910 Harmonizer from 1974, an early digital pitchshifter and delay with a very unique character. Arturia does an admirable job preserving its glitchy quirks. Pitch Shifter-910 is not a transparent effect that lets you create natural-sounding harmonies with yourself. Instead, it relishes in its weirdness, delivering chipmunk vocals at the higher ranges. There is also a more modern mode that cleans up some artifacts while preserving what makes the 910 so special. Though if you ask me, it also takes some of the fun and unpredictability out. EFX Ambient is the other new addition to Arturia’s lineup, and it’s a weird one. While it does what it says on the tin, it doesn’t always do it in predictable ways. Sure, there’s plenty of big ethereal reverbs and shimmer, but there’s also resonators, glitch processing, and reverse delays. It has six distinct modes with unique characteristics, which it feeds through a big washy reverb. And there’s an X/Y control in the middle for adding movement to your sound. Neither of the brand-new effects made the cut for the Intro version. FX Collection 6 Intro includes Efx Motions, Efx Fragments, Mix Drums, Tape Mello-Fi, Rev Plate-140, and Delay Tape-201. That offers excellent versatility covering delay, reverb, tape-like lo-fi, modulation, and even granular processing. Primarily, what you miss out on are some of the saturation and mixing effects like bus and compression, as well as the more specialty flavors of delay and reverb like Rev LX-24, based on the Lexicon 224 from 1978. $499 for the full FX Collection 6 Pro might seem steep, but as the company has grown the lineup from 15 effects in 2020 to 39 in 2026, it’s become a more attractive value proposition. And, while it’s not quite as highly regarded as Arturia’s V Collection of soft synths, it’s building a reputation for high-quality effects.",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/function.png?quality=90&strip=all&crop=0%2C0%2C100%2C61.479993867082&w=1200"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "xaskasdf/ntransformer: High-efficiency LLM inference engine in C++/CUDA. Run Llama 70B on RTX 3090.",
      "url": "https://github.com/xaskasdf/ntransformer",
      "published": "2026-02-21T20:57:30+00:00",
      "summary": "<p>Hi everyone, I'm kinda involved in some retrogaming and with some experiments I ran into the following question: \"It would be possible to run transformer models bypassing the cpu/ram, connecting the gpu to the nvme?\"<p>This is the result of that question itself and some weekend vibecoding (it has the linked library repository in the readme as well), it seems to work, even on consumer gpus, it should work better on professional ones tho</p> <hr /> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47104667\">https://news.ycombinator.com/item?id=47104667</a></p> <p>Points: 250</p> <p># Comments: 59</p>",
      "content_text": "You can’t perform that action at this time.",
      "cover_image_url": "https://opengraph.githubassets.com/afb3e95c00bef66fbef06195f20f1028fd93d1b2e699fae2d1dbfe2f18f01d92/xaskasdf/ntransformer"
    },
    {
      "industry": "technology",
      "source": "Wired",
      "title": "Sony’s WH-CH720N headphones offer excellent value at full price, but right now they're a steal.",
      "url": "https://www.wired.com/story/sony-wh-ch720n-deal-february-2026/",
      "published": "2026-02-21T20:30:46+00:00",
      "summary": "Sony’s WH-CH720N headphones offer excellent value at full price, but right now they're a steal.",
      "content_text": "We've tested oodles of noise-canceling headphones and the Sony WH-CH720N might have an unfortunate name, but they're the best budget-friendly pair we've tried. They usually offer good value when selling for the full $178 MSRP, but right now they've fallen to $95 shipped on Amazon and $100 on Best Buy . These headphones are well-built and well-designed, with great active noise cancellation and robust sound. They don't fold up and they don't come with a case, but you can get a case as a separate purchase if that's a deal-breaker for you. These are lightweight, with adaptive sound that can adjust itself to suit your environment. Moreover, if you want a pair of over-hear wireless headphones with active noise cancellation, it's very difficult to get that in a package this affordable. Tack on the long-lasting 35-hour battery, and paying under $100 becomes a no-brainer if you're in the market and on a tight budget. We haven't seen them drop this low in price before. We're nowhere near a shopping event like Amazon Prime Day or Black Friday, but this is just one of several headphone deals we've spotted recently. Check those stories out if you're on the hunt for wireless gaming earbuds or open earbuds .",
      "cover_image_url": "https://media.wired.com/photos/6998c3349043220668a7e874/191:100/w_1280,c_limit/The%20Best%20Cheap%20Noise-Canceling%20Headphones%20Are%20More%20Affordable%20Than%20Ever.png"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "Wikipedia blacklists Archive.today after alleged DDoS attack",
      "url": "https://techcrunch.com/2026/02/21/wikipedia-blacklists-archive-today-after-alleged-ddos-attack/",
      "published": "2026-02-21T20:20:15+00:00",
      "summary": "Wikipedia editors have decided to remove all links to Archive.today, a web archiving service that they said has been linked to more than 695,000 times across the online encyclopedia.",
      "content_text": "Wikipedia editors have decided to remove all links to Archive.today, a web archiving service that they said has been linked to more than 695,000 times across the online encyclopedia. Archive.today — which also operates under several other domain names, including archive.is and archive.ph — is perhaps most widely used to access content that’s otherwise inaccessible behind paywalls. That also makes it useful as a source for Wikipedia citations. However, according to the Wikipedia discussion page about this topic , “There is consensus to immediately deprecate archive.today, and, as soon as practicable, add it to the spam blacklist […] and to forthwith remove all links to it.” (Ars Technica first reported on the decision .) The discussion page says that Archive.today was previously blacklisted in 2013, only to be removed from the blacklist in 2016. Why reverse course again? Because, the discussion page says, “Wikipedia should not direct its readers towards a website that hijacks users’ computers to run a DDoS attack.” Plus, “evidence has been presented that archive.today’s operators have altered the content of archived pages, rendering it unreliable.” The distributed denial of service (DDoS) attack in question was allegedly directed at blogger Jani Patokallio. Patokallio wrote that beginning on January 11, users who loaded the archive’s CAPTCHA page have been unknowingly loading and executing JavaScript that sends a search request to his Gyrovague blog, in an apparent attempt to get Patokallio’s attention and increase his hosting bill. Back in 2023, Patokallio published a blog post examining Archive.today, whose ownership he described as “an opaque mystery.” And while he wasn’t able to track down a specific owner, he concluded the site was likely “a one-person labor of love, operated by a Russian of considerable talent and access to Europe.” Techcrunch event Boston, MA | June 9, 2026 More recently, Patokallio said the webmaster at Archive.today asked him to take the post down for two or three months. “I do not mind the post, but the issue is: journos from mainstream media (Heise, Verge, etc) cherry-pick just a couple of words from your blog, and then construct very different narratives having your post the only citable source; then they cite each other and produce a shitty result to present for a wide audience,” the webmaster said, according to emails shared by Patokallio . Patokallio said that after he declined to take the post down, the webmaster responded with “an increasingly unhinged series of threats.” Wikipedia editors also pointed to webpage snapshots in Archive.today that appeared to have been altered to insert Patokallio’s name — hence the concern that it’s become “unreliable” as an archive. Wikipedia’s guidance now calls for editors to remove links to Archive.today and related sites, replacing them with links to the original source or to other archives like the Wayback Machine. On a blog linked from the Archive.today website, the site’s apparent owner wrote that Archive.today’s value to Wikipedia was “not about paywalls” but rather “the ability to offload copyright issues.” They later wrote that things had turned out “pretty well” and said they would “scale down the ‘DDoS’.” “Why didn’t you write about such events earlier, folks of the tabloids?” they said. “I don’t expect you to write anything good, because then who would read you, but there was plenty of dramas, wasn’t there? Because there was no Jani to nudge you?”",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-1873370000.jpg?resize=1200,800"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "EDuke32 • Duke3D for Windows, Linux, and macOS",
      "url": "https://www.eduke32.com/",
      "published": "2026-02-21T20:10:13+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.eduke32.com/\">https://www.eduke32.com/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47104185\">https://news.ycombinator.com/item?id=47104185</a></p> <p>Points: 190</p> <p># Comments: 67</p>",
      "content_text": "It's time to kick ass and chew bubble gum, and I'm all outta gum! Per-pixel dynamic lighting and realtime shadows... groovy! Polymer renderer requires a bad-ass video card. More Polymer greatness. Hollywood Holocaust with classic textures Come get some! EDuke32 is an awesome, free homebrew game engine and source port of the classic PC first person shooter Duke Nukem 3D — Duke3D for short—to Windows, Linux, macOS, FreeBSD, several handhelds, your family toaster, and your girlfriend's vibrator. We've added thousands of cool and useful features and upgrades for regular players and additional editing capabilities and scripting extensions for homebrew developers and mod creators. EDuke32 is open source software that is completely free to use for all non-commercial purposes. Created by Duke4.net community leader Richard \"TerminX\" Gobeille and a team of elite ninja programmers including Evan \"Hendricks266\" Ramos , Pierre-Loup \"Plagman\" Griffais , and Philipp \"Helixhorned\" Kutin (based on work by Todd Replogle / Ken Silverman / Jonathon Fowler / Matt Saettler ), EDuke32 is the undeniable king of Duke Nukem 3D ports. EDuke32 is licensed under the GNU GPL and the BUILD license . Showcase EDuke32 is the technology that powers Ion Fury , created by Voidpoint , which was founded by EDuke32's authors. Join our community Join us on Discord or visit our forums . Questions? Once you've downloaded EDuke32, you'll probably want to read our wiki page on installation and configuration , as well as the FAQ if you have any problems. Packed with features—\"shake it, baby!\" EDuke32 runs natively without relying on emulation of any kind Windows 11, 10, 8, 7, whatever -- it'll run on it. Linux is also well supported, both via the native SDL version or with Wine. Note: \"Linux is well supported with Wine\" does not mean get drunk and install Ubuntu. EDuke32 runs at crazy screen resolutions like 10240x4320. EDuke32 allows you to choose between two different hardware-accelerated OpenGL renderers , or the classic, warped software mode you grew up with EDuke32 fixes an insane amount of programming errors which were harmless in the days of DOS but are fatal with modern protected memory models; translation: EDuke32 crashes less EDuke32 includes VoidSW , a fully-fledged port of Shadow Warrior with all the same benefits. Who wants some Wang? EDuke32 is the only Duke3D port to be actively developed and maintained for more than twenty years EDuke32 features Plagman's incredible \"Polymer\" renderer with powerful hardware-accelerated capabilities Here are some of Polymer's features: Real time dynamic colored lighting and shadow mapping Specular and normal map support md3/jpg/png/tga support Fog density (sector visibility) support— corrects the dull appearance and extreme lack of contrast in early OpenGL ports Fullbrights and glow maps (for glowing red pigcop eyes, etc!) Detail textures Blending between model animations Support for colored fog Individual brightness/contrast/gamma adjustment Full widescreen monitor support plus manual fov and aspect ratio adjustment VSync support ...and more! EDuke32 has a huge number of new extensions to the game's scripting system, allowing gameplay mods that rival even modern games. EDuke32 runs the HRP with support for all features, most of which require EDuke32; no other port can run the HRP with all features enabled EDuke32 adds a full-featured console , including Quake-style key bindings, command aliases, advanced tab completion , comprehensive command history, colored text and more EDuke32 has hundreds of code rewrites, optimizations and fixes for rare or annoying bugs in the original code EDuke32 adds tons of optional new features that make the player's life easier including modern status display/HUD , support for loading mods from the startup window, and modern, WSAD-based controls with thoroughly reworked mouse aiming EDuke32 supports Ogg Vorbis and FLAC sound and music EDuke32 is developed by people who have been in the Duke3D scene since the beginning I first saw Duke3D running on a computer in a Wal-Mart in late December of 1995. I was 11 years old. Unknown to me, it was an illegally distributed beta of what was to become Duke Nukem 3D 1.0, released in January of the next year. After seeing Duke in action for the first time, I was hooked! I had seen games like Wolfenstein 3D, Doom and Heretic before but this was different. Not long after that, we got the first family computer, and I got the first episode of Duke on CD-ROM. I immediately got nosy and said \"hey, what are these 'CON' files?\" Atomic Edition came for Christmas that year. The rest is history! —Richard \"TerminX\" Gobeille EDuke32 lets you play that game called 'NAM' you saw at the dollar store back in the 90s EDuke32 makes sandwiches! BUILD engine technology originally created by Ken Silverman , non-GPL rendering and engine technology used in EDuke32 available under BUILDLIC .",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Home |inputlag.science",
      "url": "https://inputlag.science",
      "published": "2026-02-21T19:41:52+00:00",
      "summary": "<p>Article URL: <a href=\"https://inputlag.science\">https://inputlag.science</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47103945\">https://news.ycombinator.com/item?id=47103945</a></p> <p>Points: 93</p> <p># Comments: 30</p>",
      "content_text": "Welcome Hello traveler, welcome to the repository of knowledge about input lag in gaming. The input lag in a gaming system, or any interactive system, is the latency between the user input and a reaction on the screen. Input lag is an issue that has crept in the industry, little by little, without being noticed over the years. Nowadays, finding a gaming system with a latency similar to early 2000 without image degradation is a definitive challenge. It has come to a point where some games have major issues and it can cause uproars in the press. The reasons behind this rise of the latency is mainly that systems have become more and more complex and developers often don't know or don't understand each part that can impact the latency. This website has been made to help developers and consumers better understand the latency issues and how to tackle them. There are three majors components in the lag chain: The controller The game engine The display There are obviously plenty of subtleties around those three points. This website tries to reference all the knowledge around those parts, especially the two first one, and how to precisely measure them.",
      "cover_image_url": "https://inputlag.science/assets/logo.png"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Parse, don't Validate and Type-Driven Design in Rust",
      "url": "https://www.harudagondi.space/blog/parse-dont-validate-and-type-driven-design-in-rust/",
      "published": "2026-02-21T19:40:06+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.harudagondi.space/blog/parse-dont-validate-and-type-driven-design-in-rust/\">https://www.harudagondi.space/blog/parse-dont-validate-and-type-driven-design-in-rust/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47103931\">https://news.ycombinator.com/item?id=47103931</a></p> <p>Points: 191</p> <p># Comments: 44</p>",
      "content_text": "Parse, don't Validate and Type-Driven Design in Rust Reading time: 17 min read Table of Contents 1.3 Maxims of Type Driven Design Photo by the Tingley Injury Law Firm . In the Rust Programming Language Community Server, there’s tag named -parse-dont-validate which links to an article about the concept of avoiding validation functions and encoding invariants in the type level instead. I usually recommend it to beginners/intermediates to Rust who are struggling with designing APIs. The only problem is that it uses Haskell to explain its concepts. Yeah, it’s fine , but for beginners unfamiliar with the functional paradigm, it might not be so approachable. And so I wanted so write a blog post about this pattern but in a rather Rust-centric way. So let’s start! Dividing by zero # One basic example I can give is a function that divides a number by another number. fn divide ( a : i32 , b : i32 ) -> i32 { This is fine, but unfortunately it can panic when b has the value of zero: This gives an error : Compiling playground v0 . 0 . 1 ( / playground ) Finished ` dev ` profile [ unoptimized + debuginfo ] target ( s ) in 1 . 28s Running ` target / debug / playground ` thread ' main ' ( 41 ) panicked at src / main . rs : 2 : 5 : attempt to divide by zero note : run with ` RUST_BACKTRACE= 1 ` environment variable to display a backtrace That’s fine and dandy if we want erroneous values to fail loudly at runtime, but what if we want stronger guarantees? This is especially important when some operations don’t fail loudly, like the following: fn divide_floats ( a : f32 , b : f32 ) -> f32 { dbg! ( divide_floats ( a , b )); Compiling playground v0 . 0 . 1 ( / playground ) Finished ` dev ` profile [ unoptimized + debuginfo ] target ( s ) in 0 . 62s Running ` target / debug / playground ` [ src / main . rs : 8 : 2 ] divide_floats ( a , b ) = inf There’s no error! But do we want that? We could add an assert! in the divide_floats function to emulate typical integer division behavior. fn divide_floats ( a : f32 , b : f32 ) -> f32 { assert_ne! ( b , 0 . 0 , \"Division by zero is not allowed.\" ); Compiling playground v0 . 0 . 1 ( / playground ) Finished ` dev ` profile [ unoptimized + debuginfo ] target ( s ) in 0 . 65s Running ` target / debug / playground ` thread ' main ' ( 32 ) panicked at src / main . rs : 2 : 5 : assertion ` left != right ` failed : Division by zero is not allowed . Cute! But there’s still a problem of running into panics only at runtime. My beef with Python (or any other dynamic language for that matter) is that a lot of errors only arises when you run the program. That’s why they’re adding typechecking to these languages: people want to bubble some mistakes to compile-time (or typecheck-time, whatever). We can use Rust’s rich type system to communicate these errors at build time. One way, which I think is the more common way as people are more familiar with it is the idea of fallible functions, which return either an Option or a Result . fn divide_floats ( a : f32 , b : f32 ) -> Option < f32 > { This is a fine way to do things, as it communicates that (1) the function can fail, and (2) you can handle the failing case after. † † Of course, catch_unwind exists, but I’m pretending that it doesn’t. To me, the function’s invariants ( b must not be zero) is encoded after-the-fact, aka in the return type Option<T> . This implies to me that the invariants could be encoded before-the-fact, aka in the function parameters. But what would that look like? Enter the newtype pattern. Say, let’s have a type that is something like f32 , but it’s guaranteed to never be zero. We’ll name it NonZeroF32 : This struct only contains a single field f32 . The semantics of the type understood from the name is that it’s just like a normal f32 , but does not allow the value of zero. How do we guarantee this? Since rust does encapsulation at the module level, we make this type public while have its field private. pub struct NonZeroF32 ( f32 ); Then, the only way to construct this type is via a fallible constructor function: fn new ( n : f32 ) -> Option < NonZeroF32 > { Remember to add some convenience traits. impl Add for NonZeroF32 { ... } impl Add < f32 > for NonZeroF32 { ... } impl Add < NonZeroF32 > for f32 { ... } // and a bunch of other operators... We can then use this in our divide_floats function. fn divide_floats ( a : f32 , b : NonZeroF32 ) -> f32 { There is an interesting implication in this pattern. In the second version of divide_floats , we changed the return type from f32 to Option<f32> just to avoid the panics. As described in the original article by Alexis King, this is a weakening of the return type, and the function’s promise. We temper the caller’s expectation by saying that yes, this function can fail in some way, and you have to account for that. And that weakening is described in the type system via the Option enum. In the third iteration of divide_floats , we change our perspective and ask ourselves “instead of weakening the return type, what if we strengthen the function parameters?” We communicated that via accepting a NonZeroF32 . Instead of having the validation code in our functions, we instead push that responsibility to the caller. The validation now happens before the function execution. To see the advantage of pushing the validation forward to the user, let’s say we have another function like so: // The quadratic formula! fn roots ( a : f32 , b : f32 , c : f32 ) -> [ f32 ; 2 ] { // For the sake of demonstration we will be ignoring complex roots let discriminant = b * b - 4 * a * c ; - b + discriminant . sqrt () / ( 2 * a ), - b - discriminant . sqrt () / ( 2 * a ), This function can fail if the discriminant is negative (which we will be ignoring in this contrived example), and if a is zero. The two ways of going about this can be written as follows: fn try_roots ( a : f32 , b : f32 , c : f32 ) -> Option <[ f32 ; 2 ]> { if a == 0 { return None ; } fn newtyped_roots ( a : NonZeroF32 , b : f32 , c : f32 ) -> [ f32 ; 2 ] { The Option version has me duplicating the conditional for at least two different functions, which might be icky if you are a DRY-hard. Also, not only the function has to validate if the float can be zero, the caller must then validate again by matching on the returned Option . That seems redundant. It would be ideal if we only need to check only once. let roots = try_roots ( 5 , 4 , 7 ); // `try_roots` does a validation check // and then we validate it again by matching on the result Some ( result ) => do_something (), None => { handle_error (); return }, The NonZeroF32 version can help with that as validation happens before, and happens once, instead of twice. // Handle the special case once let Some ( a ) = NonZeroF32 :: new ( 5 ) else { // `newtyped_roots` does not need to handle it again, // indicated by the function not needing to return // an `Option` and us handling the result. let [ root1 , root2 ] = newtyped_roots ( a , 4 , 7 ); Moving away from the divide_floats , let’s now use an example from the original blog post, converted to Rust: fn get_cfg_dirs () -> Result < Vec < PathBuf >, Box < dyn Error >> { let cfg_dirs_string = std :: env :: var ( \"CONFIG_DIRS\" ) ? ; let cfg_dirs_list = cfg_dirs_string . split ( ' , ' ) . collect :: < Vec < PathBuf >>(); if cfg_dirs_list . is_empty () { return Err ( \"CONFIG_DIRS cannot be empty\" . into ()); fn main () -> Result <(), Box < dyn Error >> { let cfg_dirs = get_cfg_dirs () ? ; Some ( cache_dir ) => init_cache ( cache_dir ), None => unreachable! ( \"should never happen; already checked configDirs is non-empty\" ), Notice the following: We checked if cfg_dirs_list is empty in the get_cfg_dirs function. Then, we still had to “check” it again in the main function by matching on cfg_dirs.first() . The Vec was known to be nonempty, do we have to check it again? Consequently, doesn’t this have an impact on performance, especially if we have to check it again and again and again? The original post raised a good point about resilience to refactors. If for some reason the is_empty gets refactored out for some reason, and the programmer forgot to update main , then the unreachable! branch might actually get reached and explode your computer or whatever. If we instead had a special NonEmptyVec<T> newtype (well, not exactly special) where its existence guarantees that the Vec is never empty, we could do struct NonEmptyVec < T >( T , Vec < T >); // Notice that we don't need to return an `Option` fn first ( & self ) -> & T { ... } fn get_cfg_dirs () -> Result < NonEmptyVec < PathBuf >, Box < dyn Error >> { let cfg_dirs_string = std :: env :: var ( \"CONFIG_DIRS\" ) ? ; let cfg_dirs_list = cfg_dirs_string . split ( ' , ' ) . collect :: < Vec < PathBuf >>(); // We parse the `Vec` into a more structured type let cfg_dirs_list = NonEmptyVec :: try_from ( cfg_dirs_list ) ? ; fn main () -> Result <(), Box < dyn Error >> { let cfg_dirs = get_cfg_dirs () ? ; // Notice that we don't have to check again if the `Vec` // was empty, since we guarantee that via the `NonEmptyVec` type init_cache ( cfg_dirs . first ()); In this context, we can call NonZeroF32::new and NonEmptyVec::try_from parsing functions, since they validate and convert the less semantic type to a type with more meaning imbued into it. That is, nonzeroness of a float and nonemptiness of a Vec is now encoded into a type. You can just see the word NonZeroF32 and therefore understand that going forward it is always be an f32 that is never zero. Validation and checking functions on the other hand, well, just validate the value and leave the type as that. If I have a is_nonzero(f32) -> bool function, then there’s not really much of a readable difference between an f32 that has is_nonzero called on it versus and an f32 that hasn’t. fn is_nonzero ( n : f32 ) -> bool ; fn to_nonzero ( n : f32 ) -> Option < NonZeroF32 >; By taking advantage of the existence of a nominative type system, we can communicate that this f32 is not zero by parsing it to a new type, as opposed to just validating it. If you only validate it, then you still can’t tell if f32 was nonzero unless you dig through the code. However, if you parsed it, you can say it’s always be nonzero if you see NonZeroF32 in your code. Examples in the wild # Of course, the above examples are very much contrived, but is there an instance where creating newtypes is helpful? Yes. In fact, most people have used it. It’s called a String . If we dig into the internals, String is just a newtype over the Vec<u8> type: #[ derive ( PartialEq , PartialOrd , Eq , Ord )] #[ stable ( feature = \"rust1\" , since = \"1.0.0\" )] It’s parsing function is String::from_utf8 , which contains the validation code for checking if the byte vector is valid UTF-8. #[ stable ( feature = \"rust1\" , since = \"1.0.0\" )] #[ rustc_diagnostic_item = \"string_from_utf8\" ] pub fn from_utf8 ( vec : Vec < u8 >) -> Result < String , FromUtf8Error > { match str :: from_utf8 ( & vec ) { Ok ( .. ) => Ok ( String { vec }), Err ( e ) => Err ( FromUtf8Error { bytes : vec , error : e }), So instead of passing around a Vec<u8> around and validating all over the place, just parse into a String and you can be assured with having a type-safe String with all the convenience functions you can get. Another example is serde_json . In Python, json.loads simply give you a dictionary. This is fine, especially if the data is sufficiently arbitrary, but if you have a schema and a type system, it’s better to let the type system do the work of parsing json . In our terminology, validation looks like this: use serde_json :: { from_str , Value }; const SAMPLE_JSON: & str = r#\"{ \"foo\": 1, \"bar\": [1, 2, 3] }\"# ; let json = from_str :: < Value >( SAMPLE_JSON ) let first_elem = json . get ( \"bar\" ) . and_then ( | bar | bar . get ( 0 )) // do stuff with `first_elem` That’s two unwrap s! One for checking if the string is valid json and the other is for checking if the bar field exists. Now consider this example where we use the parsing mechanic instead via types and the Deserialize derive macro. fn first_elem ( & self ) -> i32 { self . bar [ 0 ] // does not panic, by definition let json = from_str :: < Sample >( SAMPLE_JSON ) . unwrap (); let first_elem = json . first_elem (); // do stuff with `first_elem` Since we deserialized the json file into an actual type, we can safely make these guarantees: The foo and bar always exist in the json string we parse. foo always has an integer value. bar is always an array of three integers. first_elem will never panic since all elements of an array is always initialized, and indexing into the first the element of a nonzero-length array will always be successful. The only point of failure here is pushed upfront, where the from_str happens. After that point, there’s not really much error handling to be done here, since the validation is now represented at the type level instead of at the function level. Maxims of Type Driven Design # With that said, what lessons can we learn from here? Turns out, most functional language programmers already have learned several lessons, and Rust is not much different in terms of applying such FP concepts to the language. First lesson we can learn is that we should make illegal states unrepresentable . What do we mean by that? To refer back to the NonZeroF32 and NonEmptyVec examples, we say the state of being zero is illegal for NonZeroF32 and the state of being empty is illegal for NonEmptyVec . And as illegal states, they cannot be represented in such types. That’s why the only constructors available for these types are fallible; the value either parsed successfully, or it failed and does not return the new types. If we only do validation, like checking if f32 is nonzero for example, then the illegal state can still be represented. There’s a small possible that the value is zero, especially after some refactors when the conditional checks are accidentally or intentionally removed in some places. This reminds me of how other languages use integers as sentinel values. Given this code snippet from Wikipedia : int find ( int arr [] , size_t len , int val ) { for ( int i = 0 ; i < len ; i ++ ) { The error is returned as -1 , since indexing arrays is only valid for nonnegative integers. Seems weird as (1) the numbers -2 and below can exist, but not actually valid, and (2) treating certain values as special seems too error-prone, as in the future it could be that negative number can become semantically valid. Second lesson we can learn is that proving invariants should be done as early as possible . There’s this concept called shotgun parsing where the linked paper describes it as follows: Shotgun Parsing: Shotgun parsing is a programming antipattern whereby parsing and input-validating code is mixed with and spread across processing code—throwing a cloud of checks at the input, and hoping, without any systematic justification, that one or another would catch all the “bad” cases. Essentially, it describes the problem of usage of data without previous validation of its entirety of data. You could act on a part of the data that is validated beforehand, but discover that another part of the data is invalid. The paper mentions CVE-2016-0752 , which is a bug that allows attackers to read arbitrary files because you can use .. in the input. The paper argues that treating validation as emergent and not deliberate can lead to security bugs like these. If we treat validation as deliberate, then it should happen as early as possible and as comprehensive as possible. By parsing first, every invariant can be proven first before executing on said data. I remember this video about lambda calculus. It concludes that types can be represented as propositions in logic, and terms as proofs. I recommend watching the video, as it is eye-opening to me and maybe it can help you realize some things too. Fundamentally, if your program typechecks properly, then you can say that the ",
      "cover_image_url": "https://www.harudagondi.space/uploads/parse-dont-validate/tingey-injury-law-firm-veNb0DDegzE-unsplash.jpg"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "How Taalas \"prints\" LLM onto a chip?",
      "url": "https://www.anuragk.com/blog/posts/Taalas.html",
      "published": "2026-02-21T19:07:20+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.anuragk.com/blog/posts/Taalas.html\">https://www.anuragk.com/blog/posts/Taalas.html</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47103661\">https://news.ycombinator.com/item?id=47103661</a></p> <p>Points: 143</p> <p># Comments: 71</p>",
      "content_text": "How Taalas \"prints\" LLM onto a chip? or how to generate 17000 tokens per second? February 22, 2026 · 4 min read A startup called Taalas , recently released an ASIC chip running Llama 3.1 8B (3/6 bit quant) at an inference rate of 17,000 tokens per seconds. That's like writing around 30 A4 sized pages in one second. They claim it's 10x cheaper in ownership cost than GPU based inference systems and is 10x less electricity hog. And yeah, about 10x faster than state of art inference. I tried to read through their blog and they've literally \"hardwired\" the model's weights on chip. Initially, this didn't sound intuitive to me. Coming from a Software background, with hobby-ist understanding of LLMs, I couldn't wrap my head around how you just \"print\" a LLM onto a chip. So, I decided to dig into multiple blogposts, LocalLLaMA discussions, and hardware concepts. It was much more interesting than I had thought. Hence this blogpost. Basics Taalas is a 2.5 year old company and it's their first chip. Taalas's chip is a fixed-function ASIC (Application-Specific Integrated Circuit). Kinda like a CD-ROM/Game cartridge, or a printed book, it only holds one model and cannot be rewritten. HOW NVIDIA GPUs process stuff? (Inefficiency 101) LLMs consist of sequential Layers. For eg. Llama 3.1 8B has 32 layers. The task of each layer is to further refine the input. Each layer is essentially large weight matrices (the model's 'knowledge'). When a user inputs a prompt, it is converted into an vector of numbers aka embeddings. On a normal GPU, the input vector enters the compute cores. Then GPU fetches the Layer 1 weights from VRAM/HBM (GPU's RAM) , does matrix multiplication, stores the intermediate results(aka activations) back in VRAM. Then it fetches the Layer 2 weights, and previous result, does the math, and saves it to VRAM again. This cycle continues till 32nd layer just to generate a single token. Then, to generate the next token, the GPU repeats this entire 32-layer journey. So, due to this constant back-and-forth the memory bus induces latency and consumes significant amounts of energy. This is the memory bandwidth bottleneck, sometimes loosely called the Von Neumann bottleneck or the \"memory wall.\" Breaking the wall! Taalas sidesteps this wall entirely. They just engraved the 32 layers of Llama 3.1 sequentially on a chip. Essentially, the model's weights are physical transistors etched into the silicon. Importantly, they also claim to have invented a hardware scheme where they can store a 4-bit data and perform the multiplication related to it using a single transistor. I will refer it as their 'magic multiplier' Now, when the user's input arrives, it gets converted into a vector, and flows into physical transistors making up Layer1. It does multiplication via their 'magic multiplier' and instead of result being saved in a VRAM, the electrical signal simply flows down physical wires into the Layer 2 transistors (via pipeline registers from what I understand). The data streams continuously through the silicon until the final output token is generated. So, they don't use any RAM? They don't use external DRAM/HBM, but they do use a small amount of on-chip SRAM. Why SRAM? Due to cost and complexity, manufacturers don't mix DRAM and logic gates. That's why GPUs have separate VRAM. (Also SRAM isn't facing supply chain crisis, DRAM is). Taalas uses this on-chip SRAM for the KV Cache (the temporary memory/context window of an ongoing conversation) and to hold LoRA adapters for fine tuning. But isn't fabricating a custom chip for every model super expensive? Technically yes, I read lots of comments saying that. But Taalas designed a base chip with a massive, generic grid of logic gates and transistors. To map a specific model onto the chip, they only need to customize the top two layers/masks. While it's still slow, but it's much faster than building chips from ground up. It took them two months, to develop chip for Llama 3.1 8B. In the AI world where one week is a year, it's super slow. But in a world of custom chips, this is supposed to be insanely fast. As someone stuck running local models on a laptop without a massive GPU, I am keeping my fingers crossed for this type of hardware to be mass-produced soon.",
      "cover_image_url": "../staticFiles/Taalas/GPUInference.png"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "a-e-k/canvas_ity: A tiny, single-header <canvas>-like 2D rasterizer for C++",
      "url": "https://github.com/a-e-k/canvas_ity",
      "published": "2026-02-21T18:50:19+00:00",
      "summary": "<p>Article URL: <a href=\"https://github.com/a-e-k/canvas_ity\">https://github.com/a-e-k/canvas_ity</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47103506\">https://news.ycombinator.com/item?id=47103506</a></p> <p>Points: 100</p> <p># Comments: 35</p>",
      "content_text": "You can’t perform that action at this time.",
      "cover_image_url": "https://opengraph.githubassets.com/ef9afa31d71e188c58773b0b1d399034f5b49ddefc6cb2c570629ac5e46cd219/a-e-k/canvas_ity"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "65% Value Loss in a Year",
      "url": "https://carbuzz.com/toyota-mirai-massive-depreciation-one-year/",
      "published": "2026-02-21T18:09:24+00:00",
      "summary": "<p>Article URL: <a href=\"https://carbuzz.com/toyota-mirai-massive-depreciation-one-year/\">https://carbuzz.com/toyota-mirai-massive-depreciation-one-year/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47103136\">https://news.ycombinator.com/item?id=47103136</a></p> <p>Points: 149</p> <p># Comments: 336</p>",
      "content_text": "The story of hydrogen propulsion in the automotive world has always been a convoluted one. Infrastructure difficulties and the rise of competitors – most notably battery-electric power – have made it extremely difficult for hydrogen to ever properly take hold. Despite these struggles, some hydrogen-powered models are still circulating on both the new and used car market. One such model is the Toyota Mirai , the first and perhaps the most famous hydrogen-powered production car in existence. The Mirai is still being built today, but has faced several struggles which meant that even very recent used examples are catastrophically depreciating. Let’s take a closer look at this phenomenon, the reasons behind it, and the Mirai model as a whole. Base Trim Engine EV Base Trim Transmission Automatic Base Trim Drivetrain Rear-Wheel Drive UPDATE: 2026/02/19 16:59 EST BY GERHARD HORN This feature was updated with information about synthetic fuel, which is seen as yet another alternative to hydrogen fuel cells and hydrogen combustion. The Toyota Mirai: An Overview 2021 - 2025 Toyota Mirai exterior Toyota 2025 Toyota Mirai Powertrain One electric motor (hydrogen fuel cell) Power 182 hp Torque 220 lb-ft 0-60 mph 9.1 seconds Top Speed 106 mph The Toyota Mirai was the first mass-produced hydrogen fuel cell vehicle to be released on the market. The model’s first generation made its debut in late 2014, going on sale for the 2015 model year. The first-gen Mirai was fitted with one electric motor, powered by what the company dubbed the Toyota Fuel Cell System (TFCS). This system was made up of proprietary components developed in-house by Toyota, including the fuel cell itself and the hydrogen tanks. 2021 - 2025 Toyota Mirai exterior Toyota In 2020 a second generation of the Mirai was introduced, going on sale for the 2021 model year. This generation brought several powertrain and tech updates, such as an increased hydrogen capacity that results in a 30% increase in driving range. More advanced driver assistance and safety features were added, and in 2023 the infotainment system was given an update. The new Mirai also brought some minor styling updates, although the car’s silhouette is broadly similar to that of the first-gen model. Related Some of the most influential cars in automotive history, from the very earliest to the most modern. The second-gen Mirai is also powered by one electric motor fed by a hydrogen fuel cell. This time, power output increased by 30 hp (from 152 to 182 hp), while torque decreased slightly, from 247 to 220 lb-ft. Both models are equipped with a single-speed transmission. The Mirai's Pricing On The Used Market 2021 - 2025 Toyota Mirai exterior Toyota The Mirai has experienced extremely rapid depreciation. Looking at its price range on the used market today, the difference compared to prices when new – even for models only a couple of years old – is staggering. The 2021 model year, which ushered in the second generation, had a starting price of $49,500; all subsequent model years sat just above the $50,000 mark. Even if we limit our search to examples from the model’s current generation, we can still find several cars being offered for less than $10,000 (while still keeping within a five-figure mileage). The most expensive used second-gen cars hover around the $22,000 mark. 2021 - 2025 Toyota Mirai interior Toyota Examples of last year’s model, which had a starting price of $50,190, can now be found on sale for between $15,000 and $18,000, with mileage numbers that in any other car would not warrant this level of depreciation. It is clear that, when it comes to the Mirai, other, unique factors are at play: the Mirai’s depreciation is symbolic of hydrogen’s downward trajectory in the American and global automotive landscape. Why Mirai Values Have Tanked 2021 - 2025 Toyota Mirai exterior Toyota Toyota may still consider the Mirai to be a viable model, but its place on the used market makes it clear that consumers’ opinion is going in a different direction. The Mirai’s value has dropped dramatically due to a combination of factors , most of which have to do with its unconventional method of propulsion and the pitfalls associated with it. We start with the Mirai’s geographical limitations. The vehicle is only available for purchase in California, as that is the only state that currently offers enough hydrogen fueling infrastructure to support Mirai ownership. Although, in the past, it looked like hydrogen infrastructure may have expanded into other areas of the country, today that possibility looks unlikely to become a reality. The rise of battery electric vehicles as the dominant form of green transport has limited hydrogen’s potential for growth. As a result, the Mirai no longer seems to be the attractive, future-ready purchase it might have previously been. Related Keen on buying a hydrogen-powered car? You have three models to choose from... at least for now. Due to the uniqueness and rarity of the Mirai’s powertrain, prospective buyers are also more likely to be wary of buying a used example. Used cars are already known for being potential reliability time-bombs, depending on factors such as their condition and mileage; for a hydrogen fuel cell vehicle this is doubly true, as maintenance is more likely to be more complex and specialized and replacement parts are scarce. 2021 - 2025 Toyota Mirai exterior Toyota Almost No Investment In Hydrogen Infrastructure The chief reason behind the Mirai’s depreciation – and the wider struggle faced by all hydrogen vehicles in today’s automotive market – is the lack of infrastructure. Hydrogen has fallen behind other “green” sources of power, and battery electric vehicles have taken over as the main alternative to combustion. The US has just 54 hydrogen filling stations, all of which are located in California. Expansion of this limited network could come through government investment, however the US government is prioritizing BEVs. Over the past four years, the number of accessible EV charging ports in the US has doubled; this number is set to climb even further as in January this year, the government announced $635 million would be invested in the US’ renewable energy network. For comparison, hydrogen infrastructure is getting an investment of just $80 million, $55 million of which is confined to California. A single new hydrogen station is currently being built in California, and instead of being oriented towards passenger cars it will exclusively serve medium and large semi-trucks, which means potential new hydrogen car owners are left out. Another station is being built in Texas, but this will also cater exclusively to commercial vehicles rather than private ones. Some existing hydrogen filling stations are even getting shut down , which further complicates the situation for anyone looking to buy a Mirai or another hydrogen vehicle. Looking at the combinations of all these factors, it’s no wonder that Mirais are practically being given away on the used market, as their pool of potential buyers is an extremely limited one. Related Hydrogen has been battling to find a platform in the renewable fuel battle, though it seems destined to fail to catch on in the US. It is clear that the government and the automotive industry are focusing on battery-electric vehicles as the green option of the future (and the present); hydrogen is on track to remain the preserve of a few dedicated fans, with little enthusiasm from the mainstream car world. There's A Better Alternative Fuel Available 2025 Porsche 911 Carrera T Formosa Porsche As we've already discussed, the government isn't investing in hydrogen fueling stations, and that's arguably this alternative fuel's biggest downfall. But there is another way, and it doesn't require the existing fuel network to be upgraded entirely. We're talking about synthetic fuel, which only has two downsides at the moment. First, it's expensive to make, and the demand is far more than factories can supply. But these are teething problems with any new technology. HIF, the self-proclaimed global leader in fuels and Porsche's official synthetic fuel partner, will have a synthetic fuel plant operational in the USA in 2027. It will produce enough fuel to power 400,000 vehicles, which is still just a tiny percentage of cars, but it's a start. If more of these factories are built, the cost per gallon will inevitably go down, and the supply will eventually meet demand. Will this happen soon? Nope. Like hydrogen, the government doesn't appear to be interested in making life easier for synthetic fuel plants, even though it means you can hold on to your V8 and not feel guilty about it. Related Synthetic fuels, also called eFuels, could theoretically replace gasoline in your ICE car. But is the technology just science fiction? What's Next For Hydrogen Cars? Hydrogen plug Toyota Although the world of hydrogen-powered vehicles is unlikely to see much expansion in the coming years, automakers are still curious about its potential, and some brands are persevering in the research and development of hydrogen powertrains. Not only is Toyota still producing the Mirai, the Japanese automaker has also re-affirmed its commitment to hydrogen propulsion with the introduction of a new project. A US-based “North American Hydrogen Headquarters” (H2HQ) was set up by Toyota last year, creating a dedicated space for research and development of hydrogen propulsion. The company is continuing to work in several different directions when it comes to powertrain options, offering the Mirai alongside BEVs and hybrids. The Mirai is still available as a 2026 model, and Toyota keeps on offering incentives to make it more appealing. Despite that, it only managed to sell 210 units in the USA last year. Related Toyota can't stop, won't stop trying to sell a car America doesn't want. Other brands are still clinging to the hydrogen wagon as well: BMW has been testing a hydrogen-powered prototype SUV called the iX5 , with plans to release a production model in around three years’ time. Toyota is collaborating with BMW on this project, offering its established hydrogen expertise. The long-term prospects of hydrogen power may not be all that rosy, but at least for now, hydrogen is not quite dead. Even as the automotive world embraces electrification, there is always room for research into alternative methods of propulsion, and hydrogen still has some potential to fulfill. Sources: Toyota, US Department of Transportation",
      "cover_image_url": "https://static0.carbuzzimages.com/wordpress/wp-content/uploads/2025/10/toyota-mirai-main.png?w=1600&h=900&fit=crop"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "Microsoft’s new gaming CEO vows not to flood the ecosystem with ‘endless AI slop’",
      "url": "https://techcrunch.com/2026/02/21/microsofts-new-gaming-ceo-vows-not-to-flood-the-ecosystem-with-endless-ai-slop/",
      "published": "2026-02-21T17:41:27+00:00",
      "summary": "Is Microsoft's gaming division doubling down on AI?",
      "content_text": "Microsoft announced a major gaming shakeup on Friday, with Microsoft Gaming CEO Phil Spencer departing the company, along with Xbox President Sarah Bond. Spencer will be replaced by former Instacart and Meta executive Asha Sharma. With Sharma’s most recent role as the president of Microsoft’s CoreAI product, these moves suggest that Microsoft might be doubling down on bringing AI into video games. The company had already been experimenting with ways to combine AI and gaming, for example developing an AI gaming companion and releasing a buggy, AI-generated level from “Quake II.” Indeed, in an internal memo published by The Verge , Sharma wrote that Microsoft “will invent new business models and new ways to play” and said that “monetization and AI” will both “evolve and influence this future.” At the same time, she said that the company “will not chase short-term efficiency or flood our ecosystem with soulless AI slop.” “Games are and always will be art, crafted by humans, and created with the most innovative technology provided by us,” Sharma added. That’s just one of three “commitments” Sharma made in her memo. The others involve building “great games beloved by players” and prioritizing Xbox.",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2024/10/GettyImages-2174641784.jpg?resize=1200,800"
    },
    {
      "industry": "technology",
      "source": "The Verge",
      "title": "Georgia says Elon Musk’s America PAC violated election law",
      "url": "https://www.theverge.com/tech/882838/georgia-elon-musk-america-pac-voter-fraud",
      "published": "2026-02-21T16:55:36+00:00",
      "summary": "For all his bluster about voter fraud, Elon Musk has been one of the most flagrant flaunters of US election law. Now his America PAC has been slapped with a reprimand by the Georgia State Election Board for sending out pre-filled absentee ballot applications. State law prohibits anyone, other than an authorized relative, from sending [&#8230;]",
      "content_text": "For all his bluster about voter fraud , Elon Musk has been one of the most flagrant flaunters of US election law. Now his America PAC has been slapped with a reprimand by the Georgia State Election Board for sending out pre-filled absentee ballot applications. State law prohibits anyone, other than an authorized relative, from sending an absentee ballot application prefilled with the elector’s information. Residents of Chattooga, Cherokee, Coweta, Floyd, and Whitfield counties reported receiving absentee ballot applications from America PAC, partially pre-filled. According to the State Election Board, the applications also failed to note that they were not actual ballots and were not provided by the government, as is required by law. At the State Election Board meeting on February 18th, the panel quickly handed down the reprimand. America PAC did not appear to send a representative or submit a statement in defense of its actions. This is just the latest example of Musk testing the limits of election law after he repeatedly suggested paying people to sign a petition and register to vote in Pennsylvania , Wisconsin , and other swing states. It’s also another incident in which those who claim to be most concerned about election integrity have been caught potentially violating the law.",
      "cover_image_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2025/03/STK022_ELON_MUSK_4_CVIRGINIA_B.webp?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "Google VP warns that two types of AI startups may not survive",
      "url": "https://techcrunch.com/2026/02/21/google-vp-warns-that-two-types-of-ai-startups-may-not-survive/",
      "published": "2026-02-21T16:00:00+00:00",
      "summary": "As generative AI evolves, a Google VP warns that LLM wrappers and AI aggregators face mounting pressure, with shrinking margins and limited differentiation threatening their long-term viability.",
      "content_text": "The generative AI boom minted a startup a minute. But as the dust starts to settle, two once-hot business models are looking more like cautionary tales: LLM wrappers and AI aggregators. Darren Mowry, who leads Google’s global startup organization across Cloud, DeepMind, and Alphabet, says startups with these hooks have their “check engine light” on. LLM wrappers are essentially startups that wrap existing large language models, like Claude, GPT, or Gemini, with a product or UX layer to solve a specific problem. An example would be a startup that uses AI to help students study . “If you’re really just counting on the back-end model to do all the work and you’re almost white-labeling that model, the industry doesn’t have a lot of patience for that anymore,” Mowry said on this week’s episode of Equity . Wrapping “very thin intellectual property around Gemini or GPT-5” signals you’re not differentiating yourself, Mowry says. “You’ve got to have deep, wide moats that are either horizontally differentiated or something really specific to a vertical market” for a startup to “progress and grow,” he said. Examples of the deep-moat LLM wrapper type include Cursor, a GPT-powered coding assistant, or Harvey AI, a legal AI assistant. Techcrunch event Boston, MA | June 9, 2026 In other words, startups can no longer expect to slap a UI on top of a GPT and get traction on their product like they could, perhaps, in mid-2024 when OpenAI launched its ChatGPT store . The challenge now is to build sustainable product value. AI aggregators are a subset of wrappers — they’re startups that aggregate multiple LLMs into one interface or API layer to route queries across models and give users access to multiple models. These companies typically provide an orchestration layer that includes monitoring, governance, or eval tooling. Think: AI search startup Perplexity or developer platform OpenRouter, which provides access to multiple AI models via a single API. While many of these platforms have gained ground, Mowry’s message is clear to incoming startups: “Stay out of the aggregator business.” Generally speaking, aggregators aren’t seeing much growth or progression these days because, he says, users want “some intellectual property built in” to ensure they’re routed to the right model at the right time based on their needs — not because of behind-the-scenes compute or access constraints. Mowry has been in the cloud game for decades, cutting his teeth at AWS and Microsoft before setting up shop at Google Cloud, and he’s seen how this plays out. He said the situation today mirrors the early days of cloud computing in the late 2000s/early 2010s as Amazon’s cloud business started taking off. At that time, a crop of startups sprang up to resell AWS infrastructure, marketing themselves as easier entry points that provided tooling, billing consolidation, and support. But when Amazon built its own enterprise tools and customers learned to manage cloud services directly, most of those startups were squeezed out. The only survivors were the ones that added real services, like security, migration, or DevOps consulting. AI aggregators today face similar margin pressure as model providers expand into enterprise features themselves, potentially sidelining middlemen. For his part, Mowry is bullish on vibe coding and developer platforms, which had a record-breaking year in 2025 with startups like Replit, Lovable, and Cursor (all Google Cloud customers, per Mowry) attracting major investment and customer traction. Mowry also expects strong growth in direct-to-consumer tech, in companies that put some of these powerful AI tools into the hands of customers. He pointed to the opportunity for film and TV students to use Google’s AI video generator Veo to bring stories to life. Beyond AI, Mowry also thinks biotech and climate tech are having a moment — both in terms of venture investment going into the two industries and the “incredible amounts of data” startups can access to create real value “in ways we would never have been able to before.”",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2026/02/Darren-Mowry-headshot.png?resize=1200,960"
    }
  ]
}