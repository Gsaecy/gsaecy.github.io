{
  "industry": "technology",
  "collected_at": "2026-02-24T10:06:16.198812+00:00",
  "hours": 24,
  "limit": 25,
  "count": 25,
  "items": [
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "Canva acquires startups working on animation and marketing",
      "url": "https://techcrunch.com/2026/02/23/canva-acquires-startups-working-on-animation-and-marketing/",
      "published": "2026-02-24T07:39:27+00:00",
      "summary": "With the new acquisitions, the company wants to bolster its position as a marketing solution by potentially adding video creation and more granular measurement.",
      "content_text": "On Monday, creative suite maker Canva announced the dual acquisition of startups Cavalry , which works on animation, and Mango AI , which works on improving ad performance. UK-based Cavalry works on 2D motion animation for different verticals such as advertising, marketing, gaming, and generative art. Canva said that Cavalry’s tooling will add to the existing capabilities of Affinity, Canva’s professional creative editing suite for photos, vectors, and layouts, which it acquired in 2024 Canva revamped Affinity’s design last year and made it free for all users. The company said that since then, people have downloaded the software over five million times. Affinity has the capabilities of photos, vector, and layout editing. With this acquisition, Canva wants to add motion editing to its suite. “By bringing Cavalry alongside Affinity, we’re closing that [motion editing] gap and unlocking a complete professional suite spanning photo, vector, layout, and now motion editing,” the company said in a blog post. “Together, these tools form the foundation of a full-stack Creative OS for professional work, while preserving the depth and control professional creatives rely on,” it added. VIDEO Besides Cavalry, Canva has also acquired stealth startup MangoAI , which was working on building reinforcement learning systems to improve video ad performance, according to its website. Canva said that the startup’s first product helped clients create and launch ads and observe outcomes to improve future campaigns. MangoAI was built by Nirmal Govind, former Vice President of Data Science & Engineering at Netflix, and Vinith Misra, a former data scientist at Netflix and Roblox. Canva said that Govind will become Canva’s first ” Chief Algorithms Officer” and Misra will work on improving Canva’s marketing products. In January 2025, Canva acquired marketing intelligence startup Magicbrief and later last year, it launched a growth tool called Canva Grow for asset creation and performance measurement. Techcrunch event Boston, MA | June 9, 2026 MangoAI Co-Founders Nirmal Govind (left) and Vinith Misra (right) together with Canva Co-Founder and COO, Cliff Obrecht (centre).I mage Credits: Canva During a sit-down at Web Summit Qatar earlier this month, Canva co-founder and COO Cliff Obrecht told TechCrunch that Canva Grow is doing “incredibly well,” especially when it comes to creating static content and publishing it to Meta platforms. “It is quite an early product, but we’ll soon be launching a lot more things around video creation, deploying across multi platform,” Obrecht had said. “So it’s very early, but it’s very much got a very loyal small user base, but a lot of big brands are spending money, and then we’re scaling up massively.” With the new acquisitions, the company wants to bolster its position as a marketing solution by potentially adding video creation and more granular measurement. Canva closed 2025 at $4 billion in annualized revenue with more than 265 million users and 31 million paid users.",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2024/05/GettyImages-1258807457.jpg?w=1024"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Firefox 148 Launches with Exciting AI Kill Switch Feature and More Enhancements!",
      "url": "https://serverhost.com/blog/firefox-148-launches-with-exciting-ai-kill-switch-feature-and-more-enhancements/",
      "published": "2026-02-24T05:47:23+00:00",
      "summary": "<p>Article URL: <a href=\"https://serverhost.com/blog/firefox-148-launches-with-exciting-ai-kill-switch-feature-and-more-enhancements/\">https://serverhost.com/blog/firefox-148-launches-with-exciting-ai-kill-switch-feature-and-more-enhancements/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47133313\">https://news.ycombinator.com/item?id=47133313</a></p> <p>Points: 228</p> <p># Comments: 189</p>",
      "content_text": "The latest update of Firefox, version 148, introduces a much-anticipated \"AI kill switch\" feature, allowing users to disable AI functionalities such as chatbot prompts and AI-generated link summaries. Mozilla emphasizes that once AI features are turned off, future updates will not override this choice. This decision reflects the company’s new revenue-focused strategy regarding AI integrations. To disable AI features, users can navigate to Settings > AI Controls and toggle the ‘Block AI Enhancements’ option. This will prevent any in-app notifications encouraging users to try out AI features, as well as remove any previously downloaded AI models from the device. For those who wish to maintain some AI functionalities, a selective blocking option is available, enabling users to retain useful features like on-device translations while avoiding cloud-based services. Beyond the AI kill switch, Firefox 148 offers users more control over remote updates, allowing them to opt out while still minimizing data collection. Users can set these preferences under Settings > Privacy & Settings > Firefox Data Collection . The update also focuses on enhancing core web platform capabilities, including the integration of the Trusted Types API and Sanitizer API to combat cross-site scripting (XSS) issues. Additionally, Firefox 148 now includes improved screen reader compatibility for mathematical formulas in PDFs, availability of Firefox Backup on Windows 10, and translation capabilities for Vietnamese and Traditional Chinese. New tab wallpapers will also be featured in new container tabs, alongside the addition of Service worker support for WebGPU . For more detailed information on the update, users can refer to the official release notes .",
      "cover_image_url": "https://serverhost.com/blog/wp-content/uploads/2026/02/4fa59d91-110e-42c3-b1fa-e372279736ab.webp"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "Stripe, PayPal Ventures bet on India's Xflow to fix cross-border B2B payments",
      "url": "https://techcrunch.com/2026/02/23/stripe-paypal-ventures-bet-on-indias-xflow-to-fix-cross-border-b2b-payments/",
      "published": "2026-02-24T05:30:00+00:00",
      "summary": "Stripe and PayPal Ventures have participated in Xflow's $16.6 million round that gives it a post-money valuation of $85 million.",
      "content_text": "Xflow , an Indian fintech startup, has secured backing from both Stripe and PayPal Ventures in a $16.6 million funding round. The investment comes as the company works to carve out a position in cross-border B2B payments, a market still dominated by banks and manual processes. The Series A round was led by General Catalyst, with participation from existing investors Square Peg, Stripe, Lightspeed, and Moore Capital, while PayPal Ventures joined as a new backer. The all-equity round values the Bengaluru-based startup at $85 million post-investment and brings its total funding to more than $32 million to date. Despite rapid digitization in domestic payments , cross-border B2B transfers for Indian exporters remain heavily reliant on banks , often with limited visibility into fees, settlement timelines, and the final amount received in rupees. The friction is particularly acute for larger exporters moving millions of dollars into India to fund salaries and local operations, creating an opening for fintech infrastructure players such as Xflow that promise greater transparency and speed in international money movement. Founded in 2021, Xflow provides cross-border payment infrastructure for businesses ranging from exporters and SaaS firms to platforms and freelancers, enabling them to collect international payments, manage foreign exchange, and settle funds in India. “Cross-border B2B payments were stuck in a different age compared to UPI,” co-founder Anand Balaji (pictured above, center) said in an interview, referring to India’s widely used instant domestic payments network, the Unified Payments Interface. Balaji, who previously helped build out Stripe’s India business, founded Xflow with former Stripe colleagues Ashwin Bhatnagar (pictured above, right) and Abhijit Chandrasekaran (pictured above, left). Last year, Xflow said it enabled Indian businesses to collect payments from more than 100 countries in over 25 currencies. It processed close to $1 billion in annualized cross-border payment volume last year, marking roughly 10-fold growth from the same period in 2024, Balaji told TechCrunch. Techcrunch event Boston, MA | June 9, 2026 According to the company, its customer base has expanded to about 15,000 businesses spanning SaaS firms, global capability centers (which are offshore units that multinationals operate in India), IT services exporters, freelancers, and fintech platforms. Transaction sizes vary widely by segment, with global capability centers averaging about $1 million to $2 million per transaction, goods exporters around $30,000 to $40,000, and freelancers roughly $3,000, according to Balaji. Xflow is positioning itself as a payments infrastructure provider rather than a direct payments application, offering APIs that allow platforms and exporters to embed cross-border money movement into their own products. “We didn’t want to build the next Wise — we want to power the next thousand Wises,” Balaji said. The startup has also introduced an AI-based foreign exchange tool to help finance teams optimize the timing of currency conversions. Xflow says the feature has generated incremental gains for some customers through data-driven foreign exchange decisions. The tool allows businesses to set target conversion rates rather than accepting prevailing bank quotes. Balaji likened the feature to limit orders in trading — instructions to buy or sell only at a specified price. “What we’ve added is the prediction layer and the ability to actually set a limit order,” he said. The model currently provides a three-day forecast with about 92% confidence, Balaji said, though TechCrunch could not independently verify that figure. Xflow faces competition from banks that still dominate large cross-border B2B transfers, as well as fintech players such as Wise, Payoneer, and Skydo at the lower end of the market. But Balaji said the startup’s focus on high-value transactions and API-led infrastructure differentiates it from many rivals. The startup plans to deploy the new capital toward building additional products on top of its core payments infrastructure and securing regulatory licenses in new markets, Balaji said. Xflow is preparing to roll out import capabilities in the coming months and is pursuing licenses in markets including Singapore, while already holding a payments license in Canada, even as it remains focused on India as its primary market. Xflow said it has also received final authorization from the Reserve Bank of India for a Payment Aggregator–Cross Border (PA-CB) license covering both exports and imports. The startup has signed platform partnerships with Easebuzz and Drip Capital to embed its cross-border capabilities into their offerings. Backing from Stripe and PayPal Ventures, Balaji said, has helped strengthen the startup’s credibility with banking and regulatory partners, even as it continues to work with multiple payment providers commercially. The startup currently has about 65 employees as it scales its cross-border infrastructure business.",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2026/02/xflow-founders-anand-balaji-ashwin-bhatnagar-abhijit-chandrasekaran.jpg?resize=1200,800"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "GitHub - GreatScott/enveil: ENVeil: Hide .env secrets from prAIng eyes: secrets live in local encrypted stores (per project) and are injected directly into apps at runtime, never touching disk as plaintext.",
      "url": "https://github.com/GreatScott/enveil",
      "published": "2026-02-24T05:04:50+00:00",
      "summary": "<p>Article URL: <a href=\"https://github.com/GreatScott/enveil\">https://github.com/GreatScott/enveil</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47133055\">https://news.ycombinator.com/item?id=47133055</a></p> <p>Points: 74</p> <p># Comments: 37</p>",
      "content_text": "You can’t perform that action at this time.",
      "cover_image_url": "https://opengraph.githubassets.com/a02fea8542980c1dc7d83d1ed329a2e90272dde4c80744353e79d2005882ddd9/GreatScott/enveil"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Intel® Arc™ Graphics - Windows*",
      "url": "https://www.intel.com/content/www/us/en/download/785597/intel-arc-graphics-windows.html",
      "published": "2026-02-24T04:04:19+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.intel.com/content/www/us/en/download/785597/intel-arc-graphics-windows.html\">https://www.intel.com/content/www/us/en/download/785597/intel-arc-graphics-windows.html</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47132721\">https://news.ycombinator.com/item?id=47132721</a></p> <p>Points: 29</p> <p># Comments: 17</p>",
      "content_text": "Introduction This download installs Intel® Graphics Driver 32.0.101.8509 (WHQL Certified) for Intel® Arc™ B-Series Graphics, Intel® Arc™ A-Series Graphics, and Intel® Core™ Ultra Processors with Intel® Arc™ Graphics. Intel requires an accepted license agreement in order to download this file. Would you like to reconsider? Available Downloads Download gfx_win_101.8509.exe Recommended Windows 11 Family*, Windows 10* (22H2) Size: 1.3 GB SHA256: EC3123DEE34B8B1FF5E73D83D370B36914CFC3D0D19C5B7923B7DE5250B9F5CA Detailed Description Note: The generic Intel driver provides users with the latest and greatest feature enhancements and bug fixes that computer manufacturers (OEMs) might not have customized yet. OEM drivers are handpicked and include customized features and solutions to platform-specific needs. Installing this Intel generic graphics driver will overwrite your handpicked OEM graphics driver. Users can check for matching OEM driver versions at OEM websites. For more information on how the installation of this driver may impact your OEM customizations, see this article. Are you experiencing a problem where Windows Update recommends or updates to a driver older than the one you just installed? If so, then you can find a solution here . Users looking for the dedicated Intel® Arc™ Pro Graphics Driver should click here . Any graphics issues found using Intel generic graphics drivers should be reported directly to Intel . Corporate customers should always use OEM drivers and report all issues through the vendor they purchased the platforms and support from. Understand the differences between WHQL and non-WHQL Certified Graphics drivers here . Highlights: XeSS 3: Multi-Frame Generation Extended Platform Support This release extends support beyond the Intel® Arc™ GPUs in Intel® Core™ Ultra Series 3 listed products to include: Intel® Arc™ B-Series discrete GPUs (Codename Battlemage) Intel® Arc™ A-Series discrete GPUs (Codename Alchemist) Intel® Arc™ GPUs in Intel® Core™ Ultra 2 series processors (Codename Lunar Lake and Arrow Lake H) Intel® Arc™ GPUs in Intel® Core™ Ultra processors (Codename Meteor Lake) OS Support: Microsoft Windows® 10-64 - October 2022 Update (22H2) Microsoft Windows 11*-64 - October 2021 Update (21H2) Microsoft Windows 11-64 - September 2022 Update (22H2) Microsoft Windows 11-64 - October 2023 Update (23H2) Microsoft Windows 11-64 - October 2024 Update (24H2) Microsoft Windows 11 64 - September 2025 Update (25H2) Platform Support: Intel® Arc™ Graphics family (Codename Alchemist, Battlemage) Intel® Core™ Ultra processor family (Codename Meteor Lake, Lunar Lake, Arrow Lake-S, Arrow Lake-H, Panther Lake) Notes See a list of computer manufacturer support websites. Visit the Historical Drivers page for a list of older drivers.",
      "cover_image_url": "https://www.intel.com/content/dam/logos/logo-energyblue-1x1.png"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Blood test boosts Alzheimer's diagnosis accuracy to 94.5%, clinical study shows",
      "url": "https://medicalxpress.com/news/2026-02-blood-boosts-alzheimer-diagnosis-accuracy.html",
      "published": "2026-02-24T03:10:16+00:00",
      "summary": "<p>Article URL: <a href=\"https://medicalxpress.com/news/2026-02-blood-boosts-alzheimer-diagnosis-accuracy.html\">https://medicalxpress.com/news/2026-02-blood-boosts-alzheimer-diagnosis-accuracy.html</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47132388\">https://news.ycombinator.com/item?id=47132388</a></p> <p>Points: 247</p> <p># Comments: 93</p>",
      "content_text": "p-tau217 tests significantly increased diagnostic confidence in patients with cognitive symptoms. Credit: Pexels A protein lurking around in the blood can help with the accurate diagnosis of Alzheimer's disease. In a recent study , researchers from Spain investigated how blood-based biomarkers, such as a protein called p-tau217, affect both the clinical diagnosis of Alzheimer's and neurologists' confidence in their diagnosis. After following 200 consecutive new patients aged 50 and older who presented with cognitive symptoms, they found that a simple blood test measuring p-tau217 significantly improved diagnostic accuracy in routine clinical practice. When relying solely on standard clinical evaluation, doctors correctly diagnosed Alzheimer's in 75.5% of cases, but when incorporating blood test results, diagnostic accuracy increased to 94.5%. The findings are published in the Journal of Neurology . Better path to Alzheimer's diagnosis Phosphorylated tau, or p-tau217, is a protein that naturally occurs in the brain and helps keep neurons, the cells that carry signals, stable and healthy. The trouble begins when this protein becomes abnormally phosphorylated and clumps together, forming tangles that disrupt communication between brain cells. Over time, this damage can impact brain function and lead to neurodegenerative conditions such as Alzheimer's disease. While p-tau217 is not considered the direct cause of Alzheimer's, elevated levels in the blood are now recognized as one of the most accurate early warning signs of the disease. In many parts of the world, the population is rapidly aging and so is the number of age-related diseases like Alzheimer's and dementia. However, most of the standard ways to diagnose Alzheimer's today, like expensive brain scans or invasive spinal taps, are costly, uncomfortable, and often hard for patients to access. Scientists have long known that p-tau217 is a reliable biomarker for detecting early signs of Alzheimer's, but most of these data come from highly controlled research labs. How well it works in everyday medical clinics and whether it truly boosts doctors' confidence in their diagnoses remain less explored. Distribution of changes in diagnostic confidence following biomarker disclosure, categorized as improvement, no change, or worsening, according to the clinical setting (A) and cognitive stage (B) (subjective cognitive decline, mild cognitive impairment, and dementia). Credit: J Neurol (2026). DOI: 10.1007/s00415-026-13676-6 In this study, the researchers focused on both these factors in real-world medical settings. They followed patients who came in for general neurology consultations and to a specialized cognitive neurology unit with cognitive symptoms. Clinicians noted their initial diagnosis and how confident they felt about it, then reviewed the p-tau217 blood test results and recorded any changes. The team found that after reviewing the p-tau217 results, diagnostic accuracy jumped by 19%. For about one in four patients, the blood test prompted doctors to change their diagnosis. Some people who were first believed to have Alzheimer's turned out to have a different condition, while others who were thought to be experiencing normal aging were correctly identified as having Alzheimer's. Also, the doctors' confidence in their diagnoses rose from an average of 6.90 to 8.49 on a 10-point scale. The p-tau217 tests proved to be effective across every stage of cognitive decline, be it early memory complaints or late-stage decline such as dementia. The findings show that this blood test could provide a more accurate and less invasive way to diagnose Alzheimer's, potentially improving care for millions of people. Written for you by our author Sanjukta Mondal , edited by Gaby Clark , and fact-checked and reviewed by Robert Egan —this article is the result of careful human work. We rely on readers like you to keep independent science journalism alive. If this reporting matters to you, please consider a donation (especially monthly). You'll get an ad-free account as a thank-you. Publication details Jordi A. Matías-Guiu et al, Impact of blood p-tau217 testing on diagnosis and diagnostic confidence in cognitive disorders: a real-world clinical study, Journal of Neurology (2026). DOI: 10.1007/s00415-026-13676-6 Journal information: Journal of Neurology © 2026 Science X Network Citation : Blood test boosts Alzheimer's diagnosis accuracy to 94.5%, clinical study shows (2026, February 22) retrieved 24 February 2026 from https://medicalxpress.com/news/2026-02-blood-boosts-alzheimer-diagnosis-accuracy.html This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no part may be reproduced without the written permission. The content is provided for information purposes only.",
      "cover_image_url": "https://scx2.b-cdn.net/gfx/news/hires/2026/blood-test-boosts-alzh.jpg"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "x86CSS",
      "url": "https://lyra.horse/x86css/",
      "published": "2026-02-24T02:27:14+00:00",
      "summary": "<p>Article URL: <a href=\"https://lyra.horse/x86css/\">https://lyra.horse/x86css/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47132102\">https://news.ycombinator.com/item?id=47132102</a></p> <p>Points: 121</p> <p># Comments: 45</p>",
      "content_text": "About x86CSS is a working CSS-only x86 CPU/emulator/computer. Yes, the Cascading Style Sheets CSS. No JavaScript required. What you're seeing above is a C program that was compiled using GCC into native 8086 machine code being executed fully within CSS. GitHub ⧸ Fedi , Bluesky , Twitter Frequently Asked Questions Is CSS a programming language? Do you really need to ask at this point? How?? I plan on writing a blog post that explains how this works as well as many of the tricks used. Bookmark my blog or add it to your RSS reader. Surely you still need a little bit of JavaScript? Nope, this is CSS-only! There is a script tag on this site, which is there to provide a clock to the CSS - but this is only there to make the entire thing a bit faster and more stable. The CSS also has a JS-less clock implementation, so if you disable scripts on this site, it will still run. JavaScript is not required. My CSS clock uses an animation combined with style container queries, which means you don't need to interact with anything for the program to run, but it also means its a bit slower and less stable as a result. A hover-based clock, such as the one in Jane Ori's CPU Hack , is fast and stable, but requires you to hold your mouse on the screen, which some people claim does not count as turing complete for whatever reason, so I wanted this demo to be fully functional with zero user input. But you still need HTML, right? Not really... well, kind of? This entire CPU runs in just CSS and doesn't require any HTML code, but there is no way to load the CSS without a <style> tag, so that much is required. In Firefox it is possible to load CSS with no HTML, but atm this demo only works in Chromium-based browsers. What preprocessor do you use? I straight up just write CSS! The CSS in base_template.html is handwritten in Sublime Text, but for the more repetitive parts of the code I wrote a python script . Is this practical? Not really, you can get way better performance by writing code in CSS directly rather than emulating an entire archaic CPU architecture. It is fun though, and computers are made for art and fun! Can I write/run my own programs? Yes, but you'll have to compile them yourself. See below. What's x86? x86 is the CPU architecture most computers these days run on. Heavily simplified, this demo runs the same machine code in CSS that your computer does in its processor. To be fair, modern x86 is 64bit and has a bunch of useful extensions, so it's not quite the same - this here is the original 16bit x86 that ran on the 8086 . What's horsle? neigh . Compatibility This project implements most of the x86 architecture, but not literally every single instruction and quirk, because a lot of it is unnecessary and not worth adding. The way I approached this project was by writing programs I wanted to run in C, compiling them in GCC with various levels of optimization, and then implementing every instruction I needed. This way I know I have everything I need implemented. There is some behaviour that's wrong, and some things are missing (e.g. setting the CF/OF flag bits). That's okay. - 0 1 2 3 4 5 6 7 8 9 A B C D E F 0 ADD ADD ADD ADD ADD ADD PUSH POP OR OR OR OR OR OR PUSH -- 1 ADC ADC ADC ADC ADC ADC PUSH POP SBB SBB SBB SBB SBB SBB PUSH POP 2 AND AND AND AND AND AND ES: DAA SUB SUB SUB SUB SUB SUB CS: DAS 3 XOR XOR XOR XOR XOR XOR SS: AAA CMP CMP CMP CMP CMP CMP DS: AAS 4 INC INC INC INC INC INC INC INC DEC DEC DEC DEC DEC DEC DEC DEC 5 PUSH PUSH PUSH PUSH PUSH PUSH PUSH PUSH POP POP POP POP POP POP POP POP 6 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 7 JO JNO JB JNB JZ JNZ JBE JA JS JNS JPE JPO JL JGE JLE JG 8 GRP1 GRP1 GRP1 GRP1 TEST TEST XCHG XCHG MOV MOV MOV MOV MOV LEA MOV POP 9 NOP XCHG XCHG XCHG XCHG XCHG XCHG XCHG CBW CWD CALL WAIT PUSHF POPF SAHF LAHF A MOV MOV MOV MOV MOVSB MOVSW CMPSB CMPSW TEST TEST STOSB STOSW LODSB LODSW SCASB SCASW B MOV MOV MOV MOV MOV MOV MOV MOV MOV MOV MOV MOV MOV MOV MOV MOV C -- -- RET RET LES LDS MOV MOV -- -- RETF RETF INT INT INTO IRET D GRP2 GRP2 GRP2 GRP2 AAM AAD -- XLAT -- -- -- -- -- -- -- -- E LOOPNZ LOOPZ LOOP JCXZ IN IN OUT OUT CALL JMP JMP JMP IN IN OUT OUT F LOCK -- REPNZ REPZ HLT CMC GRP3a GRP3b CLC STC CLI STI CLD STD GRP4 GRP5 G ADD OR ADC SBB AND SUB XOR CMP ROL ROR RCL RCR SHL SHR -- SAR G TEST -- NOT NEG MUL IMUL DIV IDIV TEST -- NOT NEG MUL IMUL DIV IDIV G INC DEC -- -- -- -- -- -- INC DEC CALL CALL JMP JMP PUSH -- Compiling You can run your own software in this emulator! If you have 8086 assembly ready to go, clone my repo, and put the assembly in a file called program.bin . Then, put the path to the _start() function in program.start as a number. Once that's set, you can just run build_css.py with Python3 (no dependencies required!) and the output will be in x86css.html . If you want to write C code, you can do so using gcc-ia16 (you can build it yourself or install it from the PPA ). The build_c.py script does the build with the correct flags and also makes the program.bin/start files. Don't forget to run build_css.py after! This building setup works on both Linux and WSL1/2 (I haven't tried on macOS). By default there is 0x600 bytes (1.5kB) of memory, but this can be increased in the build_css.py file as necessary. The program gets loaded at memory address 0x100. There's some custom I/O addresses for you to be able to interface with the program. Here's an example program: static const char STR_4BYTES[] = \"hell\"; static const char STR_8BYTES[] = \"o world!\"; void (*writeChar1)(char); void (*writeChar4)(const char[4]); void (*writeChar8)(const char[8]); char (*readInput)(void); int _start(void) { // Set up custom stuff writeChar1 = (void*)(0x2000); writeChar4 = (void*)(0x2002); writeChar8 = (void*)(0x2004); readInput = (void*)(0x2006); int* SHOW_KEYBOARD = (int*)(0x2100); // Write a single byte to screen writeChar1(0x0a); // Write 4 bytes from pointer to screen writeChar4(STR_4BYTES); // Write 8 bytes from pointer to screen writeChar8(STR_8BYTES); // Write a character from custom charset writeChar1(0x80); while (1) { // Show numeric keyboard *SHOW_KEYBOARD = 1; // Read keyboard input char input = readInput(); if (!input) continue; *SHOW_KEYBOARD = 0; // Echo input writeChar1(input); break; } while (1) { // Show alphanumeric keyboard *SHOW_KEYBOARD = 2; char input = readInput(); if (!input) continue; *SHOW_KEYBOARD = 0; writeChar1(input); break; } return 1337; } Credits Greetz/thanks to: Your browser is unable to run this demo. Please try with an up-to-date Chromium-based browser.",
      "cover_image_url": "https://lyra.horse/x86css/static/thumbnail.png"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Iowa Farmers Are Leading the Fight for Repair",
      "url": "https://www.ifixit.com/News/115722/iowa-farmers-are-leading-the-fight-for-repair",
      "published": "2026-02-24T01:09:40+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.ifixit.com/News/115722/iowa-farmers-are-leading-the-fight-for-repair\">https://www.ifixit.com/News/115722/iowa-farmers-are-leading-the-fight-for-repair</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47131506\">https://news.ycombinator.com/item?id=47131506</a></p> <p>Points: 126</p> <p># Comments: 34</p>",
      "content_text": "If you eat food, you’re already in this story. John Deere is still trying to kill the Right to Repair, but Iowa farmers are pushing hard to retake their rights. On February 18, the Iowa House Agriculture Committee advanced a fix, HSB 751 , by an 18-5 vote. That’s a big margin, and it puts the bill on a very real path forward in one of the country’s largest agricultural producers, a state that accounts for about a fifth of US agricultural receipts . Right to Repair has passed for agriculture in Colorado and electronics in eight states, and so far this year, 55 Right to Repair bills are running the course through 20 state legislatures . Iowa might be first past the post, with an agricultural bill that’s sorely needed. Why Do We Need Agricultural Right to Repair? Why should you care if you don’t own a tractor? Because when a combine goes down in the middle of harvest, it means fewer acres cut before the rain hits. It means grain that doesn’t get hauled on time. We like to imagine the food supply as a smooth conveyor belt. In reality it’s tight timing, thin margins, and a lot of heavy machinery that increasingly runs on proprietary software. A modern combine can be mechanically fine and still be effectively broken, because the owner can’t access the diagnostic tools and software needed to complete the repair. The part might be sitting right there, but the “key” to make it work lives behind a dealer login. Agricultural Right to Repair aims to hand the keys back to farmers so they can get food back on our tables. Colorado was the first state to put that principle into law. In 2023 it passed the first-ever agricultural Right to Repair bill in the US. The law requires manufacturers like Deere to provide access to the same kinds of manuals, diagnostic tools, software, and parts that dealers use, on fair terms. And yet, even after Colorado, Deere has fought the spirit of Right to Repair. They concede a little, keep the high-leverage stuff gated, and call it a solution. Our advocacy partners at PIRG have covered the full play-by-play . Deere keeps promising “full repair access” and then kicking the can down the road. In order to hold their feet to the fire, we need to pass repair laws in more places. Farmers Just Want to Be Able to Fix Their $500k Machines Why is this an increasing problem? Software has created a specific gap in farmers’ abilities. Deere equipment relies on proprietary diagnostic software, and dealers use a special tool that is more capable than what the farmers have. That dealer-grade access includes deeper diagnostics, part calibrations, software updates, and specialty tests and resets. Farmers, meanwhile, have been a more limited tool that can be useful, but often stops short of the capabilities they need to complete the repair. PIRG’s Deere in the Headlights reporting has consistently pointed out that the gap isn’t just information. It’s missing capability, and the result is downtime. During harvest, downtime is a ticking clock. Iowa Put Deere on Notice Iowans are rolling up their sleeves. Chair Rep. Derek Wulf, a farmer himself, framed it plainly: farmers are problem-solvers. They fix things. The bill is about making solutions possible in a world where “broken” increasingly means “software-locked,” not “physically shattered.” The committee adopted a Wulf amendment aimed at dealer concerns about pricing mechanisms. The bill’s basic structure is “fair and reasonable” access, with parity to authorized repair providers and caps tied to MSRP. The amendment reportedly removed “at cost” language on parts to win dealer neutrality. That’s a familiar legislative trade, and it doesn’t change what’s at stake. Next up: The bill goes to the House floor. We think it’s got a good chance of passing. Deere’s Allies Are Stepping Back One meaningful change this year is that Iowa’s corn and soybean groups reportedly testified neutral on HSB 751, in a subcommittee meeting on Wednesday . In Iowa politics, that matters. If those groups actively oppose a bill, it usually dies. When they decline to fight it, the bill has oxygen. Neutral is not the same thing as supportive, but it does suggest the reflex to line up behind manufacturer control is weaker than it used to be. That matters because Deere’s standard strategy depends on using stand-ins to block progress. If the blockers stay out of the game, Deere’s flimsy arguments have to stand on their own. We Can’t Let Deere Run From This One This bill has a good chance of making it all the way through. That’s fantastic. If you buy a half-million-dollar machine, you should not need corporate permission to keep it running. Deere has made “permission” part of the product. Iowa farmers are trying to unbundle that. And in 2026, it looks like Deere’s usual tactics are running into a problem: they’re not working like they used to.Iowans, now’s the time to tell your legislators to support HSB 751 ! If you’re not in Iowa, weigh in on your state over at Repair.org or find a repair fight near you .",
      "cover_image_url": "https://valkyrie.cdn.ifixit.com/media/2023/03/16094247/deere_wrench_3x2.jpg"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "A Meta AI security researcher said an OpenClaw agent ran amok on her inbox",
      "url": "https://techcrunch.com/2026/02/23/a-meta-ai-security-researcher-said-an-openclaw-agent-ran-amok-on-her-inbox/",
      "published": "2026-02-24T00:57:14+00:00",
      "summary": "The viral X post from an AI security researcher reads like satire. But it's really a word of warning about what can go wrong when handing tasks to an AI agent.",
      "content_text": "The now-viral X post from Meta AI security researcher Summer Yue reads, at first, like satire. She told her OpenClaw AI agent to check her overstuffed email inbox and suggest what to delete or archive. The agent proceeded to run amok. It started deleting all her email in a “speed run” while ignoring her commands from her phone telling it to stop. “I had to RUN to my Mac mini like I was defusing a bomb,” she wrote, posting images of the ignored stop prompts as receipts. The Mac Mini, an affordable Apple computer that sits flat on a desk and fits in the palm of your hand , has become the favored device these days for running OpenClaw. (The Mini is selling “like hotcakes,” one “confused” Apple employee apparently told famed AI researcher Andrej Karpathy when he bought one to run an OpenClaw alternative called NanoClaw.) OpenClaw is, of course, the open source AI agent that achieved fame through Moltbook, an AI-only social network. OpenClaw agents were at the center of that now largely debunked episode on Moltbook in which it looked like the AIs were plotting against humans. But OpenClaw’s mission, according to its GitHub page , is not focused on social networks. It aims to be a personal AI assistant that runs on your own devices. The Silicon Valley in-crowd has fallen so in love with OpenClaw that “claw” and “claws” have become the buzzwords of choice for agents that run on personal hardware. Other such agents include ZeroClaw , IronClaw , and PicoClaw . Y Combinator’s podcast team even appeared on their most recent episode dressed in lobster costumes. Techcrunch event Boston, MA | June 9, 2026 But Yue’s post serves as a warning. As others on X noted, if an AI security researcher could run into this problem, what hope do mere mortals have? “Were you intentionally testing its guardrails or did you make a rookie mistake?” a software developer asked her on X. “Rookie mistake tbh,” she replied. She had been testing her agent with a smaller “toy” inbox, as she called it, and it had been running well on less important email. It had earned her trust, so she thought she’d let it loose on the real thing. Yue believes that the large amount of data in her real inbox “triggered compaction,” she wrote. Compaction happens when the context window — the running record of everything the AI has been told and has done in a session — grows too large, causing the agent to begin summarizing, compressing, and managing the conversation. At that point, the AI may skip over instructions that the human considers quite important. In this case, it may have skipped her last prompt — where she told it not to act — and reverted back to its instructions from the “toy” inbox. As several others on X pointed out , prompts can’t be trusted to act as security guardrails. Models may misconstrue or ignore them. Various people offered suggestions that ranged from the exact syntax Yue should have used to stop the agent, to various methods to ensure better adherence to guardrails, like writing instructions to dedicated files or using other open source tools. In the interest of full transparency, TechCrunch could not independently verify what happened to Yue’s inbox. (She didn’t respond to our request for comment, though she did respond to many questions and comments sent her way on X.) But it doesn’t really matter. The point of the tale is that agents aimed at knowledge workers, at their current stage of development, are risky. People who say they are using them successfully are cobbling together methods to protect themselves. One day, perhaps soon (by 2027? 2028?), they may be ready for widespread use. Goodness knows many of us would love help with email, grocery orders, and scheduling dentist appointments. But that day has not yet come.",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2026/02/Y-Combinator-Crab.png?resize=1200,829"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Steerling-8B: The First Inherently Interpretable Language Model",
      "url": "https://www.guidelabs.ai/post/steerling-8b-base-model-release/",
      "published": "2026-02-24T00:38:02+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.guidelabs.ai/post/steerling-8b-base-model-release/\">https://www.guidelabs.ai/post/steerling-8b-base-model-release/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47131225\">https://news.ycombinator.com/item?id=47131225</a></p> <p>Points: 158</p> <p># Comments: 38</p>",
      "content_text": "Author: Guide Labs Team Published: February 23, 2026 We are releasing Steerling-8B, the first interpretable model that can trace any token it generates to its input context, concepts a human can understand, and its training data. Trained on 1.35 trillion tokens, the model achieves downstream performance within range of models trained on 2–7× more data. Steerling-8B unlocks several capabilities which include suppressing or amplifying specific concepts at inference time without retraining, training data provenance for any generated chunk, and inference-time alignment via concept control, replacing thousands of safety training examples with explicit concept-level steering. Overview For the first time, a language model, at the 8-billion-parameter scale, can explain every token it produces in three key ways. More specifically, for any group of output tokens that Steerling generates, we can trace these tokens to: [Input context] the prompt tokens, [Concepts] human-understandable topics in the model’s representations, and [Training data] the training data drove the output. Artifacts We are releasing the weights of a base model trained on 1.35T tokens as well as companion code to interact and play with the model. Steerling-8B in Action Below we show Steerling-8B generating text from a prompt across various categories. You can select an example, then click on any highlighted chunk of the output. The panel below will update to show: Input Feature attribution: which tokens in the input prompt strongly influenced that chunk. Concept attribution: the ranked list of concepts, both tone (e.g. analytical, clinical ) and content (e.g. Genetic alteration methodologies), that the model routed through to produce that chunk. Training data attribution: how the concepts in that chunk distribute across training sources (ArXiv, Wikipedia, FLAN, etc.), showing where in the training data the model’s knowledge originates. Overview Steerling is built on a causal discrete diffusion model backbone, which lets us steer generation across multi-token tokens rather than only at the next-token. The key design choice is decomposing the model’s embeddings into three explicit pathways: ~33K supervised “known” concepts, ~100K “discovered” concepts the model learns on its own, and a residual that captures whatever remains. We then constrain the model with training loss functions that ensure the model routes signal through concepts without a fundamental tradeoff with performance. The concepts feed into logits through a linear path, every prediction decomposes exactly into per-concept contributions, and we can edit those contributions at inference time without retraining. For the full architecture, training objectives, and scaling analysis, see Scaling Interpretable Models to 8B . Performance Despite being trained on significantly fewer compute than comparable models, Steerling-8B achieves competitive performance across standard benchmarks. The figure below shows average performance (across 7 benchmarks) versus approximate training FLOPs on a log scale, with vertical lines marking multiples of Steerling’s compute budget. Steerling outperforms both LLaMA2-7B and Deepseek-7B on overall average despite using fewer FLOPs, and remains within range of models trained with 2–10× more compute. Steerling performance across various benchmarks ranging from general purpose question answering to those focused on reasoning and math. Interpretability In the previous update , we shared several ways that assess how interpretable a model’s representations are. Here we provide another metric that gives insight into the model’s use of its concepts. On a held-out validation set, over 84% of token-level contribution comes from the concept module: the model is not just using the residual to make its predictions. This matters for control: if the model’s predictions genuinely flow through concepts, then editing those concepts at inference time actually changes what the model does rather than nudging a side channel while the real work happens elsewhere. Token level logit distribution of Steerling-8B’s activations on a held-out validation set. Over 84% of token-level contribution comes from the concept module. A useful check is what happens when we remove the residual pathway. On several LM Harness tasks, dropping the residual has only a small effect, which suggests the model’s predictive signal is largely routed through concepts rather than hidden “everything-else” channels. Change in model performance across a variety of benchmarks with and without the model’s residual portion. This indicates that the model mostly relies on concepts, both supervised or discovered, for its outputs. Finally, Steerling can detect known concepts in text with 96.2% AUC on a held-out validation dataset. What this unlocks In the coming weeks, we’ll be releasing deep dives on each of these capabilities: Concept steering : precise control via intervention; Concept discovery : what did Steerling learn that we didn’t teach it? We’ll open up the discovered concept space and show structure that surprised us. Alignment without fine-tuning : replace thousands of safety training examples with a handful of concept-level interventions. Memorization & training data valuation : trace any generation back to the training data that produced it, and assign value to individual data sources. The case for inherent interpretability : what do you gain when interpretability is designed in from the start, and what do you miss when it’s bolted on after the fact? We’ll cover each of these in detail in upcoming posts, with quantitative evaluations and deployment-oriented case studies.",
      "cover_image_url": "https://www.guidelabs.ai/logo/guidelabs-social-media-thumbnail.png"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "Tesla's battle with the California Department of Motor Vehicles isn't over after all",
      "url": "https://techcrunch.com/2026/02/23/teslas-battle-with-the-california-department-of-motor-vehicles-isnt-over-after-all/",
      "published": "2026-02-24T00:36:12+00:00",
      "summary": "Tesla has filed a lawsuit against the California DMV in the ongoing battle around Autopilot.",
      "content_text": "Tesla has filed a lawsuit against the California Department of Motor Vehicles in an attempt to overturn an agency ruling. The state DMV ruled that Tesla used deceptive marketing to overstate the automated driving capabilities of its vehicles, thereby violating state law. The lawsuit reignites an issue that appeared to be resolved last week when the DMV said it would not suspend Tesla’s sales and manufacturing licenses for 30 days. This was because the EV maker complied with the ruling and stopped using the term “Autopilot” in its California marketing materials. CNBC was first to report the lawsuit . The DMV could have taken action against Tesla. It chose not to even though an administrative law judge agreed with the DMV’s request to suspend Tesla’s licenses for 30 days as a penalty. Instead of pulling its licenses, the state regulator gave Tesla 60 days to comply. And Tesla did, although in the most extreme ways. Tesla didn’t just stop using the term Autopilot; in January it discontinued Autopilot altogether in the U.S. and Canada. Perhaps it regrets that decision and is looking for a way to bring it back.",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2025/08/tesla-california-getty.jpg?resize=1200,831"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "William Shatner is making an album with 35 metal icons",
      "url": "https://www.guitarworld.com/artists/guitarists/william-shatner-announces-all-star-metal-album",
      "published": "2026-02-24T00:33:06+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.guitarworld.com/artists/guitarists/william-shatner-announces-all-star-metal-album\">https://www.guitarworld.com/artists/guitarists/william-shatner-announces-all-star-metal-album</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47131173\">https://news.ycombinator.com/item?id=47131173</a></p> <p>Points: 182</p> <p># Comments: 81</p>",
      "content_text": "You might know him as Captain James T. Kirk, the cavalier captain of the Starship Enterprise from Star Trek, but believe it or not, William Shatner is a true heavy metal warrior – and he has just announced an all-star metal album to prove it. Given all that he has been through – all those times Montgomery Scott beamed him through the transporter, the long-running feuds with Klingons and Romulans – we can forgive Shatner for not holding the electric guitar in the orthodox fashion. We might even forgive him for holding what looks like an AI-generated Gibson Les Paul or perhaps one from the Chibson Custom Shop – clock those melting upper-fret inlays or, for that matter, the mere fact there are inlays on the 11th and 13th frets at all. But one condition. This as-yet-untitled album, with an as-yet-unannounced cast of metal guitar talents, better not suck. Shatner sounds bullish on that score. “Metal has always been a place where imagination gets loud,” he says. “This album is a gathering of forces – each artist bringing their fire, their precision, their chaos. I chose them because they have something to say, and because metal demands honesty.” This, reads the statement, is not a “novelty album”. Tune in for “massive guitars, cinematic arrangements”, and for “sharp turns, dark humor, raw emotion, and moments of unexpected beauty”; this, dear reader, is life itself. Shatner got the inspiration for the project after collaborating with former Megadeth guitarist Chris Poland , delivering a spoken word intro a la Barry Clayton for the new album from Nuclear Messiah, Black Flame . All the latest guitar news, interviews, lessons, reviews, deals and more, direct to your inbox! “When Nuclear Messiah came to life, something clicked,” says Shatner. “It wasn’t just a track – it was a doorway. It made me want to go all the way in, bring in the best metal players I could find, and create something fearless.” No names have been officially attached just yet, but part of the inspiration for the project was the guitar Zakk Wylde personally gifted the Star Trek star – a gesture Shatner describes as unexpected and deeply motivating. That makes the former Ozzy guitarist a shoo-in for the record. Ditto, Chris Poland; we’d bet his legato will be dripping all over at least one of these tracks. As for the rest? Well, it’s from a roster of 35 quote/unquote metal icons, each hand-picked by Shatner – as though he has summoned the best and the brightest from The Metal Archives and lined them up in a carpark, Shatner beatifically gliding along on a Segway, pausing only to tap the chosen ones on the shoulder. William Shatner music video for \"Common People\" - YouTube Watch On Shatner says there will be some choice covers of Black Sabbath , Judas Priest and Iron Maiden standards, but there will be all-original material, too. He knows what he is doing with these all-star collaborations. In the past he has worked with a diverse cast of players, with 2004’s Has Been featuring the likes of Henry Rollins, Adrian Belew , Brad Paisley, the wonderful Aimee Mann and more – including his collab with Pulp on Common People . Paisley returned for 2021’s Bill , joining Joe Walsh and Robert Randolph in the studio. So, we wait with bated breath. We’ll bring you more details on this Shatner-fronted metal project as soon as they arrive. To paraphrase the original ‘Metal Warriors’: Klingons and Romulans leave the hall, heavy metal or no metal at all!",
      "cover_image_url": "https://cdn.mos.cms.futurecdn.net/j9NxkNJW9i3m6SkJbcStQM-1200-80.jpg"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "I ported Coreboot to the Thinkpad X270!",
      "url": "https://dork.dev/posts/2026-02-20-ported-coreboot/",
      "published": "2026-02-23T23:58:45+00:00",
      "summary": "<p>Article URL: <a href=\"https://dork.dev/posts/2026-02-20-ported-coreboot/\">https://dork.dev/posts/2026-02-20-ported-coreboot/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47130860\">https://news.ycombinator.com/item?id=47130860</a></p> <p>Points: 191</p> <p># Comments: 30</p>",
      "content_text": "I ported Coreboot to the Thinkpad X270! Posted on: 2026-02-20T17:39Z Tags: Table of Contents In my post from 2026-02-18, I committed myself towards helping work on coreboot + libreboot with the goal of porting it to the X270. It’s less than a week later and I have done it. My X270 is a 20HM model and this means that it is a Kaby Lake CPU (and chipset), not Skylake. § The process I started by dumping the BIOS image from the X270, initially for two reasons (although there are more): to have a backup in case anything went wrong to obtain the Intel Management Engine section to produce deltas for deguard Remaining reasons that are important later are: the GbE section of the BIOS is necessary for ethernet the IFD (Intel Flash Descriptor) is necessary for producing a finished image I set up pico-serprog on an RP2040-zero, which I found builds for from the Libreboot project website. This, in combination with flashprog is what I used to dump and write to the SPI flash for the X270. Setup for flashing; a SOIC-8 chip clip and an RP2040 zero. § Trouble strikes It turns out, in the process of attempting to clip onto the laptop, I knocked off a capacitor. A capacitor missing from where it’s supposed to be I didn’t know what capacitor, either, because trying to resolder it in place, it pinged off of my tweezers into orbit to never be seen again. § Identification I used silkscreen markings and referenced a schematic (which I obtained for the purpose of fixing this missing capacitor to begin with) to find the area of the board I had likely damaged. The marking “PJ304” came in helpful. The relevant part of the schematic for the board section The component in question, a 10uF 0603 format 6.3V capacitor By checking what pin on the chip below the tweezers went to the leads on the capacitor that had been accidentally ripped off and the neighbouring capacitors, it was possible to determine these capacitors were put in parallel between ground and a particular pin of the chip, allowing me to narrow it down to the above. Digikey, then, comes in clutch at like C$10 for 10 capacitors and next day shipping. § Back to business, then? I wanted to make sure that I had something valid so I checked the string content of the flash. Strings from the BIOS dump Sure, seems coherent. I then examined the file with ifdtool, using the dump switch. ifdtool showing an Intel Flash Descriptor From here, I followed the instructions in the deguard README, producing this commit in the process. You can use a patched version of the Intel Management Engine to produce the deltas and this was something prior that was unclear to me, and an interpretation another friend of mine had. It isn’t the case that it has to also be vulnerable, at all! ifdtool being used to extract regions of the image These regions being extracted separately is quite important, the flash descriptor and GbE are both very necessary to produce a final image that functions. I had also tried to figure out how deguard worked being applied to the donor image, to understand the whole system better. § Differences between the X270 and X280 - part 1 Now, I knew the X280 and X270 differed by two things, initially: The X270 lacks Thunderbolt (TBT). The X280 has some form of soldered RAM, the X270 has a single SODIMM slot. We will see some problems later, but initially, basing my work off of the X280 variant, I made changes to it where I disabled the Thunderbolt-related pins (while regarding the official Intel documentation for the generation of chipset; for example one of the TBT pins was thermal management for my machine, so it was fine to set the pin to NONE instead of something more complicated. I was seeing things lining up on the GPIO pins (gpio.h), aside from the Thunderbolt controller related pins. I did notice during this experience, referencing between the X280 and X270 schematics that the MEC1653 was something the X270 had, but the X280 had the MEC1663. This meant that WWAN_DISABLE was one of the only lines coming off of the Wireless section of the MEC1663 on the X280, but there were more lines on the X270 than that. This is unrelated, I just thought it was neat that there were so many different versions of the MEC16xx, lots of derivatives? They all seem well supported by the same MEC1653 driver in coreboot. I got something to build and then I decided to go to bed for the night. A built libreboot imageset The next day, I went with my wife to pick up my capacitors (I had missed them the day prior, whoops). § Why can’t my board boot off of NVME? Uh oh. A suspiciously shitty looking capacitor Wow, a suspiciously shitty looking capacitor. To be fair, that is 0.8x1.6mm? It’s pretty small. (This was not the only time this capacitor fell off, I had to fix it again afterwards at least 8 attempts of flashing later.) I had something that would actually get to payloads, but when I would select the NVMe NS 1 it would very, very quickly fail out. When I went back to the SeaBIOS menu, I noticed the NVMe NS 1 option then missing. Booting into a LiveUSB, I discovered I had neither the WiFi card or the NVMe present in lspci . I had posted in #libreboot on liberachat a little earlier that I had the X270 booting, however I realized pretty quickly after that it was not all sunshine and rainbows. § Asking for help After a while of talking about the X270 and potential need for upstreaming now, I started speaking about the issue and Leah Rowe, founder of Libreboot (and CanoeBoot and Minifree Ltd) worked with me on attempting to diagnose and fix the problem, producing several ROMs for me given that I potentially may have done things incorrectly. In the end, even with a mostly intact (albeit HAP-bit and Deguarded) Intel Management Engine, it was likely not anything to do with the IME like I was theorizing (I had read an issue for the t470s about being careful with me_cleaner and how truncation caused problems with NVMe and WiFi dropouts. I have to assume this was actually something that could’ve been mitigated with --whitelist MFS ). I’m very thankful for the patience shown towards me, to be frank. ^^; § Differences between the X270 and X280 - part 2 I was pretty upset by this point, but I woke up to give it a try the next day. At this point, we had figured that the problem was likely to do with PCIe allocations and perhaps the overridetree.cb? Looking into this and the schematic later in the day together with my wife, I ended up noticing that CLKREQ4 on the X280 schematic led nowhere. On the X270, the WLAN card had two CLKOUT and CLKREQ connections. Looking deeper into this, there was a table showing the separation of the WiGig and WiFi card into two separate PCIe devices despite being contained within the one card. Going down this route, I figured that using CLKREQ1 for WiFi was incorrect and that CLKREQ2 was the appropriate one. Given this, it also made the rest of the CLKREQ selections then further by one, too. Adjusting for this and regarding the WWAN allocation within the schematic, I made a new build with these adjustments and flashed it. I was greeted with… a working GRUB. GRUB prompt with a cleaner screen A full disk encryption prompt from Guix’s GRUB A Guix system booted, showing cbmem -1 on one half of the screen Guix booted and I was able to read cbmem -1 from within Guix System, showing the Libreboot console log. Wireless (albeit, proprietary) worked! NVMe worked! § Where from here? I am starting to upstream my changes: For the X270 in particular, I got an ath9k wireless dongle so I should now be able to move to linux-libre on my Guix install. I’d like to build it into the laptop if possible in the future through some means and I’ll see about doing that, honestly? I can’t recommend libreboot enough, or even heads if libreboot isn’t your speed. A big thanks to Leah Rowe for their assistance and the work they have done for libreboot over the years.",
      "cover_image_url": "flasher-setup.thumb.png"
    },
    {
      "industry": "technology",
      "source": "Wired",
      "title": "Start Your Surround Sound Journey With $50 off This Klipsch Soundbar",
      "url": "https://www.wired.com/story/klipsch-flexus-core-200-deal-226/",
      "published": "2026-02-23T23:11:41+00:00",
      "summary": "This soundbar is just the beginning, with the option to add wireless bookshelf speakers or a subwoofer.",
      "content_text": "If you're tired of listening to the crackle from the speakers on the back of your TV but aren't ready for the full subwoofer-boosted suite, I've got a good deal for you. The Klipsch Flexus Core 200 is currently marked down by $50 at Amazon , and itâ€™s a great place to start if you're looking for a soundbar that will give you options down the road. It has fewer channels built into the sound bar than some of our other favorite picks, notably lacking the side-firing drivers that help with surround effects. That doesn't keep it from sounding excellent, thanks to its 44-inch wide footprint and 2.25-inch drivers that reach all the way to either end. Our reviewer Ryan Waniata was impressed by the Core 200's clarity and detail, and in particular called out the very punchy bass response. While the bar has built-in controls for simple tasks like changing the volume and inputs, you can also use the mobile app to fine tune your audio experience. In addition to the stuff you'd expect, there's also a three-band equalizer for those who like to fiddle and advanced settings for any extra speakers you add to the setup. With eARC to communicate with your TV, you shouldn't need to touch the remote or app often anyway. That's right, one of the biggest selling points for the Klipsch Flexus Core 200 is the ability to add additional speakers to your setup. Both the Klipsch Flexus Surr 100 bookshelf speakers and Klipsch Flexus Sub 100 connect wirelessly to the Core 200 with a custom dongle, giving you a ton of freedom to stash the extra speakers wherever they'd sound best. If you have your own subwoofer that you like, there's also an RCA jack on the bar to hook it up. That's a lot of flexibility for any soundbar, let alone one at this price point. If you're ready to get the ball rolling on a proper sound system for your next movie night, you can save $50 on the Flexus Core 200 , or meander over to our roundup of the best soundbars we've tested to find the best option for you.",
      "cover_image_url": "https://media.wired.com/photos/699ca7a05317636b06014b46/191:100/w_1280,c_limit/Start%20Your%20Surround%20Sound%20Journey%20With%20$50%20off%20This%20Soundbar.png"
    },
    {
      "industry": "technology",
      "source": "VentureBeat",
      "title": "Google clamps down on Antigravity 'malicious usage', cutting off OpenClaw users in sweeping ToS enforcement move",
      "url": "https://venturebeat.com/orchestration/google-clamps-down-on-antigravity-malicious-usage-cutting-off-openclaw-users",
      "published": "2026-02-23T23:01:00+00:00",
      "summary": "<p>Google caused controversy among some developers this weekend and today, Monday, February 23rd, after restricting their usage of<a href=\"https://venturebeat.com/ai/google-antigravity-introduces-agent-first-architecture-for-asynchronous\"> its new Antigravity</a> &quot;vibe coding&quot; platform, alleging &quot;maliciously usage.&quot; </p><p>S<!-- -->ome users who had been using the open source autonomous AI agent OpenClaw in conjunction with agents built on Antigravity, as well as those who had connected OpenClaw agents to their Gmails, claimed on social media that they lost access to their Google accounts. </p><p>According to Google, said users had been using Antigravity to access a larger number of Gemini tokens via third-party platforms like OpenClaw, which overwhelmed the system for other Antigravity customers. </p><p>This move has cut off several users, underscoring the architectural and trust issues that can arise with OpenClaw. <!-- -->The timing of Google’s crackdown is particularly pointed. Just one week ago, on February 15,<a href=\"https://venturebeat.com/technology/openais-acquisition-of-openclaw-signals-the-beginning-of-the-end-of-the\"> OpenAI CEO Sam Altman announced that OpenClaw creator Peter Steinberger had joined OpenAI</a> to lead its “next generation of personal agents.” While OpenClaw remains an open-source project under an independent foundation, it is now financially backed and strategically guided by Google’s primary rival. </p><p>By cutting off OpenClaw’s access to Antigravity, Google isn’t just protecting its server load; it is effectively severing a pipeline that allows an OpenAI-adjacent tool to leverage Google’s most advanced Gemini models.</p><p>Google DeepMind engineer and former CEO and founder of Windsurf, Varun Mohan, <a href=\"https://x.com/_mohansolo/status/2025766889205739899?s=20\">said in an X post</a> that the company noticed “malicious usage” that led to service degradation.</p><p>“We’ve been seeing a massive increase in malicious usage of the Antigravity backend that has tremendously degraded the quality of service for our users. We needed to find a path to quickly shut off access to these users that are not using the product as intended. We understand that a subset of these users were not aware that this was against our ToS [Terms of Service] and will get a path for them to come back on but we have limited capacity and want to be fair to our actual users,” the post said. </p><div></div><p>A Google DeepMind spokesperson told VentureBeat that the move is not to permanently ban the use of Antigravity to access third-party platforms, but to align its use with the platform’s terms of service. </p><p>Unsurprisingly, Google’s move has caused a furor among OpenClaw users, including from OpenClaw creator Peter Steinberger, who announced that OpenClaw will remove Google support as a result. </p><div></div><h2><b>Infrastructure and connection uncertainty</b></h2><p>OpenClaw <a href=\"https://venturebeat.com/technology/what-the-openclaw-moment-means-for-enterprises-5-big-takeaways\">emerged as a way for individual users</a> to run shell commands and access local files, <a href=\"https://venturebeat.com/technology/openais-acquisition-of-openclaw-signals-the-beginning-of-the-end-of-the\">fulfilling a major promise</a> of AI agents: efficiently running workflows for users. </p><p>But, as VentureBeat has <a href=\"https://venturebeat.com/security/how-to-test-openclaw-without-giving-an-autonomous-agent-shell-access-to-your\">frequently pointed out</a>, it can often run into security and guardrail issues. There are companies building ways for <a href=\"https://venturebeat.com/orchestration/runlayer-is-now-offering-secure-openclaw-agentic-capabilities-for-large\">enterprise customers to access</a> OpenClaw securely and with a governance layer, though OpenClaw is so new that we should expect more announcements soon.</p><p>However, Google’s move was not framed as a security issue but rather as one of access and ru",
      "content_text": "<p>Google caused controversy among some developers this weekend and today, Monday, February 23rd, after restricting their usage of<a href=\"https://venturebeat.com/ai/google-antigravity-introduces-agent-first-architecture-for-asynchronous\"> its new Antigravity</a> &quot;vibe coding&quot; platform, alleging &quot;maliciously usage.&quot; </p><p>S<!-- -->ome users who had been using the open source autonomous AI agent OpenClaw in conjunction with agents built on Antigravity, as well as those who had connected OpenClaw agents to their Gmails, claimed on social media that they lost access to their Google accounts. </p><p>According to Google, said users had been using Antigravity to access a larger number of Gemini tokens via third-party platforms like OpenClaw, which overwhelmed the system for other Antigravity customers. </p><p>This move has cut off several users, underscoring the architectural and trust issues that can arise with OpenClaw. <!-- -->The timing of Google’s crackdown is particularly pointed. Just one week ago, on February 15,<a href=\"https://venturebeat.com/technology/openais-acquisition-of-openclaw-signals-the-beginning-of-the-end-of-the\"> OpenAI CEO Sam Altman announced that OpenClaw creator Peter Steinberger had joined OpenAI</a> to lead its “next generation of personal agents.” While OpenClaw remains an open-source project under an independent foundation, it is now financially backed and strategically guided by Google’s primary rival. </p><p>By cutting off OpenClaw’s access to Antigravity, Google isn’t just protecting its server load; it is effectively severing a pipeline that allows an OpenAI-adjacent tool to leverage Google’s most advanced Gemini models.</p><p>Google DeepMind engineer and former CEO and founder of Windsurf, Varun Mohan, <a href=\"https://x.com/_mohansolo/status/2025766889205739899?s=20\">said in an X post</a> that the company noticed “malicious usage” that led to service degradation.</p><p>“We’ve been seeing a massive increase in malicious usage of the Antigravity backend that has tremendously degraded the quality of service for our users. We needed to find a path to quickly shut off access to these users that are not using the product as intended. We understand that a subset of these users were not aware that this was against our ToS [Terms of Service] and will get a path for them to come back on but we have limited capacity and want to be fair to our actual users,” the post said. </p><div></div><p>A Google DeepMind spokesperson told VentureBeat that the move is not to permanently ban the use of Antigravity to access third-party platforms, but to align its use with the platform’s terms of service. </p><p>Unsurprisingly, Google’s move has caused a furor among OpenClaw users, including from OpenClaw creator Peter Steinberger, who announced that OpenClaw will remove Google support as a result. </p><div></div><h2><b>Infrastructure and connection uncertainty</b></h2><p>OpenClaw <a href=\"https://venturebeat.com/technology/what-the-openclaw-moment-means-for-enterprises-5-big-takeaways\">emerged as a way for individual users</a> to run shell commands and access local files, <a href=\"https://venturebeat.com/technology/openais-acquisition-of-openclaw-signals-the-beginning-of-the-end-of-the\">fulfilling a major promise</a> of AI agents: efficiently running workflows for users. </p><p>But, as VentureBeat has <a href=\"https://venturebeat.com/security/how-to-test-openclaw-without-giving-an-autonomous-agent-shell-access-to-your\">frequently pointed out</a>, it can often run into security and guardrail issues. There are companies building ways for <a href=\"https://venturebeat.com/orchestration/runlayer-is-now-offering-secure-openclaw-agentic-capabilities-for-large\">enterprise customers to access</a> OpenClaw securely and with a governance layer, though OpenClaw is so new that we should expect more announcements soon.</p><p>However, Google’s move was not framed as a security issue but rather as one of access and runtime, further showing that there is still significant uncertainty when users want to bring in something like OpenClaw into their workflow. </p><p>This is not the first time developers and power users of agentic AI found their access curtailed. Last year, Anthropic <a href=\"https://venturebeat.com/ai/anthropic-throttles-claude-rate-limits-devs-call-foul\">throttled access to Claude Code</a> after the company claimed some users were abusing the system by running it 24/7. </p><p>What this does highlight is the disconnect between companies like Google and OpenClaw users. OpenClaw offered many interesting possibilities for creating workflows with agents. However, because it is continually evolving, users may inadvertently run afoul of ToS or rate limits. </p><p>Mohan said Google is working to bring the banned users back, but whether this means the company will amend its ToS or figure out a secure connection between OpenClaw agents and Antigravity models remains to be seen. </p><p>For developers, the message is clear: the era of &quot;bring your own agent&quot; to a frontier model is ending. Providers are now prioritizing vertically integrated experiences where they can capture 100% of the telemetry and subscription revenue, often at the expense of the open-source interoperability that defined the early days of the LLM boom.</p><h2><b>Affected users</b></h2><p>Several users said on both the <a href=\"https://news.ycombinator.com/item?id=47115805\">Y Combinator chat boards</a> and X that they no longer had access to their Google accounts after running OpenClaw instances for certain Google products. </p><p>Google’s move mirrors a broader industry shift toward &quot;walled garden&quot; agent ecosystems. Earlier this year, Anthropic introduced &quot;client fingerprinting&quot; to ensure that its Claude Code environment remains the exclusive interface for its models, effectively locking out third-party wrappers like OpenClaw. For developers, the message is clear: the era of &quot;bring your own agent&quot; to a frontier model is ending. Providers are now prioritizing vertically integrated experiences where they can capture 100% of the telemetry and subscription revenue, often at the expense of the open-source interoperability that defined the early days of the LLM boom.</p><p>Some have said they will no longer use Google or Gemini for their projects. Right now, people who still want to keep using Antigravity will need to wait until Google figures out a way for them to use OpenClaw and access Gemini tokens in a manner Google deems “fair.” </p><p>Google DeepMind reiterated that it had only cut access to Antigravity, not to other Google applications. </p><div></div><div></div><div></div><div></div><h2><b>Conclusion: the enterprise takeaway</b></h2><p>For enterprise technical decision-makers, the &quot;Antigravity Ban&quot; serves as a definitive case study in the risks of agentic dependency. As the industry moves from chatbots to autonomous agents, the following realities must now dictate strategy:</p><ul><li><p><b>Platform fragility is the new normal:</b> The sudden lockout of $250/month &quot;Ultra&quot; users proves that even high-paying enterprise customers have little leverage when a provider decides to change its &quot;fair use&quot; definitions. Relying on OAuth-based third-party wrappers for core business logic is now a high-risk gamble.</p></li><li><p><b>The rise of local-first governance: </b>With OpenClaw moving toward an OpenAI-backed foundation and Google/Anthropic tightening their clouds, enterprises should prioritize agent frameworks that can run &quot;local-first&quot; or within VPCs. The &quot;token loophole&quot; that OpenClaw exploited is being closed; future agentic scale will require direct, high-cost API contracts rather than subsidized consumer seats.</p></li><li><p><b>Account portability as a requirement:</b> The fact that users &quot;lost access to their Google accounts&quot; underscores the danger of bundling development environments with primary identity providers. Decision-makers should decouple AI development from core corporate identity (SSO) where possible to avoid a single ToS violation paralyzing an entire team&#x27;s communications.</p></li></ul><p>Ultimately, the Antigravity incident marks the end of the &quot;Wild West&quot; for AI agents. As Google and OpenAI stake their claims, the enterprise must choose between the stability of the walled garden or the complexity (and cost) of truly independent, self-hosted infrastructure.</p>",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "VentureBeat",
      "title": "One engineer made a production SaaS product in an hour: here's the governance system that made it possible",
      "url": "https://venturebeat.com/orchestration/one-engineer-made-a-production-saas-product-in-an-hour-heres-the-governance",
      "published": "2026-02-23T22:50:00+00:00",
      "summary": "<p>Every engineering leader watching the agentic coding wave is eventually going to face the same question: if AI can generate production-quality code faster than any team, what does governance look like when the human isn&#x27;t writing the code anymore?</p><p>Most teams don&#x27;t have a good answer yet. <a href=\"https://www.treasuredata.com/press-releases/treasure-code\">Treasure Data</a>, a SoftBank-backed customer data platform serving more than 450 global brands, now has one, though they learned parts of it the hard way.</p><p>The company today <a href=\"https://www.treasuredata.com/press-releases/treasure-code\">officially announced Treasure Code</a>, a new AI-native command-line interface that lets data engineers and platform teams operate its full CDP through natural language, with Claude Code handling creation and iteration underneath. It was built by a single engineer. </p><p>The company says the coding itself took roughly 60 minutes. But that number is almost beside the point. The more important story is what had to be true before those 60 minutes were possible, and what broke after.</p><p>&quot;From a planning standpoint, we still have to plan to derisk the business, and that did take a couple of weeks,&quot; Rafa Flores, Chief Product Officer at Treasure Data, told VentureBeat. &quot;From an ideation and execution standpoint, that&#x27;s where you kind of just blend the two and you just go, go, go. And it&#x27;s not just prototyping, it&#x27;s rolling things out in production in a safe way.&quot;</p><h2><b>Build the governance layer first</b></h2><p>Before even a single line of code was written, Treasure Data had to answer a harder question: what does the system need to be prohibited from doing, and how do you enforce that at the platform level rather than hoping the code respects it?</p><p>The guardrails Treasure Data built live upstream of the code itself. When any user connects to the CDP through Treasure Code, access control and permission management are inherited directly from the platform. Users can only reach resources they already have permission for. PII cannot be exposed. API keys cannot be surfaced. The system cannot speak disparagingly about a brand or competitor.</p><p>&quot;We had to get CISOs involved. I was involved. Our CTO, heads of engineering, just to make sure that this thing didn&#x27;t just go rogue,&quot; Flores said.</p><p>This foundation made the next step possible: letting AI generate 100% of the codebase, with a three-tier quality pipeline enforcing production standards throughout.</p><h2><b>The three-tier pipeline for AI code generation </b></h2><p>The first tier is an AI-based code reviewer also using Claude Code. The code reviewer sits at the pull request stage and runs a structured review checklist against every proposed merge, checking for architectural alignment, security compliance, proper error handling, test coverage and documentation quality. When all criteria are satisfied it can merge automatically. When they aren&#x27;t, it flags for human intervention.</p><p>The fact that Treasure Data built the code reviewer in Claude Code is not incidental. It means the tool validating AI-generated code was itself AI-generated, a proof point that the workflow is self-reinforcing rather than dependent on a separate human-written quality layer.</p><p>The second tier is a standard CI/CD pipeline running automated unit, integration and end-to-end tests, static analysis, linting and security checks against every change. The third is human review, required wherever automated systems flag risk or enterprise policy demands sign-off.</p><p>The internal principle Treasure Data operates under: AI writes code, but AI does not ship code.</p><h2><b>Why this isn&#x27;t just Cursor pointed at a database</b></h2><p>The obvious question for any engineering team is why not just point an existing tool like Cursor at your data platform, or expose it as an MCP server and let Claude Code query it directly.</p><p",
      "content_text": "<p>Every engineering leader watching the agentic coding wave is eventually going to face the same question: if AI can generate production-quality code faster than any team, what does governance look like when the human isn&#x27;t writing the code anymore?</p><p>Most teams don&#x27;t have a good answer yet. <a href=\"https://www.treasuredata.com/press-releases/treasure-code\">Treasure Data</a>, a SoftBank-backed customer data platform serving more than 450 global brands, now has one, though they learned parts of it the hard way.</p><p>The company today <a href=\"https://www.treasuredata.com/press-releases/treasure-code\">officially announced Treasure Code</a>, a new AI-native command-line interface that lets data engineers and platform teams operate its full CDP through natural language, with Claude Code handling creation and iteration underneath. It was built by a single engineer. </p><p>The company says the coding itself took roughly 60 minutes. But that number is almost beside the point. The more important story is what had to be true before those 60 minutes were possible, and what broke after.</p><p>&quot;From a planning standpoint, we still have to plan to derisk the business, and that did take a couple of weeks,&quot; Rafa Flores, Chief Product Officer at Treasure Data, told VentureBeat. &quot;From an ideation and execution standpoint, that&#x27;s where you kind of just blend the two and you just go, go, go. And it&#x27;s not just prototyping, it&#x27;s rolling things out in production in a safe way.&quot;</p><h2><b>Build the governance layer first</b></h2><p>Before even a single line of code was written, Treasure Data had to answer a harder question: what does the system need to be prohibited from doing, and how do you enforce that at the platform level rather than hoping the code respects it?</p><p>The guardrails Treasure Data built live upstream of the code itself. When any user connects to the CDP through Treasure Code, access control and permission management are inherited directly from the platform. Users can only reach resources they already have permission for. PII cannot be exposed. API keys cannot be surfaced. The system cannot speak disparagingly about a brand or competitor.</p><p>&quot;We had to get CISOs involved. I was involved. Our CTO, heads of engineering, just to make sure that this thing didn&#x27;t just go rogue,&quot; Flores said.</p><p>This foundation made the next step possible: letting AI generate 100% of the codebase, with a three-tier quality pipeline enforcing production standards throughout.</p><h2><b>The three-tier pipeline for AI code generation </b></h2><p>The first tier is an AI-based code reviewer also using Claude Code. The code reviewer sits at the pull request stage and runs a structured review checklist against every proposed merge, checking for architectural alignment, security compliance, proper error handling, test coverage and documentation quality. When all criteria are satisfied it can merge automatically. When they aren&#x27;t, it flags for human intervention.</p><p>The fact that Treasure Data built the code reviewer in Claude Code is not incidental. It means the tool validating AI-generated code was itself AI-generated, a proof point that the workflow is self-reinforcing rather than dependent on a separate human-written quality layer.</p><p>The second tier is a standard CI/CD pipeline running automated unit, integration and end-to-end tests, static analysis, linting and security checks against every change. The third is human review, required wherever automated systems flag risk or enterprise policy demands sign-off.</p><p>The internal principle Treasure Data operates under: AI writes code, but AI does not ship code.</p><h2><b>Why this isn&#x27;t just Cursor pointed at a database</b></h2><p>The obvious question for any engineering team is why not just point an existing tool like Cursor at your data platform, or expose it as an MCP server and let Claude Code query it directly.</p><p>Flores argued the difference is governance depth. A generic connection gives you natural language access to data but inherits none of the platform&#x27;s existing permission structures, meaning every query runs with whatever access the API key allows. </p><p>Treasure Code inherits Treasure Data&#x27;s full access control and permissioning layer, so what a user can do through natural language is bounded by what they&#x27;re already authorized to do in the platform. </p><p>The second distinction is orchestration. Because Treasure Code connects directly to Treasure Data&#x27;s AI Agent Foundry, it can coordinate sub-agents and skills across the platform rather than executing single tasks in isolation: the difference between telling an AI to run an analysis and having it orchestrate that analysis across omni-channel activation, segmentation and reporting simultaneously.</p><h2><b>What broke anyway</b></h2><p>Even with the governance architecture in place, the launch didn&#x27;t go cleanly, and Flores was candid about it.</p><p>Treasure Data initially made Treasure Code available to customers without a go-to-market plan. The assumption was that it would stay quiet while the team figured out next steps. Customers found it anyway. More than 100 customers and close to 1,000 users adopted it within two weeks, entirely through organic discovery.</p><p>&quot;We didn&#x27;t put any go-to-market motions behind it. We didn&#x27;t think people were going to find it. Well, they did,&quot; Flores said. &quot;We were left scrambling with, how do we actually do the go-to-market motions? Do we even do a beta, since technically it&#x27;s live?&quot;</p><p>The unplanned adoption also created a compliance gap. Treasure Data is still in the process of formally certifying Treasure Code under its Trust AI compliance program, a certification it had not completed before the product reached customers.</p><p>A second problem emerged when Treasure Data opened skill development to non-engineering teams. CSMs and account directors began building and submitting skills without understanding what would get approved and merged, creating significant wasted effort and a backlog of submissions that couldn&#x27;t clear the repository&#x27;s access policies.</p><h2><b>Enterprise validation and what&#x27;s still missing</b></h2><p>Thomson Reuters is among the early adopters. Flores said that the company had been attempting to build an in-house AI agent platform and struggling to move fast enough. It connected with Treasure Data&#x27;s AI Agent Foundry to accelerate audience segmentation work, then extended into Treasure Code to customize and iterate more rapidly.</p><p>The feedback, Flores said, has centered on extensibility and flexibility, and the fact that procurement was already done, removing a significant enterprise barrier to adoption.</p><p>The gap Thomson Reuters has flagged, and that Flores acknowledges the product doesn&#x27;t yet address, is guidance on AI maturity. Treasure Code doesn&#x27;t tell users who should use it, what to tackle first, or how to structure access across different skill levels within an organization.</p><p>&quot;AI that allows you to be leveraged, but also tells you how to leverage it, I think that&#x27;s very differentiated,&quot; Flores said. He sees it as the next meaningful layer to build.</p><h2><b>What engineering leaders should take from this</b></h2><p>Flores has had time to reflect on what the experience actually taught him, and he was direct about what he&#x27;d change. Next time, he said, the release would stay internal first.</p><p>&quot;We will release it internally only. I will not release it to anyone outside of the organization,&quot; he said. &quot;It will be more of a controlled release so we can actually learn what we&#x27;re actually being exposed to at lower risk.&quot;</p><p>On skill development, the lesson was to establish clear criteria for what gets approved and merged before opening the process to teams outside engineering, not after.</p><p>The common thread in both lessons is the same one that shaped the governance architecture and the three-tier pipeline: speed is only an advantage if the structure around it holds. For engineering leaders evaluating whether agentic coding is ready for production, the Treasure Data experience translates into three practical conclusions.</p><ol><li><p><b>Governance infrastructure has to precede the code, not follow it.</b> The platform-level access controls and permission inheritance were what made it safe to let AI generate freely. Without that foundation, the speed advantage disappears because every output requires exhaustive manual review.</p></li><li><p><b>A quality gate that doesn&#x27;t depend entirely on humans is not optional at scale</b>. Build a quality gate that doesn&#x27;t depend entirely on humans. AI can review every pull request consistently, without fatigue, and check policy compliance systematically across the entire codebase. Human review remains essential, but as a final check rather than the primary quality mechanism.</p></li><li><p><b>Plan for organic adoption. </b>If the product works, people will find it before you&#x27;re ready. The compliance and go-to-market gaps Treasure Data is still closing are a direct result of underestimating that.</p></li></ol><p>&quot;Yes, vibe coding can work if done in a safe way and proper guardrails are in place,&quot; Flores said. &quot;Embrace it in a way to find means of not replacing the good work you do, but the tedious work that you can probably automate.&quot;</p>",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "Wired",
      "title": "6 Best Duffel Bags We Tested While Traveling (2026)",
      "url": "https://www.wired.com/gallery/best-duffel-bags/",
      "published": "2026-02-23T22:36:15+00:00",
      "summary": "Need to schlep some stuff? Consider these field-tested duffel bags. The Eastpak Duffel Pack S Tarp Black2 is our top pick.",
      "content_text": "This is not a true duffel bag so much as â€œthe worldâ€™s first true wide-mouth packing system,â€� as Rux calls it, but it is nevertheless an impressive piece of equipment from a company known for its modular gear-toting systems. Not unlike a foldable version of the popular 70L storage container , the Duffel box starts completely flat, but the sides pop up, and the patent-pending top rolls down to form a box that stays open on its own. There are no zippers involved in its construction, but there are multiple straps, panels, and pockets, and you will most likely need to watch an instructional YouTube video to make full use of all the features. However, the beauty of this bag is that it can be just about anything you want it to be. Long-term storage, luggage, a gear boxâ€”even a backpack. All is possible with the included straps and dividers in the right places. Over the past four months, my family has used it as a traditional duffel bag, a storage box, and, currently, a portable equipment organizer for my sonâ€™s club soccer team. Itâ€™s been stepped on, rained on, and thrown in wagons and vehicle trunks, with nary a scratch on the 105D nylon gridstop fabric. (Though it did get stuck in a downpour once, and I will say Iâ€™m not sure Iâ€™d quantify the fabric as fully waterproofâ€”closer to water resistant.) Lash points along the inside walls allow it to integrate with Ruxâ€™s line of accessories and packing bags (sold separately), in which weâ€™re currently keeping pinnies and goalkeeper gear. The Duffel Box will be officially for sale on March 16 in two sizes, 55L and 75L; pictured is the 55L. Note that a â€œPlusâ€� version will include a removable universal shoulder strap, which connects to lash points on the outside, for an extra $25. â€” Kat Merck Capacity 55L, 75L Color Options 2 Dimensions 14.2\" x 18.1\" x 12.6\" Materials Nylon gridstop with waterproof coating and PFAS-free DWR. 3-mm EVA foam. Additional Features Zipper-free. Water-resistant. Compatible with various accessories and packing bags. Warranty Lifetime",
      "cover_image_url": "https://media.wired.com/photos/699a853f32429c68a5920bbd/191:100/w_1280,c_limit/The%20Best%20Duffel%20Bags%20for%20Commuters%20and%20Overpackers.png"
    },
    {
      "industry": "technology",
      "source": "VentureBeat",
      "title": "Anthropic says DeepSeek, Moonshot, and MiniMax used 24,000 fake accounts to rip off Claude",
      "url": "https://venturebeat.com/technology/anthropic-says-deepseek-moonshot-and-minimax-used-24-000-fake-accounts-to",
      "published": "2026-02-23T22:20:00+00:00",
      "summary": "<p><a href=\"https://www.anthropic.com/\">Anthropic</a> dropped a bombshell on the artificial intelligence industry Monday, publicly accusing three prominent Chinese AI laboratories — <a href=\"https://www.deepseek.com/\">DeepSeek</a>, <a href=\"https://www.moonshot.ai/\">Moonshot AI</a>, and <a href=\"https://www.minimax.io/\">MiniMax</a> — of orchestrating coordinated, industrial-scale campaigns to siphon capabilities from its Claude models using tens of thousands of fraudulent accounts.</p><p>The San Francisco-based company said the three labs collectively generated <a href=\"https://www.anthropic.com/news/detecting-and-preventing-distillation-attacks\">more than 16 million exchanges</a> with Claude through approximately 24,000 fake accounts, all in violation of Anthropic&#x27;s terms of service and regional access restrictions. The campaigns, Anthropic said, are the most concrete and detailed public evidence to date of a practice that has haunted Silicon Valley for months: foreign competitors systematically using a technique called distillation to leapfrog years of research and billions of dollars in investment.</p><p>&quot;These campaigns are growing in intensity and sophistication,&quot; Anthropic wrote in a <a href=\"https://www.anthropic.com/news/detecting-and-preventing-distillation-attacks\">technical blog post</a> published Monday. &quot;The window to act is narrow, and the threat extends beyond any single company or region. Addressing it will require rapid, coordinated action among industry players, policymakers, and the global AI community.&quot;</p><p>The disclosure marks a dramatic escalation in the simmering tensions between American and Chinese AI developers — and it arrives at a moment when <a href=\"https://www.bis.gov/news-updates\">Washington is actively debating</a> whether to tighten or loosen export controls on the advanced chips that power AI training. Anthropic, led by CEO Dario Amodei, has been among the most vocal advocates for <a href=\"https://www.bloomberg.com/news/articles/2026-01-20/anthropic-ceo-says-selling-advanced-ai-chips-to-china-is-crazy\">restricting chip sales to China</a>, and the company explicitly connected Monday&#x27;s revelations to that policy fight.</p><h2><b>How AI distillation went from obscure research technique to geopolitical flashpoint</b></h2><p>To understand what Anthropic alleges, it helps to understand what <a href=\"https://www.ibm.com/think/topics/knowledge-distillation\">distillation</a> actually is — and how it evolved from an academic curiosity into the most contentious issue in the global AI race.</p><p>At its core, <a href=\"https://www.ibm.com/think/topics/knowledge-distillation\">distillation</a> is a process of extracting knowledge from a larger, more powerful AI model — the &quot;teacher&quot; — to create a smaller, more efficient one — the &quot;student.&quot; The student model learns not from raw data, but from the teacher&#x27;s outputs: its answers, reasoning patterns, and behaviors. Done correctly, the student can achieve performance remarkably close to the teacher&#x27;s while requiring a fraction of the compute to train.</p><p>As Anthropic itself acknowledged, distillation is &quot;a widely used and legitimate training method.&quot; Frontier AI labs, including Anthropic, routinely distill their own models to create smaller, cheaper versions for customers. But the same technique can be weaponized. A competitor can pose as a legitimate customer, bombard a frontier model with carefully crafted prompts, collect the outputs, and use those outputs to train a rival system — capturing capabilities that took years and hundreds of millions of dollars to develop.</p><p>The technique burst into public consciousness in January 2025 when <a href=\"https://www.reuters.com/world/china/chinas-deepseek-releases-an-update-its-r1-reasoning-model-2025-05-29/\">DeepSeek released its R1 reasoning model</a>, which appeared to match or approach the performance of leading American models at dramat",
      "content_text": "<p><a href=\"https://www.anthropic.com/\">Anthropic</a> dropped a bombshell on the artificial intelligence industry Monday, publicly accusing three prominent Chinese AI laboratories — <a href=\"https://www.deepseek.com/\">DeepSeek</a>, <a href=\"https://www.moonshot.ai/\">Moonshot AI</a>, and <a href=\"https://www.minimax.io/\">MiniMax</a> — of orchestrating coordinated, industrial-scale campaigns to siphon capabilities from its Claude models using tens of thousands of fraudulent accounts.</p><p>The San Francisco-based company said the three labs collectively generated <a href=\"https://www.anthropic.com/news/detecting-and-preventing-distillation-attacks\">more than 16 million exchanges</a> with Claude through approximately 24,000 fake accounts, all in violation of Anthropic&#x27;s terms of service and regional access restrictions. The campaigns, Anthropic said, are the most concrete and detailed public evidence to date of a practice that has haunted Silicon Valley for months: foreign competitors systematically using a technique called distillation to leapfrog years of research and billions of dollars in investment.</p><p>&quot;These campaigns are growing in intensity and sophistication,&quot; Anthropic wrote in a <a href=\"https://www.anthropic.com/news/detecting-and-preventing-distillation-attacks\">technical blog post</a> published Monday. &quot;The window to act is narrow, and the threat extends beyond any single company or region. Addressing it will require rapid, coordinated action among industry players, policymakers, and the global AI community.&quot;</p><p>The disclosure marks a dramatic escalation in the simmering tensions between American and Chinese AI developers — and it arrives at a moment when <a href=\"https://www.bis.gov/news-updates\">Washington is actively debating</a> whether to tighten or loosen export controls on the advanced chips that power AI training. Anthropic, led by CEO Dario Amodei, has been among the most vocal advocates for <a href=\"https://www.bloomberg.com/news/articles/2026-01-20/anthropic-ceo-says-selling-advanced-ai-chips-to-china-is-crazy\">restricting chip sales to China</a>, and the company explicitly connected Monday&#x27;s revelations to that policy fight.</p><h2><b>How AI distillation went from obscure research technique to geopolitical flashpoint</b></h2><p>To understand what Anthropic alleges, it helps to understand what <a href=\"https://www.ibm.com/think/topics/knowledge-distillation\">distillation</a> actually is — and how it evolved from an academic curiosity into the most contentious issue in the global AI race.</p><p>At its core, <a href=\"https://www.ibm.com/think/topics/knowledge-distillation\">distillation</a> is a process of extracting knowledge from a larger, more powerful AI model — the &quot;teacher&quot; — to create a smaller, more efficient one — the &quot;student.&quot; The student model learns not from raw data, but from the teacher&#x27;s outputs: its answers, reasoning patterns, and behaviors. Done correctly, the student can achieve performance remarkably close to the teacher&#x27;s while requiring a fraction of the compute to train.</p><p>As Anthropic itself acknowledged, distillation is &quot;a widely used and legitimate training method.&quot; Frontier AI labs, including Anthropic, routinely distill their own models to create smaller, cheaper versions for customers. But the same technique can be weaponized. A competitor can pose as a legitimate customer, bombard a frontier model with carefully crafted prompts, collect the outputs, and use those outputs to train a rival system — capturing capabilities that took years and hundreds of millions of dollars to develop.</p><p>The technique burst into public consciousness in January 2025 when <a href=\"https://www.reuters.com/world/china/chinas-deepseek-releases-an-update-its-r1-reasoning-model-2025-05-29/\">DeepSeek released its R1 reasoning model</a>, which appeared to match or approach the performance of leading American models at dramatically lower cost. <a href=\"https://www.cnbc.com/2025/02/21/deepseek-trained-ai-model-using-distillation-now-a-disruptive-force.html\">Databricks CEO Ali Ghodsi</a> captured the industry&#x27;s anxiety at the time, telling CNBC: &quot;This distillation technique is just so extremely powerful and so extremely cheap, and it&#x27;s just available to anyone.&quot; He predicted the technique would usher in an era of intense competition for large language models.</p><p>That prediction proved prescient. In the weeks following DeepSeek&#x27;s release, researchers at UC Berkeley said they <a href=\"https://techcrunch.com/2025/01/11/researchers-open-source-sky-t1-a-reasoning-ai-model-that-can-be-trained-for-less-than-450/\">recreated OpenAI&#x27;s reasoning model for just $450 in 19 hours</a>. Researchers at Stanford and the University of Washington followed <a href=\"https://www.theverge.com/news/607341/researchers-cheaper-openai-rival-training\">with their own version</a> built in 26 minutes for under $50 in compute credits. The startup <a href=\"https://arstechnica.com/ai/2025/02/after-24-hour-hackathon-hugging-faces-ai-research-agent-nearly-matches-openais-solution/\">Hugging Face replicated OpenAI&#x27;s Deep Research feature</a> as a 24-hour coding challenge. DeepSeek itself openly released a family of <a href=\"https://huggingface.co/deepseek-ai\">distilled models on Hugging Face</a> — including versions built on top of Qwen and Llama architectures — under the permissive MIT license, with the model card explicitly stating that the DeepSeek-R1 series supports commercial use and allows for any modifications and derivative works, &quot;including, but not limited to, distillation for training other LLMs.&quot;</p><p>But what Anthropic described Monday goes far beyond academic replication or open-source experimentation. The company detailed what it characterized as deliberate, covert, and large-scale intellectual property extraction by well-resourced commercial laboratories operating under the jurisdiction of the Chinese government.</p><h2><b>Anthropic traces 16 million fraudulent exchanges to researchers at DeepSeek, Moonshot, and MiniMax</b></h2><p>Anthropic attributed each campaign &quot;<a href=\"https://www.anthropic.com/news/detecting-and-preventing-distillation-attacks\">with high confidence</a>&quot; through IP address correlation, request metadata, infrastructure indicators, and corroboration from unnamed industry partners who observed the same actors on their own platforms. Each campaign specifically targeted what Anthropic described as Claude&#x27;s most differentiated capabilities: agentic reasoning, tool use, and coding.</p><p><a href=\"https://www.deepseek.com/\">DeepSeek</a>, the company that ignited the distillation debate, conducted what Anthropic described as the most technically sophisticated of the three operations, generating over 150,000 exchanges with Claude. Anthropic said DeepSeek&#x27;s prompts targeted reasoning capabilities, rubric-based grading tasks designed to make Claude function as a reward model for reinforcement learning, and — in a detail likely to draw particular political attention — the creation of &quot;censorship-safe alternatives to policy sensitive queries.&quot;</p><p>Anthropic alleged that DeepSeek &quot;<a href=\"https://www.anthropic.com/news/detecting-and-preventing-distillation-attacks\">generated synchronized traffic across accounts</a>&quot; with &quot;identical patterns, shared payment methods, and coordinated timing&quot; that suggested load balancing to maximize throughput while evading detection. In one particularly notable technique, Anthropic said DeepSeek&#x27;s prompts &quot;asked Claude to imagine and articulate the internal reasoning behind a completed response and write it out step by step — effectively generating chain-of-thought training data at scale.&quot; The company also alleged it observed tasks in which Claude was used to generate alternatives to politically sensitive queries about &quot;dissidents, party leaders, or authoritarianism,&quot; likely to train DeepSeek&#x27;s own models to steer conversations away from censored topics. Anthropic said it was able to trace these accounts to specific researchers at the lab.</p><p><a href=\"https://www.moonshot.ai/\">Moonshot AI</a>, the Beijing-based creator of the Kimi models, ran the second-largest operation by volume at over 3.4 million exchanges. Anthropic said Moonshot targeted agentic reasoning and tool use, coding and data analysis, computer-use agent development, and computer vision. The company employed &quot;hundreds of fraudulent accounts spanning multiple access pathways,&quot; making the campaign harder to detect as a coordinated operation. Anthropic attributed the campaign through request metadata that &quot;matched the public profiles of senior Moonshot staff.&quot; In a later phase, Anthropic said, Moonshot adopted a more targeted approach, &quot;attempting to extract and reconstruct Claude&#x27;s reasoning traces.&quot;</p><p><a href=\"https://www.minimax.io/\">MiniMax</a>, the least publicly known of the three but the most prolific by volume, generated over 13 million exchanges — more than three-quarters of the total. Anthropic said MiniMax&#x27;s campaign focused on agentic coding, tool use, and orchestration. The company said it detected MiniMax&#x27;s campaign while it was still active, &quot;before MiniMax released the model it was training,&quot; giving Anthropic &quot;unprecedented visibility into the life cycle of distillation attacks, from data generation through to model launch.&quot; In a detail that underscores the urgency and opportunism Anthropic alleges, the company said that when it released a new model during MiniMax&#x27;s active campaign, MiniMax &quot;pivoted within 24 hours, redirecting nearly half their traffic to capture capabilities from our latest system.&quot;</p><h2><b>How proxy networks and &#x27;hydra cluster&#x27; architectures helped Chinese labs bypass Anthropic&#x27;s China ban</b></h2><p>Anthropic does not currently offer commercial access to Claude in China, a policy it maintains for national security reasons. So how did these labs access the models at all?</p><p>The answer, Anthropic said, lies in commercial proxy services that resell access to Claude and other frontier AI models at scale. Anthropic described these services as running what it calls &quot;<a href=\"https://www.anthropic.com/news/detecting-and-preventing-distillation-attacks\">hydra cluster</a>&quot; architectures — sprawling networks of fraudulent accounts that distribute traffic across Anthropic&#x27;s API and third-party cloud platforms. &quot;The breadth of these networks means that there are no single points of failure,&quot; Anthropic wrote. &quot;When one account is banned, a new one takes its place.&quot; In one case, Anthropic said, a single proxy network managed more than 20,000 fraudulent accounts simultaneously, mixing distillation traffic with unrelated customer requests to make detection harder.</p><p>The description suggests a mature and well-resourced infrastructure ecosystem dedicated to circumventing access controls — one that may serve many more clients than just the three labs Anthropic named.</p><h2><b>Why Anthropic framed distillation as a national security crisis, not just an IP dispute</b></h2><p>Anthropic did not treat this as a mere terms-of-service violation. The company embedded its technical disclosure within an explicit national security argument, warning that &quot;illicitly distilled models lack necessary safeguards, creating significant national security risks.&quot;</p><p>The company argued that models built through illicit distillation are &quot;unlikely to retain&quot; the safety guardrails that American companies build into their systems — protections designed to prevent AI from being used to develop bioweapons, carry out cyberattacks, or enable mass surveillance. &quot;Foreign labs that distill American models can then feed these unprotected capabilities into military, intelligence, and surveillance systems,&quot; Anthropic wrote, &quot;enabling authoritarian governments to deploy frontier AI for offensive cyber operations, disinformation campaigns, and mass surveillance.&quot;</p><p>This framing directly connects to the chip export control debate that Amodei has made a centerpiece of his public advocacy. In a detailed <a href=\"https://darioamodei.com/post/on-deepseek-and-export-controls\">essay published in January 2025</a>, Amodei argued that export controls are &quot;the most important determinant of whether we end up in a unipolar or bipolar world&quot; — a world where either only the U.S. and its allies possess the most powerful AI, or one where China achieves parity. He specifically noted at the time that he was &quot;not taking any position on reports of distillation from Western models&quot; and would &quot;just take DeepSeek at their word that they trained it the way they said in the paper.&quot;</p><p>Monday&#x27;s disclosure is a sharp departure from that earlier restraint. Anthropic now argues that distillation attacks &quot;<a href=\"https://www.anthropic.com/news/detecting-and-preventing-distillation-attacks\">undermine</a>&quot; export controls &quot;by allowing foreign labs, including those subject to the control of the Chinese Communist Party, to close the competitive advantage that export controls are designed to preserve through other means.&quot; The company went further, asserting that &quot;without visibility into these attacks, the apparently rapid advancements made by these labs are incorrectly taken as evidence that export controls are ineffective.&quot; In other words, Anthropic is arguing that what some observers interpreted as proof that Chinese labs can innovate around chip restrictions was actually, in significant part, the result of stealing American capabilities.</p><h2><b>The murky legal landscape around AI distillation may explain Anthropic&#x27;s political strategy</b></h2><p>Anthropic&#x27;s decision to frame this as a national security issue rather than a legal dispute may reflect the difficult reality that intellectual property law offers limited recourse against distillation.</p><p>As a March 2025 analysis by the law firm Winston &amp; Strawn noted, &quot;<a href=\"https://www.winston.com/en/insights-news/is-ai-distillation-by-deepseek-ip-theft\">the legal landscape surrounding AI distillation is unclear and evolving</a>.&quot; The firm&#x27;s attorneys observed that proving a copyright claim in this context would be challenging, since it remains unclear whether the outputs of AI models qualify as copyrightable creative expression. The U.S. Copyright Office affirmed in January 2025 that copyright protection requires human authorship, and that &quot;mere provision of prompts does not render the outputs copyrightable.&quot;</p><p>The legal picture is further complicated by the way frontier labs structure output ownership. OpenAI&#x27;s terms of use, for instance, assign ownership of model outputs to the user — meaning that even if a company can prove extraction occurred, it may not hold copyrights over the extracted data. Winston &amp; Strawn noted that this dynamic means &quot;even if OpenAI can present enough evidence to show that DeepSeek extracted data from its models, OpenAI likely does not have copyrights over the data.&quot; The same logic would almost certainly apply to Anthropic&#x27;s outputs.</p><p>Contract law may offer a more promising avenue. Anthropic&#x27;s terms of service prohibit the kind of systematic extraction the company describes, and violation of those terms is a more straightforward legal claim than copyright infringement. But enforcing contractual terms against entities operating through proxy services and fraudulent accounts in a foreign jurisdiction presents its own formidable challenges.</p><p>This may explain why Anthropic chose the national security frame over a purely ",
      "cover_image_url": null
    },
    {
      "industry": "technology",
      "source": "Ars Technica",
      "title": "Pentagon buyer: We're happy with our launch industry, but payloads are lagging",
      "url": "https://arstechnica.com/space/2026/02/pentagon-buyer-were-happy-with-our-launch-industry-but-payloads-are-lagging/",
      "published": "2026-02-23T22:14:28+00:00",
      "summary": "\"The point is to get missions out the door as fast as possible. Two to three years is too slow.\"",
      "content_text": "DALLAS—The Space Force officer tasked with overseeing more than $24 billion in research and development spending says the Pentagon is more interested in supporting startups building new space sensors and payloads than adding yet another rocket company to its portfolio. The statement, made at a space finance conference in Dallas last week, was one of several points Maj. Gen. Stephen Purdy wanted to get across to a room full of investors and commercial space executives. The other points on Purdy’s agenda were that the Space Force is more interested in high-volume production than spending money to develop the latest technologies, and that the military has, at least for now, lost one of its most important tools for supporting and diversifying the space industrial base. The rhetoric around prioritizing payloads over launchers aligns with the Space Force’s recent history of supporting small startups. Since 2020, SpaceWERX, the Space Force’s commercial innovation program, has awarded 23 funding agreements —called Strategic Funding Increases (STRATFIs)—to commercial space startups developing new sensors, software, satellite components, spacecraft buses, and orbital transfer vehicles. SpaceWERX awarded a single STRATFI agreement to a launch company— ABL Space Systems —and that firm has since exited the space launch market. “We’re on path for mass-produced launch,” said Purdy, the military deputy for space acquisition in the Department of the Air Force. “We have got our ranges situated so we can do mass-produced launch. We’ve got our data centers and our data structure for mass-production. We’ve got AI pieces that are mass-produced, satellite buses are nearly there, and our payloads are the last element. Payloads at mass-produced affordability, at scale, is the key element.” K2’s Gravitas satellite, set for launch next month, will test the company’s Hall-effect thruster, solar arrays, and other systems. Credit: K2 K2’s Gravitas satellite, set for launch next month, will test the company’s Hall-effect thruster, solar arrays, and other systems. Credit: K2 Putting the money in Payloads, Purdy told Ars after his talk, are “the last frontier” for scaling space missions. “The point is to get missions out the door as fast as possible. Two to three years is too slow. We’ve got to get down to one week. I’m not talking about super exquisite [payloads]. That’s not most of our missions. The commercial industry, your Kuipers [ Amazon LEO ], your Starlinks, have sort of got the comm piece down, but we’re still struggling in a lot of other stuff.”",
      "cover_image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/05/GettyImages-1348666509-1152x648-1771883779.jpg"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Making Wolfram Tech Available as a Foundation Tool for LLM Systems",
      "url": "https://writings.stephenwolfram.com/2026/02/making-wolfram-tech-available-as-a-foundation-tool-for-llm-systems/",
      "published": "2026-02-23T22:11:34+00:00",
      "summary": "<p>Article URL: <a href=\"https://writings.stephenwolfram.com/2026/02/making-wolfram-tech-available-as-a-foundation-tool-for-llm-systems/\">https://writings.stephenwolfram.com/2026/02/making-wolfram-tech-available-as-a-foundation-tool-for-llm-systems/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47129727\">https://news.ycombinator.com/item?id=47129727</a></p> <p>Points: 168</p> <p># Comments: 86</p>",
      "content_text": "LLMs don’t—and can’t—do everything. What they do is very impressive—and useful. It’s broad. And in many ways it’s human-like. But it’s not precise. And in the end it’s not about deep computation. So how can we supplement LLM foundation models? We need a foundation tool: a tool that’s broad and general and does what LLMs themselves don’t: provides deep computation and precise knowledge. And, conveniently enough, that’s exactly what I’ve been building for the past 40 years! My goal with Wolfram Language has always been to make everything we can about the world computable. To bring together in a coherent and unified way the algorithms, the methods and the data to do precise computation whenever it’s possible. It’s been a huge undertaking, but I think it’s fair to say it’s been a hugely successful one —that’s fueled countless discoveries and inventions ( including my own ) across a remarkable range of areas of science, technology and beyond. But now it’s not just humans who can take advantage of this technology; it’s AIs—and in particular LLMs—as well. LLM foundation models are powerful. But LLM foundation models with our foundation tool are even more so. And with the maturing of LLMs we’re finally now in a position to provide to LLMs access to Wolfram tech in a standard, general way. It is, I believe, an important moment of convergence. My concept over the decades has been to build very broad and general technology—which is now a perfect fit for the breadth of LLM foundation models. LLMs can call specific specialized tools, and that will be useful for plenty of specific specialized purposes. But what Wolfram Language uniquely represents is a general tool—with general access to the great power that precise computation and knowledge bring. But there’s actually also much more. I designed Wolfram Language from the beginning to be a powerful medium not only for doing computation but also for representing and thinking about things computationally . I’d always assumed I was doing this for humans. But it now turns out that AIs need the same things—and that Wolfram Language provides the perfect medium for AIs to “think” and “reason” computationally. There’s another point as well. In its effort to make as much as possible computable, Wolfram Language not only has an immense amount inside, but also provides a uniquely unified hub for connecting to other systems and services . And that’s part of why it’s now possible to make such an effective connection between LLM foundation models and the foundation tool that is the Wolfram Language. On January 9, 2023, just weeks after ChatGPT burst onto the scene, I posted a piece entitled “ Wolfram|Alpha as the Way to Bring Computational Knowledge Superpowers to ChatGPT ”. Two months later we released the first Wolfram plugin for ChatGPT (and in between I wrote what quickly became a rather popular little book entitled What Is ChatGPT Doing … and Why Does It Work? ). The plugin was a modest but good start. But at the time LLMs and the ecosystem around them weren’t really ready for the bigger story. Would LLMs even in the end need tools at all? Or—despite the fundamental issues that seemed at least to me scientifically rather clear right from the start—would LLMs somehow magically find a way to do deep computation themselves? Or to guarantee to get precise, reliable results? And even if LLMs were going to use tools, how would that process be engineered, and what would the deployment model for it be? Three years have now passed, and much has clarified. The core capabilities of LLMs have come into better focus (even though there’s a lot we still don’t know scientifically about them). And it’s become much clearer that—at least for the modalities LLMs currently address—most of the growth in their practical value is going to have to do with how they are harnessed and connected. And this understanding highlights more than ever the broad importance of providing LLMs with the foundation tool that our technology represents. And the good news is that there are now streamlined ways to do this—using protocols and methods that have emerged around LLMs, and using new technology that we’ve developed. The tighter the integration between foundation models and our foundation tool, the more powerful the combination will be. Ultimately it’ll be a story of aligning the pre-training and core engineering of LLMs with our foundation tool. But an approach that’s immediately and broadly applicable today—and for which we’re releasing several new products—is based on what we call computation-augmented generation, or CAG. The key idea of CAG is to inject in real time capabilities from our foundation tool into the stream of content that LLMs generate. In traditional retrieval-augmented generation, or RAG, one is injecting content that has been retrieved from existing documents. CAG is like an infinite extension of RAG, in which an infinite amount of content can be generated on the fly—using computation—to feed to an LLM. Internally, CAG is a somewhat complex piece of technology that has taken a long time for us to develop. But in its deployment it’s something that we’ve made easy to integrate into existing LLM-related systems and workflows. And today we’re launching it, so that going forward any LLM system—and LLM foundation model—can count on being able to access our Foundation Tool, and being able to supplement their capabilities with the superpower of precise, deep computation and knowledge. The Practicalities Today we’re launching three primary methods for accessing our Foundation Tool, all based on computation-augmented generation (CAG), and all leveraging our rather huge software engineering technology stack. Immediately call our Foundation Tool from within any MCP-compatible LLM-based system. Most consumer LLM-based systems now support MCP, making this extremely easy to set up. Our main MCP Service is a web API, but there’s also a version that can use a local Wolfram Engine . A one-stop-shop “universal agent” combining an LLM foundation model with our Foundation Tool. Set up as a drop-in replacement for traditional LLM APIs. Direct fine-grained access to Wolfram tech for LLM systems, supporting optimized, custom integration into LLM systems of any scale. (All Wolfram tech is available in both hosted and on-premise form.) Wolfram Foundation Tool Capabilities Listing » For further information on access and integration options, contact our Partnerships group »",
      "cover_image_url": "https://content.wolfram.com/sites/43/2026/02/FoundationTool-seo-v2a.png"
    },
    {
      "industry": "technology",
      "source": "Ars Technica",
      "title": "Data center builders thought farmers would willingly sell land, learn otherwise",
      "url": "https://arstechnica.com/tech-policy/2026/02/im-not-for-sale-farmers-refuse-to-take-millions-in-data-center-deals/",
      "published": "2026-02-23T21:48:44+00:00",
      "summary": "Even in a fragile farm economy, million-dollar offers can't sway dedicated farmers.",
      "content_text": "Notably, one resident in Huddleston’s county who received an offer, 75-year-old Timothy Grosser, even declined a proposal to “name your price” when a tech company sought to buy his 250-acre farm, The Guardian reported. “There is none,” Grosser said. The farm is where he “lives, hunts, and raises cattle” and where his grandson hunts a turkey every Christmas for the family feast. “The money’s not worth giving up your lifestyle,” Grosser said. Another farmer in Wisconsin, Anthony Barta, reportedly fretted about what would happen to his neighbors if he took a deal he was offered—showing the deep bonds of people whose farms have bordered each other for years. In his community, another farmer was offered between $70 million and $80 million for 6,000 acres. “Me and my family, we own the farm and run close to 1,000 animals,” Barta said. “What would that do if that’s next to it? Can they even be there? You know, that’s our livelihood—the farm. We’re just concerned what, if it would go through, what would happen to us and our neighbors and farms and our community? What would happen to that?” Some tech companies are apparently not taking “no” for an answer. At least one farmer who spent 51 years milking cows in Pennsylvania prior to the AI boom described tech companies as “relentless.” Eighty-six-year-old Mervin Raudabaugh, Jr., found a creative solution to end the pressure to sell two contiguous farms. He reportedly staved off developers by turning to “a farmland preservation program dedicating taxpayer dollars toward protecting agricultural resources.” By working with the program, Raudabaugh will only receive about one-eighth of what the developers were offering. But he said it’s worth it to know his land would be preserved for farming purposes and out of reach of persistent tech companies. “These people have hounded the living daylights out of me,” Raudabaugh said. Data center deals come amid fragile farm economy For people in rural communities, data center fights go beyond concerns about water and electricity consumption—although those are concerns, too. Communities are defending the character of the land, which they don’t want to see suddenly disrupted by extensive construction, data center noise pollution, or untold environmental impacts from massive operations.",
      "cover_image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-1233733221-1024x648.jpg"
    },
    {
      "industry": "technology",
      "source": "TechCrunch",
      "title": "With AI, investor loyalty is (almost) dead: At least a dozen OpenAI VCs now also back Anthropic",
      "url": "https://techcrunch.com/2026/02/23/with-ai-investor-loyalty-is-almost-dead-at-least-a-dozen-openai-vcs-now-also-back-anthropic/",
      "published": "2026-02-23T21:46:41+00:00",
      "summary": "While some dual investors are understandable, others were more shocking, and signal the disregard of a longstanding ethical conflict-of-interest rule.",
      "content_text": "With OpenAI on the verge of finalizing a new $100 billion round , and Anthropic just closing its own monster $30 billion raise , one thing is clear: The concept of investor “loyalty” is only hanging on by a thread. At least a dozen direct investors in OpenAI were announced as backers in Anthropic’s $30 billion raise earlier this month, including Founders Fund, Iconiq, Insight Partners, and Sequoia Capital. Some dual investments are understandable if they come from the hedge fund or asset manager worlds, where their focus is still largely investing in public stocks (competitors or not). These include D1, Fidelity, and TPG. One of these was a bit shocking. Affiliated funds of BlackRock joined in Anthropic’s $30 billion raise even though BlackRock’s senior managing director and board member Adebayo Ogunlesi is also on OpenAI’s board of directors. In that world, it’s true that if various BlackRock funds get a chance to own OpenAI stock, they are likely to take it, never mind the personal association of a member of their senior leadership. (BlackRock runs every type of fund, including mutuals, closed-ends, and ETFs). And we all know the history of OpenAI and Microsoft’s relationship and why Microsoft is hedging its bets. Ditto for Nvidia. But venture capital funds have — until now — operated differently. VCs market themselves as “founder friendly” and “helpful,” the idea being that when a VC firm buys a chunk of a startup’s company, the investor will help that startup be successful, particularly against its major rivals. If you are an owner of both OpenAI and Anthropic, who does your loyalty belong to, besides your own investors? Techcrunch event Boston, MA | June 9, 2026 Additionally, startups are private companies. They typically share confidential information with their direct investors on their business status — data that isn’t disclosed publicly the way it is with public companies. In many cases, the VCs also take board seats, which carries another level of fiduciary responsibility to their portfolio companies. What makes this particular case even more interesting is that Sam Altman comes from the world of venture capital, as a former president of Y Combinator. He knows the drill. In 2024, he reportedly gave his investors a list of OpenAI’s rivals that he didn’t want them to back. It largely included companies launched by folks who left OpenAI, including Anthropic, xAI, and Safe Superintelligence. Altman later denied that he told OpenAI investors they would be barred from future rounds if they backed his list of perceived rivals. Altman did admit that he said if they “made non-passive investments,” they would no longer receive OpenAI’s confidential business information, according to documents in the lawsuit between Elon Musk and OpenAI, Business Insider reported . AI is also breaking the mold because of the record-breaking amounts of money that the largest AI labs are raising as they experience never-before-seen growth (and never-before-seen data center needs). At some point, when the hat is being passed around, the needs are so great and the possibilities of returns are so large, who can be expected to say no? It turns out that not all venture investors have yet slid down the slippery slope. Andreessen Horowitz backs OpenAI but not (yet) Anthropic. Menlo Ventures backs Anthropic but not (yet) OpenAI, for instance. In fact, in our admittedly not exhaustive research, we found a dozen investors that appear to only have direct investments in one of these companies, not both. Others include Bessemer Venture Partners, General Catalyst, and Greenoaks. (Note: We originally asked Claude to give us the list of dual investors. It got almost as many entries wrong as it got right, so all this for a very cool tech whose work sometimes remains less trustworthy than an intern’s.) Still, as we previously reported, the fact that this longstanding rule has been tossed by some of the most respected firms in the Valley, like Sequoia, is notable . One investor we reached out to simply shrugged and said that as long as the firm doesn’t have a board seat, no one sees the harm in it anymore. Still, conflict-of-interest policies should now become another thing that founders ask about before signing that term sheet, no matter who it’s from.",
      "cover_image_url": "https://techcrunch.com/wp-content/uploads/2019/06/GettyImages-516286525.jpg?resize=1200,1200"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "FreeBSD doesn't have Wi-Fi driver for my old MacBook. AI built one for me",
      "url": "https://vladimir.varank.in/notes/2026/02/freebsd-brcmfmac/",
      "published": "2026-02-23T21:44:28+00:00",
      "summary": "<p>Article URL: <a href=\"https://vladimir.varank.in/notes/2026/02/freebsd-brcmfmac/\">https://vladimir.varank.in/notes/2026/02/freebsd-brcmfmac/</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47129361\">https://news.ycombinator.com/item?id=47129361</a></p> <p>Points: 361</p> <p># Comments: 294</p>",
      "content_text": "FreeBSD doesn't have Wi-Fi driver for my old MacBook. AI built one for me My old 2016 MacBook Pro has been collecting dust in a cabinet for some time now. The laptop suffers from a “flexgate” problem , and I don’t have any practical use for it. For quite some time, I’ve been thinking about repurposing it as a guinea pig, to play with FreeBSD — an OS that I’d aspired to play with for a long while, but had never had a real reason to. During the recent holiday season, right after FreeBSD 15 release, I’ve finally found time to set the laptop up. Doing that I didn’t plan, or even think, this may turn into a story about AI coding. Background 2016 MacBook Pro models use Broadcom BCM4350 Wi-Fi chip. FreeBSD doesn’t have native support for this chip. To have a working Wi-Fi, a typical suggestion on FreeBSD forums, is to run wifibox — a tiny Linux VM, with the PCI Wi-Fi device in pass through, that allows Linux to manage the device through its brcmfmac driver. Brcmfmac is a Linux driver (ISC licence) for set of FullMAC chips from Broadcom. The driver offloads the processing jobs, like 802.11 frame movement, WPA encryption and decryption, etc, to the firmware, which is running inside the chip. Meanwhile, the driver and the OS do high-level management work (ref Broadcom brcmfmac(PCIe) in Linux Wireless documentation ). Say we want to build a native FreeBSD kernel module for the BCM4350 chip. In theory, this separation of jobs between the firmware and the driver sounds perfect. The “management” part of work is what FreeBSD already does for other supported Wi-Fi devices. We need to port some amount of existing “glue code” from specifics of Linux to FreeBSD. If we ignore a lot of details, the problem doesn’t sound too complicated, right? Act 1 A level-zero idea, when one hears about “porting a bunch of existing code from A to B”, in 2026 is, of course, to use AI. So that was what I tried. I cloned the brcmfmac subtree, and asked Claude Code to make it work for FreeBSD. FreeBSD already has drivers that work through LinuxKPI — compatibility layer for running Linux kernel drivers. So I specifically pointed Claude at the iwlwifi driver (a softmac driver for Intel wireless network card), asking “do as they did it”. And, at first, this even looked like this can work — Claude told me so. https://bsky.app/profile/vladimir.varank.in/post/3mawf7xbdws2r The module, indeed, compiled, but it didn’t do anything. Because, of course: the VM, where we tested the module, didn’t even have the hardware. After I set the PCI device into the VM, and attempted to load the driver against the chip, the challenges started to pop up immediately. The kernel paniced, and after Claude fixed the panics, it discovered that “module didn’t do anything”. Claude honestly tried to sift through the code, adding more and more #ifdef __FreeBSD__ wrappers here and there. It complained about missing features in LinuxKPI. The module kept causing panics, and the agent kept building FreeBSD-specific shims and callbacks, while warning me that this project will be very complicated and messy. Act 2 After a number of sessions, the diff, produced by the agent, stared to look significantly larger than what I’d hoped it will be. Even worse, the driver didn’t look even close to be working. This was right around time when Armin Ronacher posted about his experience building a game from scratch with Claude Opus and PI agent . Besides the part that working in Pi coding agent feels more productive, than in Claude Code, the video got me thinking that my approach to the task was too straightforward. The code of brcmfmac driver is moderately large. The driver supports several generations of Wi-Fi adaptors, different capabilities, etc. But my immediate task was very narrow: one chip, only PCI, only Wi-Fi client. Instead of continuing with the code, I spawned a fresh Pi session, and asked the agent to write a detailed specification of how the brcmfmac driver works, with the focus on BCM4350 Wi-Fi chip. I explicitly set the audience for the specification to be readers, who are tasked with implementing the specification in a clean-room environment. I asked the agent to explain how things work “to the bits”. I added some high-level details for how I wanted the specification to be laid out, and let the agent go brrrr. After a couple of rounds, the agent produced me a “book of 11 chapters”, that honestly looked like a fine specification % ls --tree spec/ spec ├── 00-overview.md ├── 01-data-structures.md ├── 02-bus-layer.md ├── 03-protocol-layer.md ├── 04-firmware-interface.md ├── 05-event-handling.md ├── 06-cfg80211-operations.md ├── 07-initialization.md ├── 08-data-path.md ├── 09-firmware-commands.md └── 10-structures-reference.md Of course, one can’t just trust what AI has written. To proofread the spec I spawned a clean Pi sessions, and — for fun — asked Codex model, to read the specification, and flag any places, where the text isn’t aligned with the driver’s code ( “Source code is the ground truth. The spec needs to be verified, and updated with any missing or wrong details” ). The agent followed through and found several places to fix, and also proposed multiple improvements. Of course, one can’t just trust what AI has written, even if this was in a proofreading session. To double-proofread the fixes I spawned another clean Pi sessions, asking Opus model to verify if what was proposed was aligned with how it works in the code of the driver. As a procrastination exercise, I tried this loop with a couple of coding models: Opus 4.5, Opus 4.6, Codex 5.2, Gemini 3 Pro preview. So far my experience was that Gemini hallucinated the most. This was quite sad, given that the model itself isn’t too bad for simple coding tasks, and it is free for a limited use. Having a written specification should have (in theory) explained how a driver’s code interacts with the firmware. Act 3 I started a fresh project, with nothing but the mentioned “spec”, and prompted the Pi agent, that we were building a brand new FreeBSD driver for BCM4350 chip. I pointed the agent to the specification, and asked it to ask me back about any important decisions we must make, and details we must outline, before jumping into “slopping the code”. The agent came back with questions and decision points, like “Will the driver live in the kernels source-tree?”, “Will we write the code in C?”, “Will we rely on LinuxKPI?”, “What are our high-level milestones?”, etc. One influential bit, that turned fairly productive moving forward, was that I asked the agent to document all these decision points in the project’s docs, and to explicitly referenced to these decision docs in the project’s AGENTS.md. It’s worth saying that, just like in any real project, not all decisions stayed to the end. For example, Initially I asked the agent to build the driver using linuxkpi and linuxkpi_wlan . My naive thinking was that, given the spec was written after looking at Linux driver’s code, it might be simpler for the agent, than building the on top of the native primitives. After a couple of sessions, it didn’t look like this was the case. I asked the agent to drop LinuxKPI from the code, and to refactor everything. The agent did it in one go, and updated the decision document. With specification, docs and a plan, the workflow process turned into a “boring routine”. The agent had SSH access to both the build host, and a testing VM, that had been running with the Wi-Fi PCI device passed from the host. It methodically crunched through the backlog of its own milestones, iterating over the code, building and testing the module. Every time a milestone or a portion was finished, I asked the agent to record the progress to the docs. Occasionally, an iteration of the code crashed or hanged the VM. When this happened, before fixing the problem, I asked — in a forked Pi’s session — to summarize, investigate and record the problem for agent’s future-self. After many low-involved sessions, I got a working FreeBSD kernel module for the BCM4350 Wi-Fi chip. The module supports Wi-Fi network scanning, 2.4GHz/5GHz connectivity, WPA/WPA2 authentication. https://bsky.app/profile/vladimir.varank.in/post/3mfhnvunnr22d The source code is in repository github.com/narqo/freebsd-brcmfmac . I didn’t write any piece of code there. There are several known issues, which I will task the agent to resolve, eventually. Meanwhile, I advise against using it for anything beyond a studying exercise. An existential discussion one Hacker News .",
      "cover_image_url": "/images/2026/bsky-freebsd-brcmfmac-1.png"
    },
    {
      "industry": "technology",
      "source": "Ars Technica",
      "title": "Panasonic, the former plasma king, will no longer make its own TVs",
      "url": "https://arstechnica.com/gadgets/2026/02/panasonic-the-former-plasma-king-will-no-longer-make-its-own-tvs/",
      "published": "2026-02-23T21:16:20+00:00",
      "summary": "Panasonic was one of the last Japanese companies still manufacturing TVs.",
      "content_text": "Panasonic, once revered for its plasma TVs, is giving up on making its own TV sets. Today, it announced that Chinese company Skyworth will take over manufacturing, marketing, and selling Panasonic-branded TVs. Skyworth is a Shenzhen-headquartered TV brand. The company claims to be “a top three global provider of the Android TV platform.” In July, research firm Omdia reported that Skyworth was one of the top-five TV brands by sales revenue in Q1 2025; however, Skyworth hasn’t been able to maintain that position regularly. Panasonic made its announcement at a “launch event,” FlatpanelsHD reported today. During the event, a Panasonic representative reportedly said: Under the agreement the new partner will lead sales, marketing, and logistics across the region, while Panasonic provide expertise and quality assurance to uphold its renowned audiovisual standards with full joint development on top-end OLED models. Panasonic also said that it will provide support “for all Panasonic TVs sold up to March 2026 and all those available from April.” Skyworth-made Panasonic TVs will be sold in the US and Europe. In the latter geography, the companies are aiming for double-digit market share. Panasonic’s wavering TV business Panasonic has been wavering on its commitment to the TV business for at least 12 years. When plasma ruled the living room, Panasonic dominated the market. In 2010, Panasonic controlled 40.7 percent of the plasma panel market, beating Samsung (33.7 percent) and LG (23.2 percent), according to research from consultancy DisplaySearch. But in March 2014, Panasonic quit making plasma TVs , pointing to increasing interest in flat-screen LCD TVs and economic challenges derived from the bankruptcy of global investment bank Lehman Brothers . At the time, Panasonic reportedly hadn’t made money off of its popular, high-contrast plasma TVs for years.",
      "cover_image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-56528381-1152x648-1771879994.jpg"
    },
    {
      "industry": "technology",
      "source": "Hacker News (Frontpage)",
      "title": "Software Engineer, Platform",
      "url": "https://www.ycombinator.com/companies/sim/jobs/Rj8TVRM-software-engineer-platform",
      "published": "2026-02-23T21:00:28+00:00",
      "summary": "<p>Article URL: <a href=\"https://www.ycombinator.com/companies/sim/jobs/Rj8TVRM-software-engineer-platform\">https://www.ycombinator.com/companies/sim/jobs/Rj8TVRM-software-engineer-platform</a></p> <p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=47128740\">https://news.ycombinator.com/item?id=47128740</a></p> <p>Points: 0</p> <p># Comments: 0</p>",
      "content_text": "About Sim Sim is the open-source platform to build, deploy, and orchestrate AI agents. Over 70,000 developers — from early-stage startups to the U.S. Department of Defense — use Sim to turn ideas into production-grade agentic workflows on a visual, Figma-like canvas. We connect to 1,000+ apps and LLMs, and our Copilot is the state of the art for building workflows with natural language. We're a team of 5, YC X25, and backed by a $7M Series A led by Standard Capital with participation from Paul Graham, Perplexity, SV Angel, and Y Combinator. We're based in San Francisco and build in person every day. Our mission is simple: agents will run the world, and Sim is the primary means by which that happens. About the Role You'll lead the Sim platform: the visual, end-to-end agent builder that tens of thousands of developers use every day. You'll own the front-end architecture, the real-time canvas, and the systems that turn visual flows into reliable, production agents. Sim has been called the \"Figma for AI agents\" — an intuitive drag-and-drop canvas where developers connect LLMs, tools, APIs, and logic blocks into complex agentic workflows. Your job is to make this experience fast, delightful, and powerful enough that developers never want to go back to writing agents in code. This is a foundational role. You'll define the front-end architecture, set performance standards, and shape the developer ergonomics of a product used by 70,000+ builders. On a team of 5, you own the entire surface area that developers interact with. What You'll Do Own the Sim visual canvas — the real-time, drag-and-drop workflow editor at the heart of the product Architect and build the front-end systems that render, connect, and manage complex agent workflows Design the interaction model for how developers create, test, debug, and deploy agents visually Build the bridge between visual flows and the underlying execution engine — ensuring what you see is what runs Drive performance and responsiveness across the editor, even as workflows grow in complexity Shape developer ergonomics and UX patterns across the platform — block configuration, variable wiring, real-time feedback, deployment flows Work across the stack in our Next.js monorepo when needed, shipping to production daily Set front-end standards for the codebase — component architecture, state management, testing patterns What We're Looking For Design-minded full-stack engineer with deep React and TypeScript experience Strong sense of product and UX — you care deeply about how things feel, not just how they work Experience building complex, interactive UIs — canvas editors, real-time collaborative tools, visual programming environments, or similarly demanding front-end systems Track record of shipping polished developer tools or creative tools to production Deep understanding of front-end performance — you know how to keep complex interfaces fast and responsive High agency and ownership — on a team this small, you define the roadmap as much as you execute it Comfort working across the stack in a Next.js monorepo; backend work is part of the job Experience with real-time systems (WebSockets, CRDTs, or similar) is a plus Experience with AWS infrastructure is a plus Contributions to open-source projects are a plus Why Sim Foundational impact. You're not joining a team — you're building one. Every decision you make shapes the product and the company. Real users, real scale. 70,000+ developers use Sim today. Your work ships fast and matters immediately. World-class backers. Series A - Paul Graham, Perplexity, SV Angel, and Y Combinator believe in what we're building. Open source at the core. We've been open source from day one and we're proud of it. 26,000+ GitHub stars and growing. Competitive salary + meaningful equity. Early team, significant ownership. Full benefits. Comprehensive health insurance, relocation. Visa sponsorship available.",
      "cover_image_url": "https://bookface-images.s3.amazonaws.com/logos/e18a421da73e99094f3b9b77ee157287c4bc5787.png?1771727385"
    }
  ]
}