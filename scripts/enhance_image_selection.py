#!/usr/bin/env python3
"""
ä¼˜åŒ–å›¾ç‰‡é€‰æ‹©ç®—æ³•
- æ”¹è¿›å›¾ç‰‡ä¸ä¸»é¢˜çš„åŒ¹é…åº¦
- å¢åŠ è¡Œä¸šç›¸å…³æ€§è¯„åˆ†
- ä¼˜åŒ–å›¾ç‰‡æŸ¥è¯¢å…³é”®è¯
"""

import re
import json
from pathlib import Path
from typing import List, Dict, Any
import requests
from urllib.parse import quote

class SmartImageSelector:
    """æ™ºèƒ½å›¾ç‰‡é€‰æ‹©å™¨"""
    
    def __init__(self, industry: str = "technology"):
        self.industry = industry
        self.cache_dir = Path("data/local_image_cache")
        self.cache_dir.mkdir(exist_ok=True)
        
        # è¡Œä¸šå…³é”®è¯æ˜ å°„
        self.industry_keywords = {
            "technology": {
                "primary": ["AI", "artificial intelligence", "machine learning", "cloud computing"],
                "secondary": ["software", "hardware", "robot", "digital", "tech"],
                "visual": ["computer", "server", "circuit", "chip", "data center"]
            },
            "ecommerce": {
                "primary": ["ecommerce", "online shopping", "digital marketing", "logistics"],
                "secondary": ["delivery", "package", "store", "payment", "shopping cart"],
                "visual": ["warehouse", "truck", "credit card", "mobile shopping"]
            },
            "manufacturing": {
                "primary": ["manufacturing", "factory", "industrial", "automation"],
                "secondary": ["production", "assembly", "machine", "robot", "quality control"],
                "visual": ["factory floor", "assembly line", "industrial robot", "3d printer"]
            },
            "finance": {
                "primary": ["finance", "banking", "investment", "trading"],
                "secondary": ["stock market", "cryptocurrency", "blockchain", "economy"],
                "visual": ["stock chart", "bank building", "coins", "calculator"]
            },
            "education": {
                "primary": ["education", "learning", "online course", "classroom"],
                "secondary": ["student", "teacher", "school", "university", "training"],
                "visual": ["classroom", "books", "graduation", "online learning"]
            },
            "health": {
                "primary": ["healthcare", "medical", "hospital", "doctor"],
                "secondary": ["medicine", "patient", "treatment", "research"],
                "visual": ["hospital", "medical equipment", "doctor patient", "laboratory"]
            }
        }
    
    def extract_article_keywords(self, md_text: str) -> List[str]:
        """ä»æ–‡ç« å†…å®¹æå–å…³é”®è¯"""
        
        keywords = []
        
        # æå–æ ‡é¢˜
        title_match = re.search(r'title:\s*["\']?(.+?)["\']?\s*$', md_text, re.MULTILINE | re.IGNORECASE)
        if title_match:
            title = title_match.group(1)
            # ä»æ ‡é¢˜æå–å…³é”®è¯
            title_words = re.findall(r'\b\w+\b', title.lower())
            stop_words = {"the", "and", "for", "with", "about", "analysis", "report", "daily", "weekly"}
            meaningful = [w for w in title_words if w not in stop_words and len(w) > 3]
            keywords.extend(meaningful[:5])
        
        # æå–H1å’ŒH2æ ‡é¢˜
        headings = re.findall(r'^#+\s+(.+)$', md_text, re.MULTILINE)
        for heading in headings[:3]:  # æœ€å¤šå–å‰3ä¸ªæ ‡é¢˜
            heading_words = re.findall(r'\b\w+\b', heading.lower())
            meaningful = [w for w in heading_words if len(w) > 3]
            keywords.extend(meaningful[:3])
        
        # æ·»åŠ è¡Œä¸šç‰¹å®šå…³é”®è¯
        if self.industry in self.industry_keywords:
            industry_info = self.industry_keywords[self.industry]
            keywords.extend(industry_info["primary"][:2])
            keywords.extend(industry_info["visual"][:2])
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_keywords = []
        seen = set()
        for kw in keywords:
            kw_lower = kw.lower()
            if kw_lower not in seen and len(kw_lower) > 2:
                seen.add(kw_lower)
                unique_keywords.append(kw)
        
        return unique_keywords[:10]  # æœ€å¤š10ä¸ªå…³é”®è¯
    
    def generate_search_queries(self, keywords: List[str]) -> List[str]:
        """ç”Ÿæˆå›¾ç‰‡æœç´¢æŸ¥è¯¢"""
        
        queries = []
        
        # ç»„åˆå…³é”®è¯æŸ¥è¯¢
        if keywords:
            # ä¸»è¦å…³é”®è¯ç»„åˆ
            if len(keywords) >= 2:
                queries.append(f"{keywords[0]} {keywords[1]}")
            
            # å•ä¸ªé‡è¦å…³é”®è¯
            for kw in keywords[:3]:
                if len(kw.split()) == 1:  # å•ä¸ªè¯
                    queries.append(kw)
            
            # è¡Œä¸šç‰¹å®šæŸ¥è¯¢
            if self.industry in self.industry_keywords:
                industry_info = self.industry_keywords[self.industry]
                queries.append(" ".join(industry_info["primary"][:2]))
                queries.append(" ".join(industry_info["visual"][:2]))
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æŸ¥è¯¢
        if not queries:
            queries = ["technology", "innovation", "digital transformation"]
        
        return queries[:5]  # æœ€å¤š5ä¸ªæŸ¥è¯¢
    
    def search_wikimedia_images(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """ä»Wikimedia Commonsæœç´¢å›¾ç‰‡"""
        
        try:
            # Wikimedia Commons API
            url = "https://commons.wikimedia.org/w/api.php"
            params = {
                "action": "query",
                "format": "json",
                "generator": "search",
                "gsrnamespace": "6",  # File namespace
                "gsrsearch": query,
                "gsrlimit": limit,
                "prop": "imageinfo|pageterms",
                "iiprop": "url|size|mime|extmetadata",
                "iiurlwidth": 800,
                "iiextmetadatafilter": "ImageDescription|LicenseShortName|Artist",
                "wbptterms": "label"
            }
            
            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            images = []
            if "query" in data and "pages" in data["query"]:
                for page_id, page_info in data["query"]["pages"].items():
                    if "imageinfo" in page_info and len(page_info["imageinfo"]) > 0:
                        img_info = page_info["imageinfo"][0]
                        
                        image_data = {
                            "title": page_info.get("title", "").replace("File:", ""),
                            "url": img_info.get("url", ""),
                            "thumbnail_url": img_info.get("thumburl", ""),
                            "width": img_info.get("width", 0),
                            "height": img_info.get("height", 0),
                            "description": img_info.get("extmetadata", {}).get("ImageDescription", {}).get("value", ""),
                            "license": img_info.get("extmetadata", {}).get("LicenseShortName", {}).get("value", "Unknown"),
                            "artist": img_info.get("extmetadata", {}).get("Artist", {}).get("value", "Unknown"),
                            "source": "wikimedia",
                            "query_used": query
                        }
                        
                        images.append(image_data)
            
            return images
            
        except Exception as e:
            print(f"âš ï¸  Wikimediaæœç´¢å¤±è´¥ ({query}): {e}")
            return []
    
    def calculate_relevance_score(self, image: Dict[str, Any], keywords: List[str]) -> float:
        """è®¡ç®—å›¾ç‰‡ç›¸å…³æ€§è¯„åˆ†"""
        
        score = 0.0
        
        # æ£€æŸ¥å›¾ç‰‡æ ‡é¢˜å’Œæè¿°
        image_text = f"{image.get('title', '')} {image.get('description', '')}".lower()
        
        # å…³é”®è¯åŒ¹é…
        for keyword in keywords:
            keyword_lower = keyword.lower()
            if keyword_lower in image_text:
                # å®Œå…¨åŒ¹é…åŠ åˆ†æ›´å¤š
                if f" {keyword_lower} " in f" {image_text} ":
                    score += 2.0
                else:
                    score += 1.0
        
        # è¡Œä¸šç‰¹å®šåŒ¹é…
        if self.industry in self.industry_keywords:
            industry_info = self.industry_keywords[self.industry]
            
            # æ£€æŸ¥ä¸»è¦å…³é”®è¯
            for term in industry_info["primary"]:
                if term.lower() in image_text:
                    score += 1.5
            
            # æ£€æŸ¥è§†è§‰å…³é”®è¯
            for term in industry_info["visual"]:
                if term.lower() in image_text:
                    score += 2.0  # è§†è§‰åŒ¹é…æ›´é‡è¦
        
        # å›¾ç‰‡è´¨é‡è€ƒè™‘
        width = image.get('width', 0)
        height = image.get('height', 0)
        
        if width >= 800 and height >= 600:
            score += 1.0  # é«˜åˆ†è¾¨ç‡
        elif width >= 400 and height >= 300:
            score += 0.5  # ä¸­ç­‰åˆ†è¾¨ç‡
        
        # æ¥æºå¯é æ€§
        if image.get('source') == 'wikimedia':
            score += 0.5  # Wikimediaå›¾ç‰‡è´¨é‡è¾ƒé«˜
        
        # è®¸å¯è¯è€ƒè™‘ï¼ˆä¼˜å…ˆä½¿ç”¨è‡ªç”±è®¸å¯è¯ï¼‰
        license_text = image.get('license', '').lower()
        free_licenses = ['cc-by', 'cc-by-sa', 'public domain', 'creative commons']
        if any(license in license_text for license in free_licenses):
            score += 0.5
        
        return score
    
    def select_best_images(self, md_text: str, num_images: int = 2) -> List[Dict[str, Any]]:
        """é€‰æ‹©æœ€ç›¸å…³çš„å›¾ç‰‡"""
        
        print(f"ğŸ” ä¸º{self.industry}è¡Œä¸šæ–‡ç« é€‰æ‹©å›¾ç‰‡...")
        
        # æå–å…³é”®è¯
        keywords = self.extract_article_keywords(md_text)
        print(f"   æå–å…³é”®è¯: {', '.join(keywords[:5])}")
        
        # ç”Ÿæˆæœç´¢æŸ¥è¯¢
        queries = self.generate_search_queries(keywords)
        print(f"   æœç´¢æŸ¥è¯¢: {', '.join(queries)}")
        
        # æœç´¢å›¾ç‰‡
        all_candidates = []
        for query in queries:
            print(f"   æœç´¢: {query}")
            candidates = self.search_wikimedia_images(query, limit=3)
            all_candidates.extend(candidates)
        
        if not all_candidates:
            print("âš ï¸  æœªæ‰¾åˆ°å›¾ç‰‡ï¼Œä½¿ç”¨å¤‡ç”¨æŸ¥è¯¢...")
            # å¤‡ç”¨æŸ¥è¯¢
            backup_queries = ["technology", "business", "innovation"]
            for query in backup_queries:
                candidates = self.search_wikimedia_images(query, limit=2)
                all_candidates.extend(candidates)
        
        # è®¡ç®—ç›¸å…³æ€§è¯„åˆ†
        for candidate in all_candidates:
            candidate['relevance_score'] = self.calculate_relevance_score(candidate, keywords)
        
        # æŒ‰è¯„åˆ†æ’åº
        all_candidates.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        # é€‰æ‹©æœ€ä½³å›¾ç‰‡
        selected = all_candidates[:num_images]
        
        # è¾“å‡ºç»“æœ
        print(f"âœ… æ‰¾åˆ° {len(all_candidates)} å¼ å€™é€‰å›¾ç‰‡")
        for i, img in enumerate(selected[:3], 1):
            print(f"   {i}. {img.get('title', 'Unknown')[:50]}... (è¯„åˆ†: {img['relevance_score']:.1f})")
        
        return selected
    
    def update_image_pool(self, selected_images: List[Dict[str, Any]]):
        """æ›´æ–°æœ¬åœ°å›¾ç‰‡æ± """
        
        pool_path = Path("data/public_image_pool.json")
        
        try:
            if pool_path.exists():
                with open(pool_path, 'r', encoding='utf-8') as f:
                    pool = json.load(f)
            else:
                pool = {"items": [], "last_updated": ""}
            
            # æ·»åŠ æ–°å›¾ç‰‡
            for img in selected_images:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = False
                for item in pool["items"]:
                    if item.get("url") == img.get("url"):
                        existing = True
                        break
                
                if not existing:
                    pool["items"].append(img)
            
            # é™åˆ¶æ•°é‡
            if len(pool["items"]) > 1000:
                pool["items"] = pool["items"][-1000:]
            
            pool["last_updated"] = datetime.datetime.now().isoformat()
            
            # ä¿å­˜
            with open(pool_path, 'w', encoding='utf-8') as f:
                json.dump(pool, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… å›¾ç‰‡æ± å·²æ›´æ–°ï¼Œç°æœ‰ {len(pool['items'])} å¼ å›¾ç‰‡")
            
        except Exception as e:
            print(f"âš ï¸  æ›´æ–°å›¾ç‰‡æ± å¤±è´¥: {e}")

def test_image_selection():
    """æµ‹è¯•å›¾ç‰‡é€‰æ‹©åŠŸèƒ½"""
    
    print("ğŸ§ª æµ‹è¯•å›¾ç‰‡é€‰æ‹©ç®—æ³•...")
    print("=" * 50)
    
    # æµ‹è¯•æ–‡ç« å†…å®¹
    test_article = """
---
title: "äººå·¥æ™ºèƒ½åœ¨åˆ¶é€ ä¸šçš„åº”ç”¨ä¸å‰æ™¯åˆ†æ"
date: 2026-02-25
categories: ["technology", "manufacturing"]
---

# äººå·¥æ™ºèƒ½åœ¨åˆ¶é€ ä¸šçš„åº”ç”¨ä¸å‰æ™¯åˆ†æ

## æ™ºèƒ½åˆ¶é€ çš„å‘å±•è¶‹åŠ¿

éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œåˆ¶é€ ä¸šæ­£åœ¨ç»å†ä¸€åœºæ·±åˆ»çš„æ•°å­—åŒ–è½¬å‹ã€‚ä»è‡ªåŠ¨åŒ–ç”Ÿäº§çº¿åˆ°æ™ºèƒ½è´¨é‡æ§åˆ¶ï¼ŒAIæŠ€æœ¯æ­£åœ¨é‡å¡‘åˆ¶é€ ä¸šçš„å„ä¸ªç¯èŠ‚ã€‚

## å…³é”®æŠ€æœ¯åº”ç”¨

### 1. æœºå™¨è§†è§‰æ£€æµ‹
åŸºäºæ·±åº¦å­¦ä¹ çš„æœºå™¨è§†è§‰ç³»ç»Ÿå¯ä»¥å®æ—¶æ£€æµ‹äº§å“ç¼ºé™·ï¼Œæé«˜è´¨é‡æ§åˆ¶æ•ˆç‡ã€‚

### 2. é¢„æµ‹æ€§ç»´æŠ¤
é€šè¿‡åˆ†æè®¾å¤‡ä¼ æ„Ÿå™¨æ•°æ®ï¼ŒAIå¯ä»¥é¢„æµ‹è®¾å¤‡æ•…éšœï¼Œå‡å°‘åœæœºæ—¶é—´ã€‚

### 3. æ™ºèƒ½ä¾›åº”é“¾ä¼˜åŒ–
AIç®—æ³•å¯ä»¥ä¼˜åŒ–åº“å­˜ç®¡ç†ã€ç‰©æµè°ƒåº¦å’Œç”Ÿäº§è®¡åˆ’ã€‚
"""
    
    # æµ‹è¯•ä¸åŒè¡Œä¸š
    test_industries = ["technology", "manufacturing", "ecommerce"]
    
    for industry in test_industries:
        print(f"\nğŸ“Š æµ‹è¯• {industry} è¡Œä¸š...")
        
        selector = SmartImageSelector(industry)
        
        # æå–å…³é”®è¯
        keywords = selector.extract_article_keywords(test_article)
        print(f"   å…³é”®è¯: {', '.join(keywords[:5])}")
        
        # ç”ŸæˆæŸ¥è¯¢
        queries = selector.generate_search_queries(keywords)
        print(f"   æŸ¥è¯¢: {', '.join(queries)}")
        
        # é€‰æ‹©å›¾ç‰‡
        images = selector.select_best_images(test_article, num_images=2)
        
        if images:
            print(f"   é€‰æ‹© {len(images)} å¼ å›¾ç‰‡:")
            for img in images:
                score = img.get('relevance_score', 0)
                title = img.get('title', 'Unknown')[:40]
                print(f"     - {title}... (è¯„åˆ†: {score:.1f})")
        else:
            print("   âŒ æœªæ‰¾åˆ°åˆé€‚å›¾ç‰‡")
    
    print("\n" + "=" * 50)
    print("âœ… æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    import datetime
    
    # è¿è¡Œæµ‹è¯•
    test_image_selection()
    
    print("\nğŸš€ ä½¿ç”¨è¯´æ˜:")
    print("   1. åœ¨è‡ªåŠ¨åŒ–è„šæœ¬ä¸­é›†æˆ:")
    print("      selector = SmartImageSelector(industry='technology')")
    print("      images = selector.select_best_images(article_text)")
    print("")
    print("   2. æ›´æ–°ç°æœ‰è„šæœ¬:")
    print("      python3 scripts/enhance_image_selection.py --integrate")
    print("")
    print("   3. æ‰¹é‡æµ‹è¯•:")
    print("      python3 scripts/enhance_image_selection.py --test-all")