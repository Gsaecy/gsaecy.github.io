#!/usr/bin/env python3
"""
é›†æˆå›¾ç‰‡ä¿®å¤åˆ°ç°æœ‰ç³»ç»Ÿ
- æ›´æ–° auto_add_public_images.py
- ä¿®æ”¹ç›¸å…³é…ç½®
- åˆ›å»ºå¤‡ä»½å’Œæµ‹è¯•
"""

import os
import shutil
from pathlib import Path
import re

def backup_original_script():
    """å¤‡ä»½åŸå§‹è„šæœ¬"""
    original_path = Path("scripts/auto_add_public_images.py")
    backup_path = Path("scripts/auto_add_public_images.py.backup")
    
    if original_path.exists():
        shutil.copy2(original_path, backup_path)
        print(f"âœ… å¤‡ä»½åŸå§‹è„šæœ¬: {backup_path}")
        return True
    else:
        print(f"âŒ åŸå§‹è„šæœ¬ä¸å­˜åœ¨: {original_path}")
        return False

def update_auto_add_images_script():
    """æ›´æ–°è‡ªåŠ¨æ·»åŠ å›¾ç‰‡è„šæœ¬"""
    
    # è¯»å–å¢å¼ºç‰ˆå›¾ç‰‡é€‰æ‹©å™¨çš„å…³é”®å‡½æ•°
    enhance_script = Path("scripts/enhance_image_selection.py")
    if not enhance_script.exists():
        print(f"âŒ å¢å¼ºè„šæœ¬ä¸å­˜åœ¨: {enhance_script}")
        return False
    
    with open(enhance_script, 'r', encoding='utf-8') as f:
        enhance_content = f.read()
    
    # æå–å…³é”®å‡½æ•°
    functions_to_extract = [
        "extract_article_keywords",
        "generate_search_queries", 
        "calculate_relevance_score"
    ]
    
    extracted_code = ""
    for func_name in functions_to_extract:
        pattern = rf'def {func_name}\(.*?\):(.*?)(?=\n\ndef|\nclass|\Z)'
        match = re.search(pattern, enhance_content, re.DOTALL)
        if match:
            extracted_code += f"\n# ä» enhance_image_selection.py å¯¼å…¥çš„ {func_name} å‡½æ•°\n"
            extracted_code += f"def {func_name}{match.group(0)[len(func_name)+4:]}\n"
    
    # è¯»å–åŸå§‹è„šæœ¬
    original_path = Path("scripts/auto_add_public_images.py")
    if not original_path.exists():
        print(f"âŒ åŸå§‹è„šæœ¬ä¸å­˜åœ¨: {original_path}")
        return False
    
    with open(original_path, 'r', encoding='utf-8') as f:
        original_content = f.read()
    
    # æŸ¥æ‰¾å¹¶æ›¿æ¢ extract_queries å‡½æ•°
    old_extract_queries = '''def extract_queries(md_text: str) -> List[str]:
    # Prefer front matter title (we may remove H1 from body), then fall back to headings.
    lines = md_text.splitlines()
    title = extract_front_matter_title(md_text)
    headings = []
    for ln in lines[:160]:
        if (not title) and ln.startswith("# "):
            title = ln[2:].strip()
        if ln.startswith("## "):
            headings.append(ln[3:].strip())

    base = title or "industry analysis"

    # crude domain mapping based on Chinese keywords
    # keep it minimal and predictable
    mapping = [
        ("æ–°èƒ½æºæ±½è½¦", "electric vehicle"),
        ("ç”µæ± ", "battery"),
        ("å……ç”µ", "charging station"),
        ("æ™ºèƒ½é©¾é©¶", "autonomous driving"),
        ("åŒ»ç–—", "medical technology"),
        ("é‡‘è", "financial technology"),
        ("æ•™è‚²", "online learning"),
    ]

    hints = []
    for zh, en in mapping:
        if zh in md_text:
            hints.append(en)

    # Build up to 3 queries
    queries = []
    if hints:
        queries.append(" ".join(hints[:2]))

    # Title keywords (strip punctuation). Also add a stable "AI" hint if present.
    base2 = re.sub(r"[\\W_]+", " ", base)
    if "AI" in md_text or "äººå·¥æ™ºèƒ½" in md_text:
        base2 = ("AI " + base2).strip()

    if base2.strip():
        queries.append(base2.strip())

    if headings:
        h = re.sub(r"[\\W_]+", " ", headings[0])
        if h.strip():
            queries.append(h.strip())

    # dedupe
    out = []
    for q in queries:
        q = q.strip()
        if q and q not in out:
            out.append(q)

    return out[:3]'''
    
    # æ–°çš„ extract_queries å‡½æ•°
    new_extract_queries = '''def extract_queries(md_text: str) -> List[str]:
    """å¢å¼ºç‰ˆæŸ¥è¯¢æå–ï¼Œè€ƒè™‘è¡Œä¸šå’Œä¸»é¢˜ç›¸å…³æ€§"""
    
    # é¦–å…ˆå°è¯•ä»front matterè·å–è¡Œä¸šä¿¡æ¯
    industry = "technology"  # é»˜è®¤è¡Œä¸š
    
    # ä»front matteræå–categories
    fm_match = re.match(r"^---\\n(.*?)\\n---\\n", md_text, flags=re.S)
    if fm_match:
        fm_text = fm_match.group(1)
        # æŸ¥æ‰¾categories
        cat_match = re.search(r'categories:\\s*\\[.*?(technology|ecommerce|manufacturing|finance|education|health).*?\\]', fm_text, re.IGNORECASE)
        if cat_match:
            industry = cat_match.group(1).lower()
    
    # ä½¿ç”¨å¢å¼ºç‰ˆå…³é”®è¯æå–
    keywords = extract_article_keywords(md_text, industry)
    
    # ç”Ÿæˆæœç´¢æŸ¥è¯¢
    queries = generate_search_queries(keywords, industry)
    
    return queries[:5]  # æœ€å¤š5ä¸ªæŸ¥è¯¢'''
    
    # æ›¿æ¢å‡½æ•°
    if old_extract_queries in original_content:
        updated_content = original_content.replace(old_extract_queries, new_extract_queries)
        
        # åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ å¯¼å…¥å’Œè¾…åŠ©å‡½æ•°
        imports_to_add = '''from typing import List, Dict, Any
import re

# è¡Œä¸šå…³é”®è¯æ˜ å°„
INDUSTRY_KEYWORDS = {
    "technology": {
        "primary": ["AI", "artificial intelligence", "machine learning", "cloud computing"],
        "secondary": ["software", "hardware", "robot", "digital", "tech"],
        "visual": ["computer", "server", "circuit", "chip", "data center"]
    },
    "ecommerce": {
        "primary": ["ecommerce", "online shopping", "digital marketing", "logistics"],
        "secondary": ["delivery", "package", "store", "payment", "shopping cart"],
        "visual": ["warehouse", "truck", "credit card", "mobile shopping"]
    },
    "manufacturing": {
        "primary": ["manufacturing", "factory", "industrial", "automation"],
        "secondary": ["production", "assembly", "machine", "robot", "quality control"],
        "visual": ["factory floor", "assembly line", "industrial robot", "3d printer"]
    }
}

def extract_article_keywords(md_text: str, industry: str = "technology") -> List[str]:
    """ä»æ–‡ç« å†…å®¹æå–å…³é”®è¯"""
    
    keywords = []
    
    # æå–æ ‡é¢˜
    title_match = re.search(r'title:\\s*["\\']?(.+?)["\\']?\\s*$', md_text, re.MULTILINE | re.IGNORECASE)
    if title_match:
        title = title_match.group(1)
        # ä»æ ‡é¢˜æå–å…³é”®è¯
        title_words = re.findall(r'\\b\\w+\\b', title.lower())
        stop_words = {"the", "and", "for", "with", "about", "analysis", "report", "daily", "weekly"}
        meaningful = [w for w in title_words if w not in stop_words and len(w) > 3]
        keywords.extend(meaningful[:5])
    
    # æå–H1å’ŒH2æ ‡é¢˜
    headings = re.findall(r'^#+\\s+(.+)$', md_text, re.MULTILINE)
    for heading in headings[:3]:
        heading_words = re.findall(r'\\b\\w+\\b', heading.lower())
        meaningful = [w for w in heading_words if len(w) > 3]
        keywords.extend(meaningful[:3])
    
    # æ·»åŠ è¡Œä¸šç‰¹å®šå…³é”®è¯
    if industry in INDUSTRY_KEYWORDS:
        industry_info = INDUSTRY_KEYWORDS[industry]
        keywords.extend(industry_info["primary"][:2])
        keywords.extend(industry_info["visual"][:2])
    
    # å»é‡å¹¶é™åˆ¶æ•°é‡
    unique_keywords = []
    seen = set()
    for kw in keywords:
        kw_lower = kw.lower()
        if kw_lower not in seen and len(kw_lower) > 2:
            seen.add(kw_lower)
            unique_keywords.append(kw)
    
    return unique_keywords[:10]

def generate_search_queries(keywords: List[str], industry: str = "technology") -> List[str]:
    """ç”Ÿæˆå›¾ç‰‡æœç´¢æŸ¥è¯¢"""
    
    queries = []
    
    # ç»„åˆå…³é”®è¯æŸ¥è¯¢
    if keywords:
        # ä¸»è¦å…³é”®è¯ç»„åˆ
        if len(keywords) >= 2:
            queries.append(f"{keywords[0]} {keywords[1]}")
        
        # å•ä¸ªé‡è¦å…³é”®è¯
        for kw in keywords[:3]:
            if len(kw.split()) == 1:
                queries.append(kw)
        
        # è¡Œä¸šç‰¹å®šæŸ¥è¯¢
        if industry in INDUSTRY_KEYWORDS:
            industry_info = INDUSTRY_KEYWORDS[industry]
            queries.append(" ".join(industry_info["primary"][:2]))
            queries.append(" ".join(industry_info["visual"][:2]))
    
    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æŸ¥è¯¢
    if not queries:
        queries = ["technology", "innovation", "digital transformation"]
    
    return queries[:5]

def calculate_image_relevance(image_info: dict, keywords: List[str], industry: str) -> float:
    """è®¡ç®—å›¾ç‰‡ç›¸å…³æ€§è¯„åˆ†"""
    
    score = 0.0
    
    # æ£€æŸ¥å›¾ç‰‡æ ‡é¢˜å’Œæè¿°
    image_text = f"{image_info.get('title', '')} {image_info.get('description', '')}".lower()
    
    # å…³é”®è¯åŒ¹é…
    for keyword in keywords:
        keyword_lower = keyword.lower()
        if keyword_lower in image_text:
            if f" {keyword_lower} " in f" {image_text} ":
                score += 2.0
            else:
                score += 1.0
    
    # è¡Œä¸šç‰¹å®šåŒ¹é…
    if industry in INDUSTRY_KEYWORDS:
        industry_info = INDUSTRY_KEYWORDS[industry]
        
        for term in industry_info["primary"]:
            if term.lower() in image_text:
                score += 1.5
        
        for term in industry_info["visual"]:
            if term.lower() in image_text:
                score += 2.0
    
    # å›¾ç‰‡è´¨é‡è€ƒè™‘
    width = image_info.get('width', 0)
    height = image_info.get('height', 0)
    
    if width >= 800 and height >= 600:
        score += 1.0
    elif width >= 400 and height >= 300:
        score += 0.5
    
    return score
'''
        
        # åœ¨åŸå§‹å†…å®¹ä¸­æ‰¾åˆ°åˆé€‚çš„ä½ç½®æ’å…¥
        # é€šå¸¸åœ¨å¯¼å…¥ä¹‹åï¼Œç¬¬ä¸€ä¸ªå‡½æ•°ä¹‹å‰
        lines = updated_content.split('\n')
        insert_index = 0
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå‡½æ•°å®šä¹‰
        for i, line in enumerate(lines):
            if line.strip().startswith('def '):
                insert_index = i
                break
        
        # æ’å…¥å¢å¼ºä»£ç 
        lines.insert(insert_index, imports_to_add)
        updated_content = '\n'.join(lines)
        
        # ä¿å­˜æ›´æ–°åçš„è„šæœ¬
        updated_path = Path("scripts/auto_add_public_images_enhanced.py")
        with open(updated_path, 'w', encoding='utf-8') as f:
            f.write(updated_content)
        
        print(f"âœ… åˆ›å»ºå¢å¼ºç‰ˆè„šæœ¬: {updated_path}")
        print(f"   åŸå§‹è„šæœ¬å·²å¤‡ä»½: scripts/auto_add_public_images.py.backup")
        
        return True
    else:
        print("âŒ æœªæ‰¾åˆ°åŸå§‹ extract_queries å‡½æ•°")
        return False

def create_integration_test():
    """åˆ›å»ºé›†æˆæµ‹è¯•"""
    
    test_content = '''#!/usr/bin/env python3
"""
é›†æˆæµ‹è¯•è„šæœ¬
æµ‹è¯•å¢å¼ºç‰ˆå›¾ç‰‡é€‰æ‹©åŠŸèƒ½
"""

import sys
from pathlib import Path

# æ·»åŠ è„šæœ¬ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def test_enhanced_image_selection():
    """æµ‹è¯•å¢å¼ºç‰ˆå›¾ç‰‡é€‰æ‹©"""
    
    print("ğŸ§ª æµ‹è¯•å¢å¼ºç‰ˆå›¾ç‰‡é€‰æ‹©é›†æˆ...")
    print("=" * 50)
    
    # æµ‹è¯•æ–‡ç« 
    test_articles = [
        {
            "title": "äººå·¥æ™ºèƒ½åœ¨åˆ¶é€ ä¸šçš„åº”ç”¨ä¸å‰æ™¯åˆ†æ",
            "content": """---
title: "äººå·¥æ™ºèƒ½åœ¨åˆ¶é€ ä¸šçš„åº”ç”¨ä¸å‰æ™¯åˆ†æ"
date: 2026-02-25
categories: ["technology", "manufacturing"]
---

# äººå·¥æ™ºèƒ½åœ¨åˆ¶é€ ä¸šçš„åº”ç”¨ä¸å‰æ™¯åˆ†æ

## æ™ºèƒ½åˆ¶é€ çš„å‘å±•è¶‹åŠ¿

éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œåˆ¶é€ ä¸šæ­£åœ¨ç»å†ä¸€åœºæ·±åˆ»çš„æ•°å­—åŒ–è½¬å‹ã€‚
""",
            "expected_industry": "manufacturing"
        },
        {
            "title": "è·¨å¢ƒç”µå•†å¹³å°å‘å±•è¶‹åŠ¿",
            "content": """---
title: "è·¨å¢ƒç”µå•†å¹³å°å‘å±•è¶‹åŠ¿"
date: 2026-02-25  
categories: ["ecommerce", "technology"]
---

# è·¨å¢ƒç”µå•†å¹³å°å‘å±•è¶‹åŠ¿

## å¸‚åœºç°çŠ¶åˆ†æ

å…¨çƒè·¨å¢ƒç”µå•†å¸‚åœºæŒç»­å¢é•¿ï¼Œæ–°æŠ€æœ¯æ­£åœ¨æ”¹å˜ä¼ ç»Ÿè´¸æ˜“æ¨¡å¼ã€‚
""",
            "expected_industry": "ecommerce"
        },
        {
            "title": "äº‘è®¡ç®—æŠ€æœ¯åœ¨ä¼ä¸šæ•°å­—åŒ–è½¬å‹ä¸­çš„ä½œç”¨",
            "content": """---
title: "äº‘è®¡ç®—æŠ€æœ¯åœ¨ä¼ä¸šæ•°å­—åŒ–è½¬å‹ä¸­çš„ä½œç”¨"
date: 2026-02-25
categories: ["technology"]
---

# äº‘è®¡ç®—æŠ€æœ¯åœ¨ä¼ä¸šæ•°å­—åŒ–è½¬å‹ä¸­çš„ä½œç”¨

## æŠ€æœ¯ä¼˜åŠ¿åˆ†æ

äº‘è®¡ç®—æä¾›äº†å¼¹æ€§ã€å¯æ‰©å±•çš„ITåŸºç¡€è®¾æ–½ï¼Œæ”¯æŒä¼ä¸šå¿«é€Ÿåˆ›æ–°ã€‚
""",
            "expected_industry": "technology"
        }
    ]
    
    # å¯¼å…¥å¢å¼ºç‰ˆè„šæœ¬
    try:
        from auto_add_public_images_enhanced import extract_queries, extract_article_keywords, generate_search_queries
        print("âœ… æˆåŠŸå¯¼å…¥å¢å¼ºç‰ˆå‡½æ•°")
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•æ¯ä¸ªæ–‡ç« 
    for i, article in enumerate(test_articles, 1):
        print(f"\\nğŸ“ æµ‹è¯•æ–‡ç«  {i}: {article['title']}")
        print(f"   é¢„æœŸè¡Œä¸š: {article['expected_industry']}")
        
        # æµ‹è¯•å…³é”®è¯æå–
        keywords = extract_article_keywords(article['content'], article['expected_industry'])
        print(f"   æå–å…³é”®è¯: {', '.join(keywords[:5])}")
        
        # æµ‹è¯•æŸ¥è¯¢ç”Ÿæˆ
        queries = extract_queries(article['content'])
        print(f"   ç”ŸæˆæŸ¥è¯¢: {', '.join(queries)}")
        
        # éªŒè¯æŸ¥è¯¢è´¨é‡
        if queries:
            print(f"   âœ… æŸ¥è¯¢ç”ŸæˆæˆåŠŸ ({len(queries)} ä¸ªæŸ¥è¯¢)")
            
            # æ£€æŸ¥æŸ¥è¯¢æ˜¯å¦åŒ…å«è¡Œä¸šç›¸å…³è¯æ±‡
            industry = article['expected_industry']
            industry_terms = {
                "technology": ["ai", "tech", "digital", "cloud"],
                "ecommerce": ["ecommerce", "shopping", "online", "delivery"],
                "manufacturing": ["manufacturing", "factory", "industrial", "production"]
            }
            
            if industry in industry_terms:
                found_terms = []
                for term in industry_terms[industry]:
                    for query in queries:
                        if term in query.lower():
                            found_terms.append(term)
                            break
                
                if found_terms:
                    print(f"   âœ… æŸ¥è¯¢åŒ…å«è¡Œä¸šè¯æ±‡: {', '.join(found_terms)}")
                else:
                    print(f"   âš ï¸  æŸ¥è¯¢æœªåŒ…å«æ˜æ˜¾çš„è¡Œä¸šè¯æ±‡")
        else:
            print(f"   âŒ æŸ¥è¯¢ç”Ÿæˆå¤±è´¥")
    
    print("\\n" + "=" * 50)
    print("âœ… é›†æˆæµ‹è¯•å®Œæˆ")
    
    return True

def compare_with_original():
    """ä¸åŸå§‹ç‰ˆæœ¬å¯¹æ¯”"""
    
    print("\\nğŸ“Š ä¸åŸå§‹ç‰ˆæœ¬å¯¹æ¯”...")
    print("=" * 50)
    
    # æµ‹è¯•æ–‡ç« 
    test_content = """---
title: "æ–°èƒ½æºæ±½è½¦ç”µæ± æŠ€æœ¯åˆ›æ–°æŠ¥å‘Š"
date: 2026-02-25
categories: ["technology", "manufacturing"]
---

# æ–°èƒ½æºæ±½è½¦ç”µæ± æŠ€æœ¯åˆ›æ–°æŠ¥å‘Š

## æŠ€æœ¯å‘å±•è¶‹åŠ¿

å›ºæ€ç”µæ± ã€å¿«å……æŠ€æœ¯ã€ç”µæ± ç®¡ç†ç³»ç»Ÿç­‰åˆ›æ–°æ­£åœ¨æ¨åŠ¨è¡Œä¸šå‘å±•ã€‚
"""
    
    # å¯¼å…¥ä¸¤ä¸ªç‰ˆæœ¬
    try:
        # åŸå§‹ç‰ˆæœ¬
        sys.path.insert(0, str(Path(__file__).parent))
        from auto_add_public_images import extract_queries as original_extract_queries
        
        # å¢å¼ºç‰ˆæœ¬
        from auto_add_public_images_enhanced import extract_queries as enhanced_extract_queries
        from auto_add_public_images_enhanced import extract_article_keywords
        
        print("âœ… æˆåŠŸå¯¼å…¥ä¸¤ä¸ªç‰ˆæœ¬")
        
        # åŸå§‹ç‰ˆæœ¬ç»“æœ
        print("\\nğŸ” åŸå§‹ç‰ˆæœ¬ç»“æœ:")
        original_queries = original_extract_queries(test_content)
        print(f"   æŸ¥è¯¢: {original_queries}")
        
        # å¢å¼ºç‰ˆæœ¬ç»“æœ
        print("\\nğŸ” å¢å¼ºç‰ˆæœ¬ç»“æœ:")
        enhanced_queries = enhanced_extract_queries(test_content)
        print(f"   æŸ¥è¯¢: {enhanced_queries}")
        
        # å…³é”®è¯æå–
        keywords = extract_article_keywords(test_content, "manufacturing")
        print(f"   å…³é”®è¯: {', '.join(keywords[:5])}")
        
        print("\\nğŸ“ˆ æ”¹è¿›åˆ†æ:")
        print(f"   åŸå§‹æŸ¥è¯¢æ•°é‡: {len(original_queries)}")
        print(f"   å¢å¼ºæŸ¥è¯¢æ•°é‡: {len(enhanced_queries)}")
        
        # æ£€æŸ¥æŸ¥è¯¢ç›¸å…³æ€§
        relevant_terms = ["battery", "electric vehicle", "manufacturing", "technology"]
        original_relevant = sum(1 for q in original_queries if any(term in q.lower() for term in relevant_terms))
        enhanced_relevant = sum(1 for q in enhanced_queries if any(term in q.lower() for term in relevant_terms))
        
        print(f"   åŸå§‹ç›¸å…³æŸ¥è¯¢: {original_relevant}/{len(original_queries)}")
        print(f"   å¢å¼ºç›¸å…³æŸ¥è¯¢: {enhanced_relevant}/{len(enhanced_queries)}