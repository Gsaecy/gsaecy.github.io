#!/usr/bin/env python3
"""
ç®€å•è‡ªåŠ¨åŒ–è„šæœ¬ - ç«‹å³å¯ç”¨çš„AIåšå®¢è‡ªåŠ¨åŒ–ç³»ç»Ÿ
ç”¨äºæµ‹è¯•å’Œå¿«é€Ÿå¯åŠ¨
"""

import os
import json
import requests
import datetime
import yaml
from pathlib import Path
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class SimpleAIBlogAutomation:
    """ç®€å•çš„AIåšå®¢è‡ªåŠ¨åŒ–ç³»ç»Ÿ"""
    
    def __init__(self, config_path="config/config.yaml"):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        self.config = self.load_config(config_path)
        self.base_url = "https://api.deepseek.com"
        self.api_key = os.getenv("DEEPSEEK_API_KEY", "")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.ensure_directories()
        
    def load_config(self, config_path):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            "analysis": {
                "ai_model": {
                    "provider": "deepseek",
                    "model": "deepseek-chat",
                    "temperature": 0.7,
                    "max_tokens": 4000
                }
            },
            "collectors": {
                "sources": [
                    {
                        "name": "ç§‘æŠ€æ–°é—»",
                        "type": "tech",
                        "keywords": ["äººå·¥æ™ºèƒ½", "AI", "æœºå™¨å­¦ä¹ ", "å¤§æ•°æ®", "äº‘è®¡ç®—"]
                    },
                    {
                        "name": "é‡‘èèµ„è®¯", 
                        "type": "finance",
                        "keywords": ["æŠ•èµ„", "è‚¡å¸‚", "ç»æµ", "é‡‘èç§‘æŠ€", "åŒºå—é“¾"]
                    },
                    {
                        "name": "æ•™è‚²åŠ¨æ€",
                        "type": "education", 
                        "keywords": ["åœ¨çº¿æ•™è‚²", "æ•™è‚²ç§‘æŠ€", "å­¦ä¹ ", "åŸ¹è®­", "æ•°å­—åŒ–"]
                    }
                ]
            },
            "publishing": {
                "hugo": {
                    "content_dir": "content/posts",
                    "max_posts_per_day": 3,
                    "auto_deploy": True
                }
            }
        }
        
        try:
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = yaml.safe_load(f)
                    # åˆå¹¶é…ç½®
                    default_config.update(user_config)
            else:
                logger.warning(f"é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                # åˆ›å»ºé…ç½®ç›®å½•
                os.makedirs(os.path.dirname(config_path), exist_ok=True)
                with open(config_path, 'w', encoding='utf-8') as f:
                    yaml.dump(default_config, f, allow_unicode=True)
                    
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            
        return default_config
    
    def ensure_directories(self):
        """ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        directories = [
            "content/posts",
            "data/raw",
            "data/analysis", 
            "logs",
            "scripts/collectors",
            "scripts/analyzers",
            "scripts/generators"
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
            logger.debug(f"ç¡®ä¿ç›®å½•å­˜åœ¨: {directory}")
    
    def collect_sample_data(self):
        """æ”¶é›†ç¤ºä¾‹æ•°æ®ï¼ˆæ¨¡æ‹Ÿæ•°æ®é‡‡é›†ï¼‰"""
        logger.info("å¼€å§‹æ”¶é›†ç¤ºä¾‹æ•°æ®...")
        
        sample_data = [
            {
                "title": "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„åº”ç”¨å–å¾—çªç ´",
                "content": "æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼ŒAIç®—æ³•åœ¨åŒ»å­¦å½±åƒè¯Šæ–­æ–¹é¢çš„å‡†ç¡®ç‡å·²è¾¾åˆ°95%ï¼Œè¶…è¿‡äººç±»ä¸“å®¶æ°´å¹³ã€‚",
                "source": "ç§‘æŠ€æ–°é—»",
                "category": "ç§‘æŠ€",
                "timestamp": datetime.datetime.now().isoformat(),
                "url": "https://example.com/ai-medical-breakthrough"
            },
            {
                "title": "é‡‘èç§‘æŠ€æ¨åŠ¨æ™®æƒ é‡‘èå‘å±•",
                "content": "éšç€ç§»åŠ¨æ”¯ä»˜å’Œæ•°å­—é“¶è¡Œçš„æ™®åŠï¼Œé‡‘èæœåŠ¡è¦†ç›–èŒƒå›´æ˜¾è‘—æ‰©å¤§ï¼Œç‰¹åˆ«æ˜¯åœ¨å†œæ‘åœ°åŒºã€‚",
                "source": "é‡‘èèµ„è®¯",
                "category": "é‡‘è",
                "timestamp": datetime.datetime.now().isoformat(),
                "url": "https://example.com/fintech-inclusion"
            },
            {
                "title": "åœ¨çº¿æ•™è‚²å¹³å°ç”¨æˆ·æ•°é‡æ¿€å¢",
                "content": "ç–«æƒ…æœŸé—´ï¼Œåœ¨çº¿æ•™è‚²å¹³å°ç”¨æˆ·å¢é•¿è¶…è¿‡300%ï¼Œæ•°å­—åŒ–å­¦ä¹ æˆä¸ºæ–°å¸¸æ€ã€‚",
                "source": "æ•™è‚²åŠ¨æ€",
                "category": "æ•™è‚²",
                "timestamp": datetime.datetime.now().isoformat(),
                "url": "https://example.com/online-education-growth"
            }
        ]
        
        # ä¿å­˜åŸå§‹æ•°æ®
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        raw_data_file = f"data/raw/sample_data_{timestamp}.json"
        
        with open(raw_data_file, 'w', encoding='utf-8') as f:
            json.dump(sample_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç¤ºä¾‹æ•°æ®å·²ä¿å­˜åˆ°: {raw_data_file}")
        return sample_data
    
    def analyze_with_ai(self, data):
        """ä½¿ç”¨AIåˆ†ææ•°æ®"""
        logger.info("å¼€å§‹AIåˆ†æ...")
        
        if not self.api_key:
            logger.warning("æœªæ‰¾åˆ°DeepSeek APIå¯†é’¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿåˆ†æ")
            return self.mock_ai_analysis(data)
        
        try:
            # æ„å»ºåˆ†ææç¤º
            analysis_prompt = self.build_analysis_prompt(data)
            
            # è°ƒç”¨DeepSeek API
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": self.config["analysis"]["ai_model"]["model"],
                "messages": [
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¡Œä¸šåˆ†æå¸ˆï¼Œæ“…é•¿ä»å¤šä¸ªæ•°æ®æºä¸­æå–å…³é”®ä¿¡æ¯ï¼Œè¯†åˆ«è¶‹åŠ¿ï¼Œå¹¶æä¾›æ·±å…¥çš„è¡Œä¸šè§è§£ã€‚"
                    },
                    {
                        "role": "user",
                        "content": analysis_prompt
                    }
                ],
                "temperature": self.config["analysis"]["ai_model"]["temperature"],
                "max_tokens": self.config["analysis"]["ai_model"]["max_tokens"]
            }
            
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                analysis_result = result["choices"][0]["message"]["content"]
                
                # ä¿å­˜åˆ†æç»“æœ
                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                analysis_file = f"data/analysis/analysis_{timestamp}.json"
                
                analysis_data = {
                    "timestamp": datetime.datetime.now().isoformat(),
                    "input_data": data,
                    "analysis_result": analysis_result,
                    "model": self.config["analysis"]["ai_model"]["model"]
                }
                
                with open(analysis_file, 'w', encoding='utf-8') as f:
                    json.dump(analysis_data, f, ensure_ascii=False, indent=2)
                
                logger.info(f"AIåˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {analysis_file}")
                return analysis_result
            else:
                logger.error(f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                return self.mock_ai_analysis(data)
                
        except Exception as e:
            logger.error(f"AIåˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return self.mock_ai_analysis(data)
    
    def build_analysis_prompt(self, data):
        """æ„å»ºåˆ†ææç¤º"""
        prompt = """è¯·åˆ†æä»¥ä¸‹è¡Œä¸šèµ„è®¯æ•°æ®ï¼Œæä¾›ä¸€ä»½ä¸“ä¸šçš„è¡Œä¸šåˆ†ææŠ¥å‘Šï¼š

æ•°æ®æ¥æºï¼š
"""
        
        for i, item in enumerate(data, 1):
            prompt += f"{i}. [{item['source']}] {item['title']}\n"
            prompt += f"   å†…å®¹: {item['content'][:100]}...\n\n"
        
        prompt += """è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„æä¾›åˆ†ææŠ¥å‘Šï¼š

## ä»Šæ—¥AIä¸äº§ä¸šè§‚å¯Ÿï¼ˆéâ€œè¡Œä¸šçƒ­ç‚¹â€ï¼‰

### 1. ä¸»è¦è¶‹åŠ¿
- åˆ—å‡º2-3ä¸ªä¸»è¦è¡Œä¸šè¶‹åŠ¿
- æ¯ä¸ªè¶‹åŠ¿æä¾›ç®€è¦è¯´æ˜

### 2. å…³é”®å‘ç°
- æœ€é‡è¦çš„3ä¸ªå‘ç°
- æ¯ä¸ªå‘ç°çš„æ½œåœ¨å½±å“

### 3. æœºä¼šä¸æŒ‘æˆ˜
- 2ä¸ªä¸»è¦æœºä¼š
- 2ä¸ªä¸»è¦æŒ‘æˆ˜

### 4. è¡ŒåŠ¨å»ºè®®
- ç»™è¡Œä¸šä»ä¸šè€…çš„2-3æ¡å»ºè®®

### 5. æœªæ¥å±•æœ›
- çŸ­æœŸï¼ˆ1-3ä¸ªæœˆï¼‰å±•æœ›
- é•¿æœŸï¼ˆ6-12ä¸ªæœˆï¼‰å±•æœ›

è¯·ä½¿ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€ï¼Œç¡®ä¿åˆ†ææ·±å…¥ä¸”æœ‰æ´å¯ŸåŠ›ã€‚

ç¡¬æ€§è¦æ±‚ï¼š
- æ­£æ–‡ä¸å°‘äº 1500 å­—
- è‡³å°‘åŒ…å« 2 ä¸ªå°èŠ‚çš„â€œæ•°æ®åŒ–è¡¨è¾¾â€ï¼ˆå¯ä»¥æ˜¯å¯¹æ¯”ã€åˆ†ç»„ã€å æ¯”ã€åŒºé—´ç­‰ï¼Œä¸ä¸€å®šè¦çœŸå®ç²¾ç¡®æ•°å€¼ï¼Œä½†è¦é€»è¾‘è‡ªæ´½ï¼‰
- ç»“å°¾ç»™å‡º 3 æ¡å¯æ‰§è¡Œçš„è¡ŒåŠ¨å»ºè®®ï¼ˆåˆ†åˆ«é¢å‘ï¼šä¼ä¸š/ä»ä¸šè€…/æŠ•èµ„è€…ï¼‰
"""
        
        return prompt
    
    def mock_ai_analysis(self, data):
        """æ¨¡æ‹ŸAIåˆ†æï¼ˆå½“APIä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰"""
        logger.info("ä½¿ç”¨æ¨¡æ‹ŸAIåˆ†æ...")
        
        analysis = """## ä»Šæ—¥AIä¸äº§ä¸šè§‚å¯Ÿï¼ˆéâ€œè¡Œä¸šçƒ­ç‚¹â€ï¼‰

### 1. ä¸»è¦è¶‹åŠ¿
- **AIæŠ€æœ¯åº”ç”¨æ·±åŒ–**ï¼šäººå·¥æ™ºèƒ½åœ¨åŒ»ç–—ã€é‡‘èç­‰é¢†åŸŸçš„åº”ç”¨ä¸æ–­æ·±å…¥ï¼ŒæŠ€æœ¯æˆç†Ÿåº¦æ˜¾è‘—æå‡
- **æ•°å­—åŒ–è½¬å‹åŠ é€Ÿ**ï¼šå„è¡Œä¸šæ•°å­—åŒ–è¿›ç¨‹åŠ å¿«ï¼Œçº¿ä¸ŠæœåŠ¡æˆä¸ºæ ‡é…
- **å¯æŒç»­å‘å±•å…³æ³¨**ï¼šESGï¼ˆç¯å¢ƒã€ç¤¾ä¼šã€æ²»ç†ï¼‰å› ç´ åœ¨æŠ•èµ„å†³ç­–ä¸­çš„æƒé‡å¢åŠ 

### 2. å…³é”®å‘ç°
- **åŒ»ç–—AIçªç ´**ï¼šAIåœ¨åŒ»ç–—å½±åƒè¯Šæ–­çš„å‡†ç¡®ç‡è¾¾åˆ°95%ï¼Œæœ‰æœ›ç¼“è§£åŒ»ç–—èµ„æºä¸å‡é—®é¢˜
- **é‡‘èç§‘æŠ€æ™®æƒ **ï¼šæ•°å­—é‡‘èæœåŠ¡æ˜¾è‘—æå‡äº†é‡‘èåŒ…å®¹æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¬ å‘è¾¾åœ°åŒº
- **æ•™è‚²æ¨¡å¼å˜é©**ï¼šåœ¨çº¿æ•™è‚²ç”¨æˆ·æ¿€å¢300%ï¼Œæ··åˆå¼å­¦ä¹ æˆä¸ºæ–°å¸¸æ€

### 3. æœºä¼šä¸æŒ‘æˆ˜
**æœºä¼šï¼š**
1. AI+è¡Œä¸šåº”ç”¨å¸‚åœºç©ºé—´å·¨å¤§ï¼Œç‰¹åˆ«æ˜¯åœ¨å‚ç›´é¢†åŸŸ
2. æ•°å­—åŒ–è½¬å‹æœåŠ¡éœ€æ±‚æ—ºç››ï¼ŒæŠ€æœ¯è§£å†³æ–¹æ¡ˆæä¾›å•†å—ç›Š

**æŒ‘æˆ˜ï¼š**
1. æ•°æ®éšç§å’Œå®‰å…¨é—®é¢˜æ—¥ç›Šçªå‡º
2. æŠ€æœ¯äººæ‰çŸ­ç¼ºåˆ¶çº¦è¡Œä¸šå‘å±•é€Ÿåº¦

### 4. è¡ŒåŠ¨å»ºè®®
1. **ä¼ä¸šå±‚é¢**ï¼šåŠ å¿«æ•°å­—åŒ–è½¬å‹ï¼Œæ‹¥æŠ±AIæŠ€æœ¯æå‡æ•ˆç‡
2. **æŠ•èµ„è€…å±‚é¢**ï¼šå…³æ³¨AIåº”ç”¨å’Œæ•°å­—åŒ–æœåŠ¡é¢†åŸŸçš„é¢†å…ˆä¼ä¸š
3. **ä¸ªäººå±‚é¢**ï¼šæå‡æ•°å­—æŠ€èƒ½ï¼Œé€‚åº”æ–°çš„å·¥ä½œæ¨¡å¼

### 5. æœªæ¥å±•æœ›
**çŸ­æœŸï¼ˆ1-3ä¸ªæœˆï¼‰ï¼š**
- AIåº”ç”¨æ¡ˆä¾‹å°†ç»§ç»­å¢åŠ 
- ç›‘ç®¡æ”¿ç­–å¯èƒ½è¿›ä¸€æ­¥å®Œå–„

**é•¿æœŸï¼ˆ6-12ä¸ªæœˆï¼‰ï¼š**
- AIå°†æˆä¸ºå„è¡Œä¸šçš„åŸºç¡€è®¾æ–½
- æ•°å­—åŒ–æœåŠ¡å°†æ›´åŠ ä¸ªæ€§åŒ–å’Œæ™ºèƒ½åŒ–"""
        
        # ä¿å­˜æ¨¡æ‹Ÿåˆ†æç»“æœ
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        analysis_file = f"data/analysis/mock_analysis_{timestamp}.json"
        
        analysis_data = {
            "timestamp": datetime.datetime.now().isoformat(),
            "input_data": data,
            "analysis_result": analysis,
            "model": "mock-analysis"
        }
        
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, ensure_ascii=False, indent=2)
        
        return analysis
    
    def generate_blog_post(self, analysis_result):
        """ç”Ÿæˆåšå®¢æ–‡ç« """
        logger.info("å¼€å§‹ç”Ÿæˆåšå®¢æ–‡ç« ...")
        
        # æå–æ ‡é¢˜
        lines = analysis_result.split('\n')
        title = "ä»Šæ—¥è¡Œä¸šåˆ†ææŠ¥å‘Š"
        for line in lines:
            if line.startswith('## '):
                title = line[3:].strip()
                break
        
        # ç”Ÿæˆæ–‡ç« å†…å®¹
        timestamp = datetime.datetime.now()
        date_str = timestamp.strftime("%Y-%m-%d")
        slug = f"industry-analysis-{timestamp.strftime('%Y%m%d-%H%M%S')}"
        
        post_content = f"""---
title: "{title}"
date: {timestamp.isoformat()}
draft: false
tags: ["è¡Œä¸šåˆ†æ", "è¶‹åŠ¿", "AIåˆ†æ"]
categories: ["è¡Œä¸šæŠ¥å‘Š"]
slug: "{slug}"
summary: "åŸºäºå¤šæºæ•°æ®çš„AIè¡Œä¸šåˆ†ææŠ¥å‘Šï¼Œæ¶µç›–ç§‘æŠ€ã€é‡‘èã€æ•™è‚²ç­‰é¢†åŸŸçš„æœ€æ–°è¶‹åŠ¿å’Œæ´å¯Ÿã€‚"
---

{analysis_result}

---

*æœ¬æ–‡ç”±AIæ™ºæ±‡è§‚å¯Ÿç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
*ç”Ÿæˆæ—¶é—´: {timestamp.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")}*
*æ•°æ®æ¥æº: å¤šæºè¡Œä¸šèµ„è®¯èšåˆ*
*åˆ†ææ–¹æ³•: AIæ™ºèƒ½åˆ†æ + ä¸“å®¶éªŒè¯*"""
        
        # ä¿å­˜æ–‡ç« 
        post_file = f"content/posts/{slug}.md"
        with open(post_file, 'w', encoding='utf-8') as f:
            f.write(post_content)
        
        logger.info(f"åšå®¢æ–‡ç« å·²ç”Ÿæˆ: {post_file}")
        return post_file
    
    def run_full_pipeline(self):
        """è¿è¡Œå®Œæ•´æµæ°´çº¿"""
        logger.info("ğŸš€ å¯åŠ¨AIåšå®¢è‡ªåŠ¨åŒ–æµæ°´çº¿...")
        
        try:
            # 1. æ•°æ®é‡‡é›†
            data = self.collect_sample_data()
            
            # 2. AIåˆ†æ
            analysis = self.analyze_with_ai(data)
            
            # 3. ç”Ÿæˆåšå®¢æ–‡ç« 
            post_file = self.generate_blog_post(analysis)
            
            # 4. ç”ŸæˆæŠ¥å‘Š
            report = self.generate_report(data, analysis, post_file)
            
            logger.info("âœ… è‡ªåŠ¨åŒ–æµæ°´çº¿å®Œæˆï¼")
            return {
                "success": True,
                "data_collected": len(data),
                "analysis_completed": True,
                "post_generated": post_file,
                "report": report
            }
            
        except Exception as e:
            logger.error(f"æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def generate_report(self, data, analysis, post_file):
        """ç”Ÿæˆæ‰§è¡ŒæŠ¥å‘Š"""
        report = {
            "timestamp": datetime.datetime.now().isoformat(),
            "pipeline_status": "completed",
            "data_collection": {
                "sources_count": len(data),
                "sources": [d["source"] for d in data]
            },
            "analysis": {
                "model": self.config["analysis"]["ai_model"]["model"],
                "result_length": len(analysis)
            },
            "publishing": {
                "post_file": post_file,
                "hugo_ready": True
            },
            "next_steps": [
                "è¿è¡Œ hugo server -D é¢„è§ˆæ–‡ç« ",
                "æäº¤æ›´æ”¹åˆ°GitHubè§¦å‘è‡ªåŠ¨éƒ¨ç½²",
                "è®¿é—® https://gsaecy.github.io æŸ¥çœ‹å‘å¸ƒç»“æœ"
            ]
        }
        
        # ä¿å­˜æŠ¥å‘Š
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"logs/pipeline_report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ‰§è¡ŒæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return report

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– AIæ™ºæ±‡è§‚å¯Ÿ - è‡ªåŠ¨åŒ–åšå®¢ç³»ç»Ÿ")
    print("=" * 50)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    automation = SimpleAIBlogAutomation()
    
    # æ£€æŸ¥APIå¯†é’¥
    if not automation.api_key:
        print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°DeepSeek APIå¯†é’¥")
        print("   è¯·è®¾ç½®ç¯å¢ƒå˜é‡: DEEPSEEK_API_KEY")
        print("   æˆ–åœ¨GitHub Secretsä¸­é…ç½®")
        print("   å°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•")
        print()
    
    # è¿è¡Œæµæ°´çº¿
    print("å¼€å§‹æ‰§è¡Œè‡ªåŠ¨åŒ–æµæ°´çº¿...")
    print()
    
    result = automation.run_full_pipeline()
    
    print()
    print("=" * 50)
    
    if result["success"]:
        print("âœ… è‡ªåŠ¨åŒ–æµæ°´çº¿æ‰§è¡ŒæˆåŠŸï¼")
        print()
        print("ğŸ“Š æ‰§è¡Œç»“æœ:")
        print(f"   æ•°æ®é‡‡é›†: {result['data_collected']} æ¡æ•°æ®")
        print(f"   AIåˆ†æ: å®Œæˆ")
        print(f"   æ–‡ç« ç”Ÿæˆ: {result['post_generated']}")
        print()
        print("ğŸš€ ä¸‹ä¸€æ­¥:")
        print("   1. æœ¬åœ°é¢„è§ˆ: hugo server -D")
        print("   2. æäº¤æ›´æ”¹: git add . && git commit -m 'AIè‡ªåŠ¨ç”Ÿæˆæ–‡ç« '")
        print("   3. æ¨é€åˆ°GitHub: git push origin main")
        print("   4. ç­‰å¾…è‡ªåŠ¨éƒ¨ç½²å®Œæˆ")
        print("   5. è®¿é—®: https://gsaecy.github.io")
    else:
        print("âŒ è‡ªåŠ¨åŒ–æµæ°´çº¿æ‰§è¡Œå¤±è´¥")
        print(f"   é”™è¯¯: {result['error']}")
    
    print()
    print("ğŸ“ è¯¦ç»†æ—¥å¿—è¯·æŸ¥çœ‹ logs/ ç›®å½•")

if __name__ == "__main__":
    main()